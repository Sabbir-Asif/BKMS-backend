[
    {
        "topic_name": "Paraphrase Detection",
        "summary": "default",
        "papers": [
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "Improving Paraphrase Detection with the Adversarial Paraphrasing Task",
                "Authors": "Animesh Nighojkar, John Licato",
                "Abstract": "  If two sentences have the same meaning, it should follow that they are\nequivalent in their inferential properties, i.e., each sentence should\ntextually entail the other. However, many paraphrase datasets currently in\nwidespread use rely on a sense of paraphrase based on word overlap and syntax.\nCan we teach them instead to identify paraphrases in a way that draws on the\ninferential properties of the sentences, and is not over-reliant on lexical and\nsyntactic similarities of a sentence pair? We apply the adversarial paradigm to\nthis question, and introduce a new adversarial method of dataset creation for\nparaphrase identification: the Adversarial Paraphrasing Task (APT), which asks\nparticipants to generate semantically equivalent (in the sense of mutually\nimplicative) but lexically and syntactically disparate paraphrases. These\nsentence pairs can then be used both to test paraphrase identification models\n(which get barely random accuracy) and then improve their performance. To\naccelerate dataset generation, we explore automation of APT using T5, and show\nthat the resulting dataset also improves accuracy. We discuss implications for\nparaphrase detection and release our dataset in the hope of making paraphrase\ndetection models better able to detect sentence-level meaning equivalence.\n",
                "Link": "http://arxiv.org/pdf/2106.07691v1",
                "arxiv_id": "2106.07691v1",
                "publicationDate": "2021-06-14T18:15:20Z"
            },
            {
                "Title": "Paraphrase Types for Generation and Detection",
                "Authors": "Jan Philip Wahle, Bela Gipp, Terry Ruas",
                "Abstract": "  Current approaches in paraphrase generation and detection heavily rely on a\nsingle general similarity score, ignoring the intricate linguistic properties\nof language. This paper introduces two new tasks to address this shortcoming by\nconsidering paraphrase types - specific linguistic perturbations at particular\ntext positions. We name these tasks Paraphrase Type Generation and Paraphrase\nType Detection. Our results suggest that while current techniques perform well\nin a binary classification scenario, i.e., paraphrased or not, the inclusion of\nfine-grained paraphrase types poses a significant challenge. While most\napproaches are good at generating and detecting general semantic similar\ncontent, they fail to understand the intrinsic linguistic variables they\nmanipulate. Models trained in generating and identifying paraphrase types also\nshow improvements in tasks without them. In addition, scaling these models\nfurther improves their ability to understand paraphrase types. We believe\nparaphrase types can unlock a new paradigm for developing paraphrase models and\nsolving tasks in the future.\n",
                "Link": "http://arxiv.org/pdf/2310.14863v1",
                "arxiv_id": "2310.14863v1",
                "publicationDate": "2023-10-23T12:32:41Z"
            },
            {
                "Title": "Methods for Detecting Paraphrase Plagiarism",
                "Authors": "Victor Thompson",
                "Abstract": "  Paraphrase plagiarism is one of the difficult challenges facing plagiarism\ndetection systems. Paraphrasing occur when texts are lexically or syntactically\naltered to look different, but retain their original meaning. Most plagiarism\ndetection systems (many of which are commercial based) are designed to detect\nword co-occurrences and light modifications, but are unable to detect severe\nsemantic and structural alterations such as what is seen in many academic\ndocuments. Hence many paraphrase plagiarism cases go undetected. In this paper,\nwe approached the problem of paraphrase plagiarism by proposing methods for\ndetecting the most common techniques (phenomena) used in paraphrasing texts\n(namely; lexical substitution, insertion/deletion and word and phrase\nreordering), and combined the methods into a paraphrase detection model. We\nevaluated our proposed methods and model on collections containing paraphrase\ntexts. Experimental results show significant improvement in performance when\nthe methods were combined (the proposed model) as opposed to running them\nindividually. The results also show that the proposed paraphrase detection\nmodel outperformed a standard baseline (based on greedy string tilling), and\nprevious studies.\n",
                "Link": "http://arxiv.org/pdf/1712.10309v1",
                "arxiv_id": "1712.10309v1",
                "publicationDate": "2017-12-29T18:53:12Z"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1",
                "publicationDate": "2023-11-10T17:38:46Z"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1",
                "publicationDate": "2023-10-17T02:12:12Z"
            },
            {
                "Title": "Acquiring Lexical Paraphrases from a Single Corpus",
                "Authors": "Oren Glickman, Ido Dagan",
                "Abstract": "  This paper studies the potential of identifying lexical paraphrases within a\nsingle corpus, focusing on the extraction of verb paraphrases. Most previous\napproaches detect individual paraphrase instances within a pair (or set) of\ncomparable corpora, each of them containing roughly the same information, and\nrely on the substantial level of correspondence of such corpora. We present a\nnovel method that successfully detects isolated paraphrase instances within a\nsingle corpus without relying on any a-priori structure and information. A\ncomparison suggests that an instance-based approach may be combined with a\nvector based approach in order to assess better the paraphrase likelihood for\nmany verb pairs.\n",
                "Link": "http://arxiv.org/pdf/cs/0312058v1",
                "arxiv_id": "0312058v1",
                "publicationDate": "2003-12-25T16:45:20Z"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1",
                "publicationDate": "2023-11-22T08:25:15Z"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1",
                "publicationDate": "2021-11-12T09:38:15Z"
            },
            {
                "Title": "What's Mine becomes Yours: Defining, Annotating and Detecting\n  Context-Dependent Paraphrases in News Interview Dialogs",
                "Authors": "Anna Wegmann, Tijs van den Broek, Dong Nguyen",
                "Abstract": "  Best practices for high conflict conversations like counseling or customer\nsupport almost always include recommendations to paraphrase the previous\nspeaker. Although paraphrase classification has received widespread attention\nin NLP, paraphrases are usually considered independent from context, and common\nmodels and datasets are not applicable to dialog settings. In this work, we\ninvestigate paraphrases in dialog (e.g., Speaker 1: \"That book is mine.\"\nbecomes Speaker 2: \"That book is yours.\"). We provide an operationalization of\ncontext-dependent paraphrases, and develop a training for crowd-workers to\nclassify paraphrases in dialog. We introduce a dataset with utterance pairs\nfrom NPR and CNN news interviews annotated for context-dependent paraphrases.\nTo enable analyses on label variation, the dataset contains 5,581 annotations\non 600 utterance pairs. We present promising results with in-context learning\nand with token classification models for automatic paraphrase detection in\ndialog.\n",
                "Link": "http://arxiv.org/pdf/2404.06670v1",
                "arxiv_id": "2404.06670v1",
                "publicationDate": "2024-04-10T01:14:12Z"
            },
            {
                "Title": "'John ate 5 apples' != 'John ate some apples': Self-Supervised\n  Paraphrase Quality Detection for Algebraic Word Problems",
                "Authors": "Rishabh Gupta, Venktesh V, Mukesh Mohania, Vikram Goyal",
                "Abstract": "  This paper introduces the novel task of scoring paraphrases for Algebraic\nWord Problems (AWP) and presents a self-supervised method for doing so. In the\ncurrent online pedagogical setting, paraphrasing these problems is helpful for\nacademicians to generate multiple syntactically diverse questions for\nassessments. It also helps induce variation to ensure that the student has\nunderstood the problem instead of just memorizing it or using unfair means to\nsolve it. The current state-of-the-art paraphrase generation models often\ncannot effectively paraphrase word problems, losing a critical piece of\ninformation (such as numbers or units) which renders the question unsolvable.\nThere is a need for paraphrase scoring methods in the context of AWP to enable\nthe training of good paraphrasers. Thus, we propose ParaQD, a self-supervised\nparaphrase quality detection method using novel data augmentations that can\nlearn latent representations to separate a high-quality paraphrase of an\nalgebraic question from a poor one by a wide margin. Through extensive\nexperimentation, we demonstrate that our method outperforms existing\nstate-of-the-art self-supervised methods by up to 32% while also demonstrating\nimpressive zero-shot performance.\n",
                "Link": "http://arxiv.org/pdf/2206.08263v1",
                "arxiv_id": "2206.08263v1",
                "publicationDate": "2022-06-16T16:01:59Z"
            },
            {
                "Title": "A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase\n  Detection in Short Texts",
                "Authors": "Muhammad Haroon Shakeel, Asim Karim, Imdadullah Khan",
                "Abstract": "  Paraphrase detection is an important task in text analytics with numerous\napplications such as plagiarism detection, duplicate question identification,\nand enhanced customer support helpdesks. Deep models have been proposed for\nrepresenting and classifying paraphrases. These models, however, require large\nquantities of human-labeled data, which is expensive to obtain. In this work,\nwe present a data augmentation strategy and a multi-cascaded model for improved\nparaphrase detection in short texts. Our data augmentation strategy considers\nthe notions of paraphrases and non-paraphrases as binary relations over the set\nof texts. Subsequently, it uses graph theoretic concepts to efficiently\ngenerate additional paraphrase and non-paraphrase pairs in a sound manner. Our\nmulti-cascaded model employs three supervised feature learners (cascades) based\non CNN and LSTM networks with and without soft-attention. The learned features,\ntogether with hand-crafted linguistic features, are then forwarded to a\ndiscriminator network for final classification. Our model is both wide and deep\nand provides greater robustness across clean and noisy short texts. We evaluate\nour approach on three benchmark datasets and show that it produces a comparable\nor state-of-the-art performance on all three.\n",
                "Link": "http://arxiv.org/pdf/1912.12068v1",
                "arxiv_id": "1912.12068v1",
                "publicationDate": "2019-12-27T12:10:10Z"
            },
            {
                "Title": "Semantic Search as Extractive Paraphrase Span Detection",
                "Authors": "Jenna Kanerva, Hanna Kitti, Li-Hsin Chang, Teemu Vahtola, Mathias Creutz, Filip Ginter",
                "Abstract": "  In this paper, we approach the problem of semantic search by framing the\nsearch task as paraphrase span detection, i.e. given a segment of text as a\nquery phrase, the task is to identify its paraphrase in a given document, the\nsame modelling setup as typically used in extractive question answering. On the\nTurku Paraphrase Corpus of 100,000 manually extracted Finnish paraphrase pairs\nincluding their original document context, we find that our paraphrase span\ndetection model outperforms two strong retrieval baselines (lexical similarity\nand BERT sentence embeddings) by 31.9pp and 22.4pp respectively in terms of\nexact match, and by 22.3pp and 12.9pp in terms of token-level F-score. This\ndemonstrates a strong advantage of modelling the task in terms of span\nretrieval, rather than sentence similarity. Additionally, we introduce a method\nfor creating artificial paraphrase data through back-translation, suitable for\nlanguages where manually annotated paraphrase resources for training the span\ndetection model are not available.\n",
                "Link": "http://arxiv.org/pdf/2112.04886v1",
                "arxiv_id": "2112.04886v1",
                "publicationDate": "2021-12-09T13:16:42Z"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1",
                "publicationDate": "2020-05-29T15:38:54Z"
            },
            {
                "Title": "BanglaSarc: A Dataset for Sarcasm Detection",
                "Authors": "Tasnim Sakib Apon, Ramisa Anan, Elizabeth Antora Modhu, Arjun Suter, Ifrit Jamal Sneha, MD. Golam Rabiul Alam",
                "Abstract": "  Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.\n",
                "Link": "http://arxiv.org/pdf/2209.13461v1",
                "arxiv_id": "2209.13461v1",
                "publicationDate": "2022-09-27T15:28:21Z"
            },
            {
                "Title": "ARPA: Armenian Paraphrase Detection Corpus and Models",
                "Authors": "Arthur Malajyan, Karen Avetisyan, Tsolak Ghukasyan",
                "Abstract": "  In this work, we employ a semi-automatic method based on back translation to\ngenerate a sentential paraphrase corpus for the Armenian language. The initial\ncollection of sentences is translated from Armenian to English and back twice,\nresulting in pairs of lexically distant but semantically similar sentences. The\ngenerated paraphrases are then manually reviewed and annotated. Using the\nmethod train and test datasets are created, containing 2360 paraphrases in\ntotal. In addition, the datasets are used to train and evaluate BERTbased\nmodels for detecting paraphrase in Armenian, achieving results comparable to\nthe state-of-the-art of other languages.\n",
                "Link": "http://arxiv.org/pdf/2009.12615v1",
                "arxiv_id": "2009.12615v1",
                "publicationDate": "2020-09-26T14:56:57Z"
            },
            {
                "Title": "How Large Language Models are Transforming Machine-Paraphrased\n  Plagiarism",
                "Authors": "Jan Philip Wahle, Terry Ruas, Frederic Kirstein, Bela Gipp",
                "Abstract": "  The recent success of large language models for text generation poses a\nsevere threat to academic integrity, as plagiarists can generate realistic\nparaphrases indistinguishable from original work. However, the role of large\nautoregressive transformers in generating machine-paraphrased plagiarism and\ntheir detection is still developing in the literature. This work explores T5\nand GPT-3 for machine-paraphrase generation on scientific articles from arXiv,\nstudent theses, and Wikipedia. We evaluate the detection performance of six\nautomated solutions and one commercial plagiarism detection software and\nperform a human study with 105 participants regarding their detection\nperformance and the quality of generated examples. Our results suggest that\nlarge models can rewrite text humans have difficulty identifying as\nmachine-paraphrased (53% mean acc.). Human experts rate the quality of\nparaphrases generated by GPT-3 as high as original texts (clarity 4.0/5,\nfluency 4.2/5, coherence 3.8/5). The best-performing detection model (GPT-3)\nachieves a 66% F1-score in detecting paraphrases.\n",
                "Link": "http://arxiv.org/pdf/2210.03568v3",
                "arxiv_id": "2210.03568v3",
                "publicationDate": "2022-10-07T14:08:57Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "Automatically Ranked Russian Paraphrase Corpus for Text Generation",
                "Authors": "Vadim Gudkov, Olga Mitrofanova, Elizaveta Filippskikh",
                "Abstract": "  The article is focused on automatic development and ranking of a large corpus\nfor Russian paraphrase generation which proves to be the first corpus of such\ntype in Russian computational linguistics. Existing manually annotated\nparaphrase datasets for Russian are limited to small-sized ParaPhraser corpus\nand ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and\nplagiarism detection, sentence similarity and relatedness estimation, etc. Due\nto size restrictions, these datasets can hardly be applied in end-to-end text\ngeneration solutions. Meanwhile, paraphrase generation requires a large amount\nof training data. In our study we propose a solution to the problem: we\ncollect, rank and evaluate a new publicly available headline paraphrase corpus\n(ParaPhraser Plus), and then perform text generation experiments with manual\nevaluation on automatically ranked corpora using the Universal Transformer\narchitecture.\n",
                "Link": "http://arxiv.org/pdf/2006.09719v1",
                "arxiv_id": "2006.09719v1",
                "publicationDate": "2020-06-17T08:40:52Z"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1",
                "publicationDate": "2023-10-13T13:25:16Z"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1",
                "publicationDate": "2018-09-04T11:55:34Z"
            },
            {
                "Title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language",
                "Authors": "Aunabil Chakma, Masum Hasan",
                "Abstract": "  This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023\n",
                "Link": "http://arxiv.org/pdf/2311.12735v1",
                "arxiv_id": "2311.12735v1",
                "publicationDate": "2023-11-21T17:21:15Z"
            },
            {
                "Title": "Paraphrase Detection: Human vs. Machine Content",
                "Authors": "Jonas Becker, Jan Philip Wahle, Terry Ruas, Bela Gipp",
                "Abstract": "  The growing prominence of large language models, such as GPT-4 and ChatGPT,\nhas led to increased concerns over academic integrity due to the potential for\nmachine-generated content and paraphrasing. Although studies have explored the\ndetection of human- and machine-paraphrased content, the comparison between\nthese types of content remains underexplored. In this paper, we conduct a\ncomprehensive analysis of various datasets commonly employed for paraphrase\ndetection tasks and evaluate an array of detection methods. Our findings\nhighlight the strengths and limitations of different detection methods in terms\nof performance on individual datasets, revealing a lack of suitable\nmachine-generated datasets that can be aligned with human expectations. Our\nmain finding is that human-authored paraphrases exceed machine-generated ones\nin terms of difficulty, diversity, and similarity implying that automatically\ngenerated texts are not yet on par with human-level performance. Transformers\nemerged as the most effective method across datasets with TF-IDF excelling on\nsemantically diverse corpora. Additionally, we identify four datasets as the\nmost diverse and challenging for paraphrase detection.\n",
                "Link": "http://arxiv.org/pdf/2303.13989v1",
                "arxiv_id": "2303.13989v1",
                "publicationDate": "2023-03-24T13:25:46Z"
            },
            {
                "Title": "Acquisition of Phrase Correspondences using Natural Deduction Proofs",
                "Authors": "Hitomi Yanaka, Koji Mineshima, Pascual Martinez-Gomez, Daisuke Bekki",
                "Abstract": "  How to identify, extract, and use phrasal knowledge is a crucial problem for\nthe task of Recognizing Textual Entailment (RTE). To solve this problem, we\npropose a method for detecting paraphrases via natural deduction proofs of\nsemantic relations between sentence pairs. Our solution relies on a graph\nreformulation of partial variable unifications and an algorithm that induces\nsubgraph alignments between meaning representations. Experiments show that our\nmethod can automatically detect various paraphrases that are absent from\nexisting paraphrase databases. In addition, the detection of paraphrases using\nproof information improves the accuracy of RTE tasks.\n",
                "Link": "http://arxiv.org/pdf/1804.07656v1",
                "arxiv_id": "1804.07656v1",
                "publicationDate": "2018-04-20T15:02:25Z"
            },
            {
                "Title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an\n  effective defense",
                "Authors": "Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer",
                "Abstract": "  The rise in malicious usage of large language models, such as fake content\ncreation and academic plagiarism, has motivated the development of approaches\nthat identify AI-generated text, including those based on watermarking or\noutlier detection. However, the robustness of these detection algorithms to\nparaphrases of AI-generated text remains unclear. To stress test these\ndetectors, we build a 11B parameter paraphrase generation model (DIPPER) that\ncan paraphrase paragraphs, condition on surrounding context, and control\nlexical diversity and content reordering. Using DIPPER to paraphrase text\ngenerated by three large language models (including GPT3.5-davinci-003)\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection\naccuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of\n1%), without appreciably modifying the input semantics.\n  To increase the robustness of AI-generated text detection to paraphrase\nattacks, we introduce a simple defense that relies on retrieving\nsemantically-similar generations and must be maintained by a language model API\nprovider. Given a candidate text, our algorithm searches a database of\nsequences previously generated by the API, looking for sequences that match the\ncandidate text within a certain threshold. We empirically verify our defense\nusing a database of 15M generations from a fine-tuned T5-XXL model and find\nthat it can detect 80% to 97% of paraphrased generations across different\nsettings while only classifying 1% of human-written sequences as AI-generated.\nWe open-source our models, code and data.\n",
                "Link": "http://arxiv.org/pdf/2303.13408v2",
                "arxiv_id": "2303.13408v2",
                "publicationDate": "2023-03-23T16:29:27Z"
            },
            {
                "Title": "Corpus-Based Paraphrase Detection Experiments and Review",
                "Authors": "Tedo Vrbanec, Ana Mestrovic",
                "Abstract": "  Paraphrase detection is important for a number of applications, including\nplagiarism detection, authorship attribution, question answering, text\nsummarization, text mining in general, etc. In this paper, we give a\nperformance overview of various types of corpus-based models, especially deep\nlearning (DL) models, with the task of paraphrase detection. We report the\nresults of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO,\nand USE) evaluated on three different public available corpora: Microsoft\nResearch Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase\nCorpus 2011. Through a great number of experiments, we decided on the most\nappropriate approaches for text pre-processing: hyper-parameters, sub-model\nselection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and\nsemantic similarity/paraphrase detection threshold. Our findings and those of\nother researchers who have used deep learning models show that DL models are\nvery competitive with traditional state-of-the-art approaches and have\npotential that should be further developed.\n",
                "Link": "http://arxiv.org/pdf/2106.00145v1",
                "arxiv_id": "2106.00145v1",
                "publicationDate": "2021-05-31T23:29:24Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Paraphrase Identification with Deep Learning: A Review of Datasets and\n  Methods",
                "Authors": "Chao Zhou, Cheng Qiu, Daniel E. Acuna",
                "Abstract": "  The rapid advancement of AI technology has made text generation tools like\nGPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can\npose serious threat to the credibility of various forms of media if these\ntechnologies are used for plagiarism, including scientific literature and news\nsources. Despite the development of automated methods for paraphrase\nidentification, detecting this type of plagiarism remains a challenge due to\nthe disparate nature of the datasets on which these methods are trained. In\nthis study, we review traditional and current approaches to paraphrase\nidentification and propose a refined typology of paraphrases. We also\ninvestigate how this typology is represented in popular datasets and how\nunder-representation of certain types of paraphrases impacts detection\ncapabilities. Finally, we outline new directions for future research and\ndatasets in the pursuit of more effective paraphrase detection using AI.\n",
                "Link": "http://arxiv.org/pdf/2212.06933v1",
                "arxiv_id": "2212.06933v1",
                "publicationDate": "2022-12-13T23:06:20Z"
            },
            {
                "Title": "Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking",
                "Authors": "Md Muzakker Hossain, Zahin Awosaf, Md. Salman Hossan Prottoy, Abu Saleh Muhammod Alvy, Md. Kishor Morol",
                "Abstract": "  Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n",
                "Link": "http://arxiv.org/pdf/2203.11486v1",
                "arxiv_id": "2203.11486v1",
                "publicationDate": "2022-03-22T06:33:01Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1",
                "publicationDate": "2023-11-25T13:47:34Z"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1",
                "publicationDate": "2024-03-31T09:52:25Z"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "PerPaDa: A Persian Paraphrase Dataset based on Implicit Crowdsourcing\n  Data Collection",
                "Authors": "Salar Mohtaj, Fatemeh Tavakkoli, Habibollah Asghari",
                "Abstract": "  In this paper we introduce PerPaDa, a Persian paraphrase dataset that is\ncollected from users' input in a plagiarism detection system. As an implicit\ncrowdsourcing experience, we have gathered a large collection of original and\nparaphrased sentences from Hamtajoo; a Persian plagiarism detection system, in\nwhich users try to conceal cases of text re-use in their documents by\nparaphrasing and re-submitting manuscripts for analysis. The compiled dataset\ncontains 2446 instances of paraphrasing. In order to improve the overall\nquality of the collected data, some heuristics have been used to exclude\nsentences that don't meet the proposed criteria. The introduced corpus is much\nlarger than the available datasets for the task of paraphrase identification in\nPersian. Moreover, there is less bias in the data compared to the similar\ndatasets, since the users did not try some fixed predefined rules in order to\ngenerate similar texts to their original inputs.\n",
                "Link": "http://arxiv.org/pdf/2201.06573v1",
                "arxiv_id": "2201.06573v1",
                "publicationDate": "2022-01-17T18:48:39Z"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1",
                "publicationDate": "2019-11-19T20:37:03Z"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1",
                "publicationDate": "2020-04-19T07:42:22Z"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1",
                "publicationDate": "2021-12-03T13:35:18Z"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1",
                "publicationDate": "2022-06-01T10:10:15Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1",
                "publicationDate": "2023-05-31T04:08:57Z"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1",
                "publicationDate": "2024-04-17T21:09:13Z"
            },
            {
                "Title": "BDSL 49: A Comprehensive Dataset of Bangla Sign Language",
                "Authors": "Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein",
                "Abstract": "  Language is a method by which individuals express their thoughts. Each\nlanguage has its own set of alphabetic and numeric characters. People can\ncommunicate with one another through either oral or written communication.\nHowever, each language has a sign language counterpart. Individuals who are\ndeaf and/or mute communicate through sign language. The Bangla language also\nhas a sign language, which is called BDSL. The dataset is about Bangla hand\nsign images. The collection contains 49 individual Bangla alphabet images in\nsign language. BDSL49 is a dataset that consists of 29,490 images with 49\nlabels. Images of 14 different adult individuals, each with a distinct\nbackground and appearance, have been recorded during data collection. Several\nstrategies have been used to eliminate noise from datasets during preparation.\nThis dataset is available to researchers for free. They can develop automated\nsystems using machine learning, computer vision, and deep learning techniques.\nIn addition, two models were used in this dataset. The first is for detection,\nwhile the second is for recognition.\n",
                "Link": "http://arxiv.org/pdf/2208.06827v1",
                "arxiv_id": "2208.06827v1",
                "publicationDate": "2022-08-14T10:54:49Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1",
                "publicationDate": "2023-09-27T14:10:57Z"
            },
            {
                "Title": "A Robust Semantics-based Watermark for Large Language Model against\n  Paraphrasing",
                "Authors": "Jie Ren, Han Xu, Yiding Liu, Yingqian Cui, Shuaiqiang Wang, Dawei Yin, Jiliang Tang",
                "Abstract": "  Large language models (LLMs) have show great ability in various natural\nlanguage tasks. However, there are concerns that LLMs are possible to be used\nimproperly or even illegally. To prevent the malicious usage of LLMs, detecting\nLLM-generated text becomes crucial in the deployment of LLM applications.\nWatermarking is an effective strategy to detect the LLM-generated content by\nencoding a pre-defined secret watermark to facilitate the detection process.\nHowever, the majority of existing watermark methods leverage the simple hashes\nof precedent tokens to partition vocabulary. Such watermark can be easily\neliminated by paraphrase and correspondingly the detection effectiveness will\nbe greatly compromised. Thus, to enhance the robustness against paraphrase, we\npropose a semantics-based watermark framework SemaMark. It leverages the\nsemantics as an alternative to simple hashes of tokens since the paraphrase\nwill likely preserve the semantic meaning of the sentences. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and robustness of\nSemaMark under different paraphrases.\n",
                "Link": "http://arxiv.org/pdf/2311.08721v2",
                "arxiv_id": "2311.08721v2",
                "publicationDate": "2023-11-15T06:19:02Z"
            },
            {
                "Title": "Extracting and filtering paraphrases by bridging natural language\n  inference and paraphrasing",
                "Authors": "Matej Klemen, Marko Robnik-\u0160ikonja",
                "Abstract": "  Paraphrasing is a useful natural language processing task that can contribute\nto more diverse generated or translated texts. Natural language inference (NLI)\nand paraphrasing share some similarities and can benefit from a joint approach.\nWe propose a novel methodology for the extraction of paraphrasing datasets from\nNLI datasets and cleaning existing paraphrasing datasets. Our approach is based\non bidirectional entailment; namely, if two sentences can be mutually entailed,\nthey are paraphrases. We evaluate our approach using several large pretrained\ntransformer language models in the monolingual and cross-lingual setting. The\nresults show high quality of extracted paraphrasing datasets and surprisingly\nhigh noise levels in two existing paraphrasing datasets.\n",
                "Link": "http://arxiv.org/pdf/2111.07119v1",
                "arxiv_id": "2111.07119v1",
                "publicationDate": "2021-11-13T14:06:37Z"
            },
            {
                "Title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural\n  Paraphrase Detection",
                "Authors": "Jan Philip Wahle, Terry Ruas, Norman Meuschke, Bela Gipp",
                "Abstract": "  The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n",
                "Link": "http://arxiv.org/pdf/2103.12450v5",
                "arxiv_id": "2103.12450v5",
                "publicationDate": "2021-03-23T11:01:35Z"
            },
            {
                "Title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT",
                "Authors": "Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim",
                "Abstract": "  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n",
                "Link": "http://arxiv.org/pdf/2403.05519v1",
                "arxiv_id": "2403.05519v1",
                "publicationDate": "2024-03-08T18:42:59Z"
            }
        ]
    },
    {
        "topic_name": "Morphological Analysis",
        "summary": "default",
        "papers": [
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2",
                "publicationDate": "2023-10-13T16:46:38Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1",
                "publicationDate": "2012-06-02T13:23:18Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1",
                "publicationDate": "2023-11-06T13:02:07Z"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1",
                "publicationDate": "2022-05-28T15:39:09Z"
            }
        ]
    },
    {
        "topic_name": "Anaphora Resolution",
        "summary": "default",
        "papers": [
            {
                "Title": "Anaphora Resolution in Japanese Sentences Using Surface Expressions and\n  Examples",
                "Authors": "Masaki Murata",
                "Abstract": "  Anaphora resolution is one of the major problems in natural language\nprocessing. It is also one of the important tasks in machine translation and\nman/machine dialogue. We solve the problem by using surface expressions and\nexamples. Surface expressions are the words in sentences which provide clues\nfor anaphora resolution. Examples are linguistic data which are actually used\nin conversations and texts. The method using surface expressions and examples\nis a practical method. This thesis handles almost all kinds of anaphora: i. The\nreferential property and number of a noun phrase ii. Noun phrase direct\nanaphora iii. Noun phrase indirect anaphora iv. Pronoun anaphora v. Verb phrase\nellipsis\n",
                "Link": "http://arxiv.org/pdf/cs/0009011v1",
                "arxiv_id": "0009011v1",
                "publicationDate": "2000-09-19T00:44:47Z"
            },
            {
                "Title": "Stay Together: A System for Single and Split-antecedent Anaphora\n  Resolution",
                "Authors": "Juntao Yu, Nafise Sadat Moosavi, Silviu Paun, Massimo Poesio",
                "Abstract": "  The state-of-the-art on basic, single-antecedent anaphora has greatly\nimproved in recent years. Researchers have therefore started to pay more\nattention to more complex cases of anaphora such as split-antecedent anaphora,\nas in Time-Warner is considering a legal challenge to Telecommunications Inc's\nplan to buy half of Showtime Networks Inc-a move that could lead to all-out war\nbetween the two powerful companies. Split-antecedent anaphora is rarer and more\ncomplex to resolve than single-antecedent anaphora; as a result, it is not\nannotated in many datasets designed to test coreference, and previous work on\nresolving this type of anaphora was carried out in unrealistic conditions that\nassume gold mentions and/or gold split-antecedent anaphors are available. These\nsystems also focus on split-antecedent anaphors only. In this work, we\nintroduce a system that resolves both single and split-antecedent anaphors, and\nevaluate it in a more realistic setting that uses predicted mentions. We also\nstart addressing the question of how to evaluate single and split-antecedent\nanaphors together using standard coreference evaluation metrics.\n",
                "Link": "http://arxiv.org/pdf/2104.05320v1",
                "arxiv_id": "2104.05320v1",
                "publicationDate": "2021-04-12T10:01:08Z"
            },
            {
                "Title": "ParseTalk about Sentence- and Text-Level Anaphora",
                "Authors": "Michael Strube, Udo Hahn",
                "Abstract": "  We provide a unified account of sentence-level and text-level anaphora within\nthe framework of a dependency-based grammar model. Criteria for anaphora\nresolution within sentence boundaries rephrase major concepts from GB's binding\ntheory, while those for text-level anaphora incorporate an adapted version of a\nGrosz-Sidner-style focus model.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9503006v1",
                "arxiv_id": "9503006v1",
                "publicationDate": "1995-03-03T16:53:53Z"
            },
            {
                "Title": "Stress Testing BERT Anaphora Resolution Models for Reaction Extraction\n  in Chemical Patents",
                "Authors": "Chieling Yueh, Evangelos Kanoulas, Bruno Martins, Camilo Thorne, Saber Akhondi",
                "Abstract": "  The high volume of published chemical patents and the importance of a timely\nacquisition of their information gives rise to automating information\nextraction from chemical patents. Anaphora resolution is an important component\nof comprehensive information extraction, and is critical for extracting\nreactions. In chemical patents, there are five anaphoric relations of interest:\nco-reference, transformed, reaction associated, work up, and contained. Our\ngoal is to investigate how the performance of anaphora resolution models for\nreaction texts in chemical patents differs in a noise-free and noisy\nenvironment and to what extent we can improve the robustness against noise of\nthe model.\n",
                "Link": "http://arxiv.org/pdf/2306.13379v1",
                "arxiv_id": "2306.13379v1",
                "publicationDate": "2023-06-23T09:01:56Z"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1",
                "publicationDate": "2021-09-24T05:31:01Z"
            },
            {
                "Title": "Enhanced Word Representations for Bridging Anaphora Resolution",
                "Authors": "Yufang Hou",
                "Abstract": "  Most current models of word representations(e.g.,GloVe) have successfully\ncaptured fine-grained semantics. However, semantic similarity exhibited in\nthese word embeddings is not suitable for resolving bridging anaphora, which\nrequires the knowledge of associative similarity (i.e., relatedness) instead of\nsemantic similarity information between synonyms or hypernyms. We create word\nembeddings (embeddings_PP) to capture such relatedness by exploring the\nsyntactic structure of noun phrases. We demonstrate that using embeddings_PP\nalone achieves around 30% of accuracy for bridging anaphora resolution on the\nISNotes corpus. Furthermore, we achieve a substantial gain over the\nstate-of-the-art system (Hou et al., 2013) for bridging antecedent selection.\n",
                "Link": "http://arxiv.org/pdf/1803.04790v2",
                "arxiv_id": "1803.04790v2",
                "publicationDate": "2018-03-13T13:33:06Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Focusing for Pronoun Resolution in English Discourse: An Implementation",
                "Authors": "Ebru Ersan, Varol Akman",
                "Abstract": "  Anaphora resolution is one of the most active research areas in natural\nlanguage processing. This study examines focusing as a tool for the resolution\nof pronouns which are a kind of anaphora. Focusing is a discourse phenomenon\nlike anaphora. Candy Sidner formalized focusing in her 1979 MIT PhD thesis and\ndevised several algorithms to resolve definite anaphora including pronouns. She\npresented her theory in a computational framework but did not generally\nimplement the algorithms. Her algorithms related to focusing and pronoun\nresolution are implemented in this thesis. This implementation provides a\nbetter comprehension of the theory both from a conceptual and a computational\npoint of view. The resulting program is tested on different discourse segments,\nand evaluation and analysis of the experiments are presented together with the\nstatistical results.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9409005v1",
                "arxiv_id": "9409005v1",
                "publicationDate": "1994-09-07T14:26:08Z"
            },
            {
                "Title": "NP Animacy Identification for Anaphora Resolution",
                "Authors": "R. J. Evans, C. Orasan",
                "Abstract": "  In anaphora resolution for English, animacy identification can play an\nintegral role in the application of agreement restrictions between pronouns and\ncandidates, and as a result, can improve the accuracy of anaphora resolution\nsystems. In this paper, two methods for animacy identification are proposed and\nevaluated using intrinsic and extrinsic measures. The first method is a\nrule-based one which uses information about the unique beginners in WordNet to\nclassify NPs on the basis of their animacy. The second method relies on a\nmachine learning algorithm which exploits a WordNet enriched with animacy\ninformation for each sense. The effect of word sense disambiguation on the two\nmethods is also assessed. The intrinsic evaluation reveals that the machine\nlearning method reaches human levels of performance. The extrinsic evaluation\ndemonstrates that animacy identification can be beneficial in anaphora\nresolution, especially in the cases where animate entities are identified with\nhigh precision.\n",
                "Link": "http://arxiv.org/pdf/1110.2215v1",
                "arxiv_id": "1110.2215v1",
                "publicationDate": "2011-10-10T22:13:24Z"
            },
            {
                "Title": "Bridging Anaphora Resolution as Question Answering",
                "Authors": "Yufang Hou",
                "Abstract": "  Most previous studies on bridging anaphora resolution (Poesio et al., 2004;\nHou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and\nassume that the gold mention information is given. In this paper, we cast\nbridging anaphora resolution as question answering based on context. This\nallows us to find the antecedent for a given anaphor without knowing any gold\nmention information (except the anaphor itself). We present a question\nanswering framework (BARQA) for this task, which leverages the power of\ntransfer learning. Furthermore, we propose a novel method to generate a large\namount of \"quasi-bridging\" training data. We show that our model pre-trained on\nthis dataset and fine-tuned on a small amount of in-domain dataset achieves new\nstate-of-the-art results for bridging anaphora resolution on two bridging\ncorpora (ISNotes (Markert et al., 2012) and BASHI (Roesiger, 2018)).\n",
                "Link": "http://arxiv.org/pdf/2004.07898v3",
                "arxiv_id": "2004.07898v3",
                "publicationDate": "2020-04-16T19:42:43Z"
            },
            {
                "Title": "A Deterministic Algorithm for Bridging Anaphora Resolution",
                "Authors": "Yufang Hou",
                "Abstract": "  Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et\nal., 2013b) use syntactic preposition patterns to calculate word relatedness.\nHowever, such patterns only consider NPs' head nouns and hence do not fully\ncapture the semantics of NPs. Recently, Hou (2018) created word embeddings\n(embeddings_PP) to capture associative similarity (ie, relatedness) between\nnouns by exploring the syntactic structure of noun phrases. But embeddings_PP\nonly contains word representations for nouns. In this paper, we create new word\nvectors by combining embeddings_PP with GloVe. This new word embeddings\n(embeddings_bridging) are a more general lexical knowledge resource for\nbridging and allow us to represent the meaning of an NP beyond its head easily.\nWe therefore develop a deterministic approach for bridging anaphora resolution,\nwhich represents the semantics of an NP based on its head noun and\nmodifications. We show that this simple approach achieves the competitive\nresults compared to the best system in Hou et al.(2013b) which explores Markov\nLogic Networks to model the problem. Additionally, we further improve the\nresults for bridging anaphora resolution reported in Hou (2018) by combining\nour simple deterministic approach with Hou et al.(2013b)'s best system MLN II.\n",
                "Link": "http://arxiv.org/pdf/1811.05721v1",
                "arxiv_id": "1811.05721v1",
                "publicationDate": "2018-11-14T10:49:39Z"
            },
            {
                "Title": "Computational Approach to Anaphora Resolution in Spanish Dialogues",
                "Authors": "P. Martinez-Barco, M. Palomar",
                "Abstract": "  This paper presents an algorithm for identifying noun-phrase antecedents of\npronouns and adjectival anaphors in Spanish dialogues. We believe that anaphora\nresolution requires numerous sources of information in order to find the\ncorrect antecedent of the anaphor. These sources can be of different kinds,\ne.g., linguistic information, discourse/dialogue structure information, or\ntopic information. For this reason, our algorithm uses various different kinds\nof information (hybrid information). The algorithm is based on linguistic\nconstraints and preferences and uses an anaphoric accessibility space within\nwhich the algorithm finds the noun phrase. We present some experiments related\nto this algorithm and this space using a corpus of 204 dialogues. The algorithm\nis implemented in Prolog. According to this study, 95.9% of antecedents were\nlocated in the proposed space, a precision of 81.3% was obtained for pronominal\nanaphora resolution, and 81.5% for adjectival anaphora.\n",
                "Link": "http://arxiv.org/pdf/1106.0673v1",
                "arxiv_id": "1106.0673v1",
                "publicationDate": "2011-06-03T14:54:46Z"
            },
            {
                "Title": "A Public Reference Implementation of the RAP Anaphora Resolution\n  Algorithm",
                "Authors": "Long Qiu, Min-Yen Kan, Tat-Seng Chua",
                "Abstract": "  This paper describes a standalone, publicly-available implementation of the\nResolution of Anaphora Procedure (RAP) given by Lappin and Leass (1994). The\nRAP algorithm resolves third person pronouns, lexical anaphors, and identifies\npleonastic pronouns. Our implementation, JavaRAP, fills a current need in\nanaphora resolution research by providing a reference implementation that can\nbe benchmarked against current algorithms. The implementation uses the\nstandard, publicly available Charniak (2000) parser as input, and generates a\nlist of anaphora-antecedent pairs as output. Alternately, an in-place\nannotation or substitution of the anaphors with their antecedents can be\nproduced. Evaluation on the MUC-6 co-reference task shows that JavaRAP has an\naccuracy of 57.9%, similar to the performance given previously in the\nliterature (e.g., Preiss 2002).\n",
                "Link": "http://arxiv.org/pdf/cs/0406031v1",
                "arxiv_id": "0406031v1",
                "publicationDate": "2004-06-17T12:29:29Z"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1",
                "publicationDate": "2014-01-06T20:25:26Z"
            },
            {
                "Title": "Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of\n  In-Context Experts",
                "Authors": "Nghia T. Le, Fan Bai, Alan Ritter",
                "Abstract": "  Anaphora resolution is an important task for information extraction across a\nrange of languages, text genres, and domains, motivating the need for methods\nthat do not require large annotated datasets. In-context learning has emerged\nas a promising approach, yet there are a number of challenges in applying\nin-context learning to resolve anaphora. For example, encoding a single\nin-context demonstration that consists of: an anaphor, a paragraph-length\ncontext, and a list of corresponding antecedents, requires conditioning a\nlanguage model on a long sequence of tokens, limiting the number of\ndemonstrations per prompt. In this paper, we present MICE (Mixtures of\nIn-Context Experts), which we demonstrate is effective for few-shot anaphora\nresolution in scientific protocols (Tamari et al., 2021). Given only a handful\nof training examples, MICE combines the predictions of hundreds of in-context\nexperts, yielding a 30% increase in F1 score over a competitive prompt\nretrieval baseline. Furthermore, we show MICE can be used to train compact\nstudent models without sacrificing performance. As far as we are aware, this is\nthe first work to present experimental results demonstrating the effectiveness\nof in-context learning on the task of few-shot anaphora resolution in\nscientific protocols.\n",
                "Link": "http://arxiv.org/pdf/2210.03690v2",
                "arxiv_id": "2210.03690v2",
                "publicationDate": "2022-10-07T16:51:45Z"
            },
            {
                "Title": "A Mention-Ranking Model for Abstract Anaphora Resolution",
                "Authors": "Ana Marasovi\u0107, Leo Born, Juri Opitz, Anette Frank",
                "Abstract": "  Resolving abstract anaphora is an important, but difficult task for text\nunderstanding. Yet, with recent advances in representation learning this task\nbecomes a more tangible aim. A central property of abstract anaphora is that it\nestablishes a relation between the anaphor embedded in the anaphoric sentence\nand its (typically non-nominal) antecedent. We propose a mention-ranking model\nthat learns how abstract anaphors relate to their antecedents with an\nLSTM-Siamese Net. We overcome the lack of training data by generating\nartificial anaphoric sentence--antecedent pairs. Our model outperforms\nstate-of-the-art results on shell noun resolution. We also report first\nbenchmark results on an abstract anaphora subset of the ARRAU corpus. This\ncorpus presents a greater challenge due to a mixture of nominal and pronominal\nanaphors and a greater range of confounders. We found model variants that\noutperform the baselines for nominal anaphors, without training on individual\nanaphor data, but still lag behind for pronominal anaphors. Our model selects\nsyntactically plausible candidates and -- if disregarding syntax --\ndiscriminates candidates using deeper features.\n",
                "Link": "http://arxiv.org/pdf/1706.02256v2",
                "arxiv_id": "1706.02256v2",
                "publicationDate": "2017-06-07T16:58:59Z"
            },
            {
                "Title": "A Knowledge-poor Pronoun Resolution System for Turkish",
                "Authors": "Dilek K\u00fc\u00e7\u00fck, Meltem Turhan Y\u00f6ndem",
                "Abstract": "  A pronoun resolution system which requires limited syntactic knowledge to\nidentify the antecedents of personal and reflexive pronouns in Turkish is\npresented. As in its counterparts for languages like English, Spanish and\nFrench, the core of the system is the constraints and preferences determined\nempirically. In the evaluation phase, it performed considerably better than the\nbaseline algorithm used for comparison. The system is significant for its being\nthe first fully specified knowledge-poor computational framework for pronoun\nresolution in Turkish where Turkish possesses different structural properties\nfrom the languages for which knowledge-poor systems had been developed.\n",
                "Link": "http://arxiv.org/pdf/1504.04751v1",
                "arxiv_id": "1504.04751v1",
                "publicationDate": "2015-04-18T18:34:19Z"
            },
            {
                "Title": "Translation of Pronominal Anaphora between English and Spanish:\n  Discrepancies and Evaluation",
                "Authors": "A. Ferrandez, J. Peral",
                "Abstract": "  This paper evaluates the different tasks carried out in the translation of\npronominal anaphora in a machine translation (MT) system. The MT interlingua\napproach named AGIR (Anaphora Generation with an Interlingua Representation)\nimproves upon other proposals presented to date because it is able to translate\nintersentential anaphors, detect co-reference chains, and translate Spanish\nzero pronouns into English---issues hardly considered by other systems. The\npaper presents the resolution and evaluation of these anaphora problems in AGIR\nwith the use of different kinds of knowledge (lexical, morphological,\nsyntactic, and semantic). The translation of English and Spanish anaphoric\nthird-person personal pronouns (including Spanish zero pronouns) into the\ntarget language has been evaluated on unrestricted corpora. We have obtained a\nprecision of 80.4% and 84.8% in the translation of Spanish and English\npronouns, respectively. Although we have only studied the Spanish and English\nlanguages, our approach can be easily extended to other languages such as\nPortuguese, Italian, or Japanese.\n",
                "Link": "http://arxiv.org/pdf/1106.4862v1",
                "arxiv_id": "1106.4862v1",
                "publicationDate": "2011-06-24T00:55:35Z"
            },
            {
                "Title": "Cooperation between Pronoun and Reference Resolution for Unrestricted\n  Texts",
                "Authors": "Andrei Popescu-Belis, Isabelle Robba",
                "Abstract": "  Anaphora resolution is envisaged in this paper as part of the reference\nresolution process. A general open architecture is proposed, which can be\nparticularized and configured in order to simulate some classic anaphora\nresolution methods. With the aim of improving pronoun resolution, the system\ntakes advantage of elementary cues about characters of the text, which are\nrepresented through a particular data structure. In its most robust\nconfiguration, the system uses only a general lexicon, a local morpho-syntactic\nparser and a dictionary of synonyms. A short comparative corpus analysis shows\nthat narrative texts are the most suitable for testing such a system.\n",
                "Link": "http://arxiv.org/pdf/cs/0208037v1",
                "arxiv_id": "0208037v1",
                "publicationDate": "2002-08-21T14:36:13Z"
            },
            {
                "Title": "Recognizing Referential Links: An Information Extraction Perspective",
                "Authors": "Megumi Kameyama",
                "Abstract": "  We present an efficient and robust reference resolution algorithm in an\nend-to-end state-of-the-art information extraction system, which must work with\na considerably impoverished syntactic analysis of the input sentences.\nConsidering this disadvantage, the basic setup to collect, filter, then order\nby salience does remarkably well with third-person pronouns, but needs more\nsemantic and discourse information to improve the treatments of other\nexpression types.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9707009v1",
                "arxiv_id": "9707009v1",
                "publicationDate": "1997-07-18T22:41:57Z"
            },
            {
                "Title": "Evaluating a Focus-Based Approach to Anaphora Resolution",
                "Authors": "Saliha Azzam, Kevin Humphreys, Robert Gaizauskas",
                "Abstract": "  We present an approach to anaphora resolution based on a focusing algorithm,\nand implemented within an existing MUC (Message Understanding Conference)\nInformation Extraction system, allowing quantitative evaluation against a\nsubstantial corpus of annotated real-world texts. Extensions to the basic\nfocusing mechanism can be easily tested, resulting in refinements to the\nmechanism and resolution rules. Results are compared with the results of a\nsimpler heuristic-based approach.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9807001v1",
                "arxiv_id": "9807001v1",
                "publicationDate": "1998-07-06T17:28:35Z"
            },
            {
                "Title": "Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution",
                "Authors": "Ryuto Konno, Shun Kiyono, Yuichiroh Matsubayashi, Hiroki Ouchi, Kentaro Inui",
                "Abstract": "  Masked language models (MLMs) have contributed to drastic performance\nimprovements with regard to zero anaphora resolution (ZAR). To further improve\nthis approach, in this study, we made two proposals. The first is a new\npretraining task that trains MLMs on anaphoric relations with explicit\nsupervision, and the second proposal is a new finetuning method that remedies a\nnotorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese\nZAR demonstrated that our two proposals boost the state-of-the-art performance,\nand our detailed analysis provides new insights on the remaining challenges.\n",
                "Link": "http://arxiv.org/pdf/2104.07425v2",
                "arxiv_id": "2104.07425v2",
                "publicationDate": "2021-04-15T12:43:57Z"
            },
            {
                "Title": "CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
                "Authors": "Bo-Hsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Lin Li, Hong Yu",
                "Abstract": "  Anaphora and ellipses are two common phenomena in dialogues. Without\nresolving referring expressions and information omission, dialogue systems may\nfail to generate consistent and coherent responses. Traditionally, anaphora is\nresolved by coreference resolution and ellipses by query rewrite. In this work,\nwe propose a novel joint learning framework of modeling coreference resolution\nand query rewriting for complex, multi-turn dialogue understanding. Given an\nongoing dialogue between a user and a dialogue assistant, for the user query,\nour joint learning model first predicts coreference links between the query and\nthe dialogue context, and then generates a self-contained rewritten user query.\nTo evaluate our model, we annotate a dialogue based coreference resolution\ndataset, MuDoCo, with rewritten queries. Results show that the performance of\nquery rewrite can be substantially boosted (+2.3% F1) with the aid of\ncoreference modeling. Furthermore, our joint model outperforms the\nstate-of-the-art coreference resolution model (+2% F1) on this dataset.\n",
                "Link": "http://arxiv.org/pdf/2105.09914v1",
                "arxiv_id": "2105.09914v1",
                "publicationDate": "2021-05-20T17:17:26Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "Extending DRT with a Focusing Mechanism for Pronominal Anaphora and\n  Ellipsis Resolution",
                "Authors": "Jose Abracos, Jose Gabriel Lopes",
                "Abstract": "  Cormack (1992) proposed a framework for pronominal anaphora resolution. Her\nproposal integrates focusing theory (Sidner et al.) and DRT (Kamp and Reyle).\nWe analyzed this methodology and adjusted it to the processing of Portuguese\ntexts. The scope of the framework was widened to cover sentences containing\nrestrictive relative clauses and subject ellipsis. Tests were conceived and\napplied to probe the adequacy of proposed modifications when dealing with\nprocessing of current texts.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9411016v1",
                "arxiv_id": "9411016v1",
                "publicationDate": "1994-11-09T17:53:57Z"
            },
            {
                "Title": "An Algorithm to Co-Ordinate Anaphora Resolution and PPS Disambiguation\n  Process",
                "Authors": "Saliha Azzam",
                "Abstract": "  This paper concerns both anaphora resolution and prepositional phrase (PP)\nattachment that are the most frequent ambiguities in natural language\nprocessing. Several methods have been proposed to deal with each phenomenon\nseparately, however none of proposed systems has considered the way of dealing\nboth phenomena. We tackle this issue, proposing an algorithm to co-ordinate the\ntreatment of these two problems efficiently, i.e., the aim is also to exploit\nat each step all the results that each component can provide.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9502033v1",
                "arxiv_id": "9502033v1",
                "publicationDate": "1995-02-24T19:34:10Z"
            },
            {
                "Title": "Anaphora Resolution in Dialogue Systems for South Asian Languages",
                "Authors": "Vinay Annam, Nikhil Koditala, Radhika Mamidi",
                "Abstract": "  Anaphora resolution is a challenging task which has been the interest of NLP\nresearchers for a long time. Traditional resolution techniques like eliminative\nconstraints and weighted preferences were successful in many languages.\nHowever, they are ineffective in free word order languages like most SouthAsian\nlanguages.Heuristic and rule-based techniques were typical in these languages,\nwhich are constrained to context and domain.In this paper, we venture a new\nstrategy us-ing neural networks for resolving anaphora in human-human\ndialogues. The architecture chiefly consists of three components, a shallow\nparser for extracting features, a feature vector generator which produces the\nword embed-dings, and a neural network model which will predict the antecedent\nmention of an anaphora.The system has been trained and tested on Telugu\nconversation corpus we generated. Given the advantage of the semantic\ninformation in word embeddings and appending actor, gender, number, person and\npart of plural features the model has reached an F1-score of 86.\n",
                "Link": "http://arxiv.org/pdf/1911.09994v1",
                "arxiv_id": "1911.09994v1",
                "publicationDate": "2019-11-22T12:20:44Z"
            },
            {
                "Title": "TAPHSIR: Towards AnaPHoric Ambiguity Detection and ReSolution In\n  Requirements",
                "Authors": "Saad Ezzini, Sallam Abualhaija, Chetan Arora, Mehrdad Sabetzadeh",
                "Abstract": "  We introduce TAPHSIR, a tool for anaphoric ambiguity detection and anaphora\nresolution in requirements. TAPHSIR facilities reviewing the use of pronouns in\na requirements specification and revising those pronouns that can lead to\nmisunderstandings during the development process. To this end, TAPHSIR detects\nthe requirements which have potential anaphoric ambiguity and further attempts\ninterpreting anaphora occurrences automatically. TAPHSIR employs a hybrid\nsolution composed of an ambiguity detection solution based on machine learning\nand an anaphora resolution solution based on a variant of the BERT language\nmodel. Given a requirements specification, TAPHSIR decides for each pronoun\noccurrence in the specification whether the pronoun is ambiguous or\nunambiguous, and further provides an automatic interpretation for the pronoun.\nThe output generated by TAPHSIR can be easily reviewed and validated by\nrequirements engineers. TAPHSIR is publicly available on Zenodo (DOI:\n10.5281/zenodo.5902117).\n",
                "Link": "http://arxiv.org/pdf/2206.10227v1",
                "arxiv_id": "2206.10227v1",
                "publicationDate": "2022-06-21T09:53:13Z"
            },
            {
                "Title": "The Referential Reader: A Recurrent Entity Network for Anaphora\n  Resolution",
                "Authors": "Fei Liu, Luke Zettlemoyer, Jacob Eisenstein",
                "Abstract": "  We present a new architecture for storing and accessing entity mentions\nduring online text processing. While reading the text, entity references are\nidentified, and may be stored by either updating or overwriting a cell in a\nfixed-length memory. The update operation implies coreference with the other\nmentions that are stored in the same cell; the overwrite operation causes these\nmentions to be forgotten. By encoding the memory operations as differentiable\ngates, it is possible to train the model end-to-end, using both a supervised\nanaphora resolution objective as well as a supplementary language modeling\nobjective. Evaluation on a dataset of pronoun-name anaphora demonstrates strong\nperformance with purely incremental text processing.\n",
                "Link": "http://arxiv.org/pdf/1902.01541v2",
                "arxiv_id": "1902.01541v2",
                "publicationDate": "2019-02-05T04:41:55Z"
            },
            {
                "Title": "Resolution of Indirect Anaphora in Japanese Sentences Using Examples 'X\n  no Y (Y of X)'",
                "Authors": "M. Murata, H. Isahara, M. Nagao",
                "Abstract": "  A noun phrase can indirectly refer to an entity that has already been\nmentioned. For example, ``I went into an old house last night. The roof was\nleaking badly and ...'' indicates that ``the roof'' is associated with `` an\nold house}'', which was mentioned in the previous sentence. This kind of\nreference (indirect anaphora) has not been studied well in natural language\nprocessing, but is important for coherence resolution, language understanding,\nand machine translation. In order to analyze indirect anaphora, we need a case\nframe dictionary for nouns that contains knowledge of the relationships between\ntwo nouns but no such dictionary presently exists. Therefore, we are forced to\nuse examples of ``X no Y'' (Y of X) and a verb case frame dictionary instead.\nWe tried estimating indirect anaphora using this information and obtained a\nrecall rate of 63% and a precision rate of 68% on test sentences. This\nindicates that the information of ``X no Y'' is useful to a certain extent when\nwe cannot make use of a noun case frame dictionary. We estimated the results\nthat would be given by a noun case frame dictionary, and obtained recall and\nprecision rates of 71% and 82% respectively. Finally, we proposed a way to\nconstruct a noun case frame dictionary by using examples of ``X no Y.''\n",
                "Link": "http://arxiv.org/pdf/cs/9912003v1",
                "arxiv_id": "9912003v1",
                "publicationDate": "1999-12-13T04:42:25Z"
            },
            {
                "Title": "Scoring Coreference Chains with Split-Antecedent Anaphors",
                "Authors": "Silviu Paun, Juntao Yu, Nafise Sadat Moosavi, Massimo Poesio",
                "Abstract": "  Anaphoric reference is an aspect of language interpretation covering a\nvariety of types of interpretation beyond the simple case of identity reference\nto entities introduced via nominal expressions covered by the traditional\ncoreference task in its most recent incarnation in ONTONOTES and similar\ndatasets. One of these cases that go beyond simple coreference is anaphoric\nreference to entities that must be added to the discourse model via\naccommodation, and in particular split-antecedent references to entities\nconstructed out of other entities, as in split-antecedent plurals and in some\ncases of discourse deixis. Although this type of anaphoric reference is now\nannotated in many datasets, systems interpreting such references cannot be\nevaluated using the Reference coreference scorer Pradhan et al. (2014). As part\nof the work towards a new scorer for anaphoric reference able to evaluate all\naspects of anaphoric interpretation in the coverage of the Universal Anaphora\ninitiative, we propose in this paper a solution to the technical problem of\ngeneralizing existing metrics for identity anaphora so that they can also be\nused to score cases of split-antecedents. This is the first such proposal in\nthe literature on anaphora or coreference, and has been successfully used to\nscore both split-antecedent plural references and discourse deixis in the\nrecent CODI/CRAC anaphora resolution in dialogue shared tasks.\n",
                "Link": "http://arxiv.org/pdf/2205.12323v1",
                "arxiv_id": "2205.12323v1",
                "publicationDate": "2022-05-24T19:07:36Z"
            },
            {
                "Title": "A Predictive Model for Notional Anaphora in English",
                "Authors": "Amir Zeldes",
                "Abstract": "  Notional anaphors are pronouns which disagree with their antecedents'\ngrammatical categories for notional reasons, such as plural to singular\nagreement in: 'the government ... they'. Since such cases are rare and conflict\nwith evidence from strictly agreeing cases ('the government ... it'), they\npresent a substantial challenge to both coreference resolution and referring\nexpression generation. Using the OntoNotes corpus, this paper takes an ensemble\napproach to predicting English notional anaphora in context on the basis of the\nlargest empirical data to date. In addition to state of the art prediction\naccuracy, the results suggest that theoretical approaches positing a plural\nconstrual at the antecedent's utterance are insufficient, and that\ncircumstances at the anaphor's utterance location, as well as global factors\nsuch as genre, have a strong effect on the choice of referring expression.\n",
                "Link": "http://arxiv.org/pdf/1804.07375v1",
                "arxiv_id": "1804.07375v1",
                "publicationDate": "2018-04-19T20:48:53Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Anaphora and Coreference Resolution: A Review",
                "Authors": "Rhea Sukthanker, Soujanya Poria, Erik Cambria, Ramkumar Thirunavukarasu",
                "Abstract": "  Entity resolution aims at resolving repeated references to an entity in a\ndocument and forms a core component of natural language processing (NLP)\nresearch. This field possesses immense potential to improve the performance of\nother NLP fields like machine translation, sentiment analysis, paraphrase\ndetection, summarization, etc. The area of entity resolution in NLP has seen\nproliferation of research in two separate sub-areas namely: anaphora resolution\nand coreference resolution. Through this review article, we aim at clarifying\nthe scope of these two tasks in entity resolution. We also carry out a detailed\nanalysis of the datasets, evaluation metrics and research methods that have\nbeen adopted to tackle this NLP problem. This survey is motivated with the aim\nof providing the reader with a clear understanding of what constitutes this NLP\nproblem and the issues that require attention.\n",
                "Link": "http://arxiv.org/pdf/1805.11824v1",
                "arxiv_id": "1805.11824v1",
                "publicationDate": "2018-05-30T06:49:15Z"
            },
            {
                "Title": "Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022\n  Shared Task)",
                "Authors": "Tatiana Anikina, Natalia Skachkova, Joseph Renner, Priyansh Trivedi",
                "Abstract": "  We describe three models submitted for the CODI-CRAC 2022 shared task. To\nperform identity anaphora resolution, we test several combinations of the\nincremental clustering approach based on the Workspace Coreference System (WCS)\nwith other coreference models. The best result is achieved by adding the\n''cluster merging'' version of the coref-hoi model, which brings up to 10.33%\nimprovement 1 over vanilla WCS clustering. Discourse deixis resolution is\nimplemented as multi-task learning: we combine the learning objective of\ncorefhoi with anaphor type classification. We adapt the higher-order resolution\nmodel introduced in Joshi et al. (2019) for bridging resolution given gold\nmentions and anaphors.\n",
                "Link": "http://arxiv.org/pdf/2301.02113v1",
                "arxiv_id": "2301.02113v1",
                "publicationDate": "2023-01-05T15:42:17Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "A Cluster Ranking Model for Full Anaphora Resolution",
                "Authors": "Juntao Yu, Alexandra Uma, Massimo Poesio",
                "Abstract": "  Anaphora resolution (coreference) systems designed for the CONLL 2012 dataset\ntypically cannot handle key aspects of the full anaphora resolution task such\nas the identification of singletons and of certain types of non-referring\nexpressions (e.g., expletives), as these aspects are not annotated in that\ncorpus. However, the recently released dataset for the CRAC 2018 Shared Task\ncan now be used for that purpose. In this paper, we introduce an architecture\nto simultaneously identify non-referring expressions (including expletives,\npredicative s, and other types) and build coreference chains, including\nsingletons. Our cluster-ranking system uses an attention mechanism to determine\nthe relative importance of the mentions in the same cluster. Additional\nclassifiers are used to identify singletons and non-referring markables. Our\ncontributions are as follows. First all, we report the first result on the CRAC\ndata using system mentions; our result is 5.8% better than the shared task\nbaseline system, which used gold mentions. Second, we demonstrate that the\navailability of singleton clusters and non-referring expressions can lead to\nsubstantially improved performance on non-singleton clusters as well. Third, we\nshow that despite our model not being designed specifically for the CONLL data,\nit achieves a score equivalent to that of the state-of-the-art system by Kantor\nand Globerson (2019) on that dataset.\n",
                "Link": "http://arxiv.org/pdf/1911.09532v2",
                "arxiv_id": "1911.09532v2",
                "publicationDate": "2019-11-21T15:14:39Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Processing Complex Sentences in the Centering Framework",
                "Authors": "Michael Strube",
                "Abstract": "  We extend the centering model for the resolution of intra-sentential anaphora\nand specify how to handle complex sentences. An empirical evaluation indicates\nthat the functional information structure guides the search for an antecedent\nwithin the sentence.\n",
                "Link": "http://arxiv.org/pdf/cmp-lg/9605022v1",
                "arxiv_id": "9605022v1",
                "publicationDate": "1996-05-14T08:30:05Z"
            },
            {
                "Title": "A Probabilistic Method for Analyzing Japanese Anaphora Integrating Zero\n  Pronoun Detection and Resolution",
                "Authors": "Kazuhiro Seki, Atsushi Fujii, Tetsuya Ishikawa",
                "Abstract": "  This paper proposes a method to analyze Japanese anaphora, in which zero\npronouns (omitted obligatory cases) are used to refer to preceding entities\n(antecedents). Unlike the case of general coreference resolution, zero pronouns\nhave to be detected prior to resolution because they are not expressed in\ndiscourse. Our method integrates two probability parameters to perform zero\npronoun detection and resolution in a single framework. The first parameter\nquantifies the degree to which a given case is a zero pronoun. The second\nparameter quantifies the degree to which a given entity is the antecedent for a\ndetected zero pronoun. To compute these parameters efficiently, we use corpora\nwith/without annotations of anaphoric relations. We show the effectiveness of\nour method by way of experiments.\n",
                "Link": "http://arxiv.org/pdf/cs/0206030v1",
                "arxiv_id": "0206030v1",
                "publicationDate": "2002-06-20T07:13:59Z"
            },
            {
                "Title": "Adapted End-to-End Coreference Resolution System for Anaphoric\n  Identities in Dialogues",
                "Authors": "Liyan Xu, Jinho D. Choi",
                "Abstract": "  We present an effective system adapted from the end-to-end neural coreference\nresolution model, targeting on the task of anaphora resolution in dialogues.\nThree aspects are specifically addressed in our approach, including the support\nof singletons, encoding speakers and turns throughout dialogue interactions,\nand knowledge transfer utilizing existing resources. Despite the simplicity of\nour adaptation strategies, they are shown to bring significant impact to the\nfinal performance, with up to 27 F1 improvement over the baseline. Our final\nsystem ranks the 1st place on the leaderboard of the anaphora resolution track\nin the CRAC 2021 shared task, and achieves the best evaluation results on all\nfour datasets.\n",
                "Link": "http://arxiv.org/pdf/2109.00185v2",
                "arxiv_id": "2109.00185v2",
                "publicationDate": "2021-09-01T04:51:29Z"
            },
            {
                "Title": "Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution",
                "Authors": "Juntao Yu, Nafise Sadat Moosavi, Silviu Paun, Massimo Poesio",
                "Abstract": "  Now that the performance of coreference resolvers on the simpler forms of\nanaphoric reference has greatly improved, more attention is devoted to more\ncomplex aspects of anaphora. One limitation of virtually all coreference\nresolution models is the focus on single-antecedent anaphors. Plural anaphors\nwith multiple antecedents-so-called split-antecedent anaphors (as in John met\nMary. They went to the movies) have not been widely studied, because they are\nnot annotated in ONTONOTES and are relatively infrequent in other corpora. In\nthis paper, we introduce the first model for unrestricted resolution of\nsplit-antecedent anaphors. We start with a strong baseline enhanced by BERT\nembeddings, and show that we can substantially improve its performance by\naddressing the sparsity issue. To do this, we experiment with auxiliary corpora\nwhere split-antecedent anaphors were annotated by the crowd, and with transfer\nlearning models using element-of bridging references and single-antecedent\ncoreference as auxiliary tasks. Evaluation on the gold annotated ARRAU corpus\nshows that the out best model uses a combination of three auxiliary corpora\nachieved F1 scores of 70% and 43.6% when evaluated in a lenient and strict\nsetting, respectively, i.e., 11 and 21 percentage points gain when compared\nwith our baseline.\n",
                "Link": "http://arxiv.org/pdf/2011.00245v1",
                "arxiv_id": "2011.00245v1",
                "publicationDate": "2020-10-31T11:21:39Z"
            },
            {
                "Title": "Handling Verb Phrase Anaphora with Dependent Types and Events",
                "Authors": "Daniyar Itegulov, Ekaterina Lebedeva",
                "Abstract": "  This paper studies how dependent typed events can be used to treat verb\nphrase anaphora. We introduce a framework that extends Dependent Type Semantics\n(DTS) with a new atomic type for neo-Davidsonian events and an extended\n@-operator that can return new events that share properties of events\nreferenced by verb phrase anaphora. The proposed framework, along with\nillustrative examples of its use, are presented after a brief overview of the\nnecessary background and of the major challenges posed by verb phrase anaphora.\n",
                "Link": "http://arxiv.org/pdf/1803.10421v1",
                "arxiv_id": "1803.10421v1",
                "publicationDate": "2018-03-28T05:47:31Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1",
                "publicationDate": "2010-09-23T11:42:41Z"
            },
            {
                "Title": "Analysis of the Effect of Dependency Information on Predicate-Argument\n  Structure Analysis and Zero Anaphora Resolution",
                "Authors": "Koichiro Yoshino, Shinsuke Mori, Satoshi Nakamura",
                "Abstract": "  This paper investigates and analyzes the effect of dependency information on\npredicate-argument structure analysis (PASA) and zero anaphora resolution (ZAR)\nfor Japanese, and shows that a straightforward approach of PASA and ZAR works\neffectively even if dependency information was not available. We constructed an\nanalyzer that directly predicts relationships of predicates and arguments with\ntheir semantic roles from a POS-tagged corpus. The features of the system are\ndesigned to compensate for the absence of syntactic information by using\nfeatures used in dependency parsing as a reference. We also constructed\nanalyzers that use the oracle dependency and the real dependency parsing\nresults, and compared with the system that does not use any syntactic\ninformation to verify that the improvement provided by dependencies is not\ncrucial.\n",
                "Link": "http://arxiv.org/pdf/1705.10962v1",
                "arxiv_id": "1705.10962v1",
                "publicationDate": "2017-05-31T07:32:29Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "A Quantum Natural Language Processing Approach to Pronoun Resolution",
                "Authors": "Hadi Wazni, Kin Ian Lo, Lachlan McPheat, Mehrnoosh Sadrzadeh",
                "Abstract": "  We use the Lambek Calculus with soft sub-exponential modalities to model and\nreason about discourse relations such as anaphora and ellipsis. A semantics for\nthis logic is obtained by using truncated Fock spaces, developed in our\nprevious work. We depict these semantic computations via a new string diagram.\nThe Fock Space semantics has the advantage that its terms are learnable from\nlarge corpora of data using machine learning and they can be experimented with\non mainstream natural language tasks. Further, and thanks to an existing\ntranslation from vector spaces to quantum circuits, we can also learn these\nterms on quantum computers and their simulators, such as the IBMQ range. We\nextend the existing translation to Fock spaces and develop quantum circuit\nsemantics for discourse relations. We then experiment with the IBMQ\nAerSimulations of these circuits in a definite pronoun resolution task, where\nthe highest accuracies were recorded for models when the anaphora was resolved.\n",
                "Link": "http://arxiv.org/pdf/2208.05393v1",
                "arxiv_id": "2208.05393v1",
                "publicationDate": "2022-08-10T15:22:58Z"
            },
            {
                "Title": "An Empirical Study of Contextual Data Augmentation for Japanese Zero\n  Anaphora Resolution",
                "Authors": "Ryuto Konno, Yuichiroh Matsubayashi, Shun Kiyono, Hiroki Ouchi, Ryo Takahashi, Kentaro Inui",
                "Abstract": "  One critical issue of zero anaphora resolution (ZAR) is the scarcity of\nlabeled data. This study explores how effectively this problem can be\nalleviated by data augmentation. We adopt a state-of-the-art data augmentation\nmethod, called the contextual data augmentation (CDA), that generates labeled\ntraining instances using a pretrained language model. The CDA has been reported\nto work well for several other natural language processing tasks, including\ntext classification and machine translation. This study addresses two\nunderexplored issues on CDA, that is, how to reduce the computational cost of\ndata augmentation and how to ensure the quality of the generated data. We also\npropose two methods to adapt CDA to ZAR: [MASK]-based augmentation and\nlinguistically-controlled masking. Consequently, the experimental results on\nJapanese ZAR show that our methods contribute to both the accuracy gain and the\ncomputation cost reduction. Our closer analysis reveals that the proposed\nmethod can improve the quality of the augmented training data when compared to\nthe conventional CDA.\n",
                "Link": "http://arxiv.org/pdf/2011.00948v2",
                "arxiv_id": "2011.00948v2",
                "publicationDate": "2020-11-02T13:05:00Z"
            },
            {
                "Title": "Visualizing Domain Ontology using Enhanced Anaphora Resolution Algorithm",
                "Authors": "L. Jegatha Deborah, R. Baskaran, A. Kannan",
                "Abstract": "  Enormous explosion in the number of the World Wide Web pages occur every day\nand since the efficiency of most of the information processing systems is found\nto be less, the potential of the Internet applications is often underutilized.\nEfficient utilization of the web can be exploited when similar web pages are\nrigorously, exhaustively organized and clustered based on some domain knowledge\n(semantic-based) .Ontology which is a formal representation of domain knowledge\naids in such efficient utilization. The performance of almost all the\nsemantic-based clustering techniques depends on the constructed ontology,\ndescribing the domain knowledge . The proposed methodology provides an enhanced\npronominal anaphora resolution, one of the key aspects of semantic analysis in\nNatural Language Processing for obtaining cross references within a web page\nproviding better ontology construction. The experimental data sets exhibits\nbetter efficiency of the proposed method compared to earlier traditional\nalgorithms.\n",
                "Link": "http://arxiv.org/pdf/1109.2321v1",
                "arxiv_id": "1109.2321v1",
                "publicationDate": "2011-09-11T15:08:11Z"
            }
        ]
    },
    {
        "topic_name": "Code-Switching Detection",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1",
                "publicationDate": "2023-11-10T17:38:46Z"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1",
                "publicationDate": "2023-10-17T02:12:12Z"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1",
                "publicationDate": "2023-11-22T08:25:15Z"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1",
                "publicationDate": "2021-11-12T09:38:15Z"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1",
                "publicationDate": "2020-05-29T15:38:54Z"
            },
            {
                "Title": "BanglaSarc: A Dataset for Sarcasm Detection",
                "Authors": "Tasnim Sakib Apon, Ramisa Anan, Elizabeth Antora Modhu, Arjun Suter, Ifrit Jamal Sneha, MD. Golam Rabiul Alam",
                "Abstract": "  Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.\n",
                "Link": "http://arxiv.org/pdf/2209.13461v1",
                "arxiv_id": "2209.13461v1",
                "publicationDate": "2022-09-27T15:28:21Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1",
                "publicationDate": "2023-10-13T13:25:16Z"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1",
                "publicationDate": "2018-09-04T11:55:34Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking",
                "Authors": "Md Muzakker Hossain, Zahin Awosaf, Md. Salman Hossan Prottoy, Abu Saleh Muhammod Alvy, Md. Kishor Morol",
                "Abstract": "  Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n",
                "Link": "http://arxiv.org/pdf/2203.11486v1",
                "arxiv_id": "2203.11486v1",
                "publicationDate": "2022-03-22T06:33:01Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1",
                "publicationDate": "2023-11-25T13:47:34Z"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1",
                "publicationDate": "2024-03-31T09:52:25Z"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1",
                "publicationDate": "2019-11-19T20:37:03Z"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1",
                "publicationDate": "2020-04-19T07:42:22Z"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1",
                "publicationDate": "2021-12-03T13:35:18Z"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1",
                "publicationDate": "2022-06-01T10:10:15Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1",
                "publicationDate": "2023-05-31T04:08:57Z"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1",
                "publicationDate": "2024-04-17T21:09:13Z"
            },
            {
                "Title": "BDSL 49: A Comprehensive Dataset of Bangla Sign Language",
                "Authors": "Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein",
                "Abstract": "  Language is a method by which individuals express their thoughts. Each\nlanguage has its own set of alphabetic and numeric characters. People can\ncommunicate with one another through either oral or written communication.\nHowever, each language has a sign language counterpart. Individuals who are\ndeaf and/or mute communicate through sign language. The Bangla language also\nhas a sign language, which is called BDSL. The dataset is about Bangla hand\nsign images. The collection contains 49 individual Bangla alphabet images in\nsign language. BDSL49 is a dataset that consists of 29,490 images with 49\nlabels. Images of 14 different adult individuals, each with a distinct\nbackground and appearance, have been recorded during data collection. Several\nstrategies have been used to eliminate noise from datasets during preparation.\nThis dataset is available to researchers for free. They can develop automated\nsystems using machine learning, computer vision, and deep learning techniques.\nIn addition, two models were used in this dataset. The first is for detection,\nwhile the second is for recognition.\n",
                "Link": "http://arxiv.org/pdf/2208.06827v1",
                "arxiv_id": "2208.06827v1",
                "publicationDate": "2022-08-14T10:54:49Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1",
                "publicationDate": "2023-09-27T14:10:57Z"
            },
            {
                "Title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT",
                "Authors": "Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim",
                "Abstract": "  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n",
                "Link": "http://arxiv.org/pdf/2403.05519v1",
                "arxiv_id": "2403.05519v1",
                "publicationDate": "2024-03-08T18:42:59Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1",
                "publicationDate": "2019-01-17T04:27:34Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1",
                "publicationDate": "2010-09-23T11:42:41Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
                "Authors": "Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam",
                "Abstract": "  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n",
                "Link": "http://arxiv.org/pdf/2303.12772v1",
                "arxiv_id": "2303.12772v1",
                "publicationDate": "2023-03-22T17:35:35Z"
            },
            {
                "Title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection",
                "Authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor Ryen, Arman Hossain, Faisal Bin Ashraf",
                "Abstract": "  Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.\n",
                "Link": "http://arxiv.org/pdf/2102.02478v1",
                "arxiv_id": "2102.02478v1",
                "publicationDate": "2021-02-04T08:35:18Z"
            },
            {
                "Title": "Textual Toxicity in Social Media: Understanding the Bangla Toxic\n  Language Expressed in Facebook Comment",
                "Authors": "Mohammad Mamun Or Rashid",
                "Abstract": "  Social Media is a repository of digital literature including user-generated\ncontent. The users of social media are expressing their opinion with diverse\nmediums such as text, emojis, memes, and also through other visual and textual\nmediums. A major portion of these media elements could be treated as harmful to\nothers and they are known by many words including Cyberbullying and Toxic\nLanguage . The goal of this research paper is to analyze a curated and\nvalue-added dataset of toxic language titled ToxLex_bn . It is an exhaustive\nwordlist that can be used as classifier material to detect toxicity in social\nmedia. The toxic language/script used by the Bengali community as\ncyberbullying, hate speech and moral policing became major trends in social\nmedia culture in Bangladesh and West Bengal. The toxicity became so high that\nthe victims has to post as a counter or release explanation video for the\nhaters. Most cases are pointed to women celebrity and their relation, dress,\nlifestyle are became trolled and toxicity flooded in comments boxes. Not only\ncelebrity bashing but also hates occurred between Hindu Muslims,\nIndia-Bangladesh, Two opponents of 1971 and these are very common for virtual\nconflict in the comment thread. Even many times facebook comment causes sue and\nlegal matters in Bangladesh and thus it requires more study. In this study, a\nBangla toxic language dataset has been analyzed which was inputted by the user\nin Bengali script & language. For this, about 1968 unique bigrams or phrases as\nwordlists have been analyzed which are derived from 2207590 comments. It is\nassumed that this analysis will reinforce the detection of Bangla's toxic\nlanguage used in social media and thus cure this virtual disease.\n",
                "Link": "http://arxiv.org/pdf/2312.05467v1",
                "arxiv_id": "2312.05467v1",
                "publicationDate": "2023-12-09T05:04:34Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1",
                "publicationDate": "2022-01-25T05:27:57Z"
            },
            {
                "Title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language\n  Models for Violence Inciting Text Detection",
                "Authors": "Saurabh Page, Sudeep Mangalvedhekar, Kshitij Deshpande, Tanmay Chavan, Sheetal Sonawane",
                "Abstract": "  This paper presents our work for the Violence Inciting Text Detection shared\ntask in the First Workshop on Bangla Language Processing. Social media has\naccelerated the propagation of hate and violence-inciting speech in society. It\nis essential to develop efficient mechanisms to detect and curb the propagation\nof such texts. The problem of detecting violence-inciting texts is further\nexacerbated in low-resource settings due to sparse research and less data. The\ndata provided in the shared task consists of texts in the Bangla language,\nwhere each example is classified into one of the three categories defined based\non the types of violence-inciting texts. We try and evaluate several BERT-based\nmodels, and then use an ensemble of the models as our final submission. Our\nsubmission is ranked 10th in the final leaderboard of the shared task with a\nmacro F1 score of 0.737.\n",
                "Link": "http://arxiv.org/pdf/2311.18778v1",
                "arxiv_id": "2311.18778v1",
                "publicationDate": "2023-11-30T18:23:38Z"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4",
                "publicationDate": "2021-01-01T09:28:45Z"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1",
                "publicationDate": "2012-03-05T12:22:23Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1",
                "publicationDate": "2010-09-25T06:55:27Z"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1",
                "publicationDate": "2012-01-10T10:33:18Z"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1",
                "publicationDate": "2014-10-08T10:01:47Z"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1",
                "publicationDate": "2012-03-05T12:06:54Z"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2",
                "publicationDate": "2022-08-20T15:21:35Z"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1",
                "publicationDate": "2010-09-26T02:09:41Z"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1",
                "publicationDate": "2021-09-24T05:31:01Z"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1",
                "publicationDate": "2021-05-31T20:39:35Z"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1",
                "publicationDate": "2019-07-18T01:00:42Z"
            }
        ]
    },
    {
        "topic_name": "Domain Adaptation",
        "summary": "default",
        "papers": [
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3",
                "publicationDate": "2023-05-11T06:27:38Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1",
                "publicationDate": "2022-05-28T15:39:09Z"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1",
                "publicationDate": "2018-09-02T14:03:30Z"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1",
                "publicationDate": "2023-11-06T13:02:07Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1",
                "publicationDate": "2023-09-24T15:51:39Z"
            },
            {
                "Title": "BanglaSarc: A Dataset for Sarcasm Detection",
                "Authors": "Tasnim Sakib Apon, Ramisa Anan, Elizabeth Antora Modhu, Arjun Suter, Ifrit Jamal Sneha, MD. Golam Rabiul Alam",
                "Abstract": "  Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.\n",
                "Link": "http://arxiv.org/pdf/2209.13461v1",
                "arxiv_id": "2209.13461v1",
                "publicationDate": "2022-09-27T15:28:21Z"
            },
            {
                "Title": "Investigating self-supervised, weakly supervised and fully supervised\n  training approaches for multi-domain automatic speech recognition: a study on\n  Bangladeshi Bangla",
                "Authors": "Ahnaf Mozib Samin, M. Humayon Kobir, Md. Mushtaq Shahriyar Rafee, M. Firoz Ahmed, Mehedi Hasan, Partha Ghosh, Shafkat Kibria, M. Shahidur Rahman",
                "Abstract": "  Despite huge improvements in automatic speech recognition (ASR) employing\nneural networks, ASR systems still suffer from a lack of robustness and\ngeneralizability issues due to domain shifting. This is mainly because\nprincipal corpus design criteria are often not identified and examined\nadequately while compiling ASR datasets. In this study, we investigate the\nrobustness of the state-of-the-art transfer learning approaches such as\nself-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully\nsupervised convolutional neural networks (CNNs) for multi-domain ASR. We also\ndemonstrate the significance of domain selection while building a corpus by\nassessing these models on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains approximately 6.52 hours of\nhuman-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO,\na mostly read speech corpus for the morphologically rich language Bangla, has\nbeen used to train the ASR systems. Experimental evaluation reveals that\nself-supervised cross-lingual pre-training is the best strategy compared to\nweak supervision and full supervision to tackle the multi-domain ASR task.\nMoreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech\nfrom domains with mostly spontaneous speech. The BanSpeech will be publicly\navailable to meet the need for a challenging evaluation benchmark for Bangla\nASR.\n",
                "Link": "http://arxiv.org/pdf/2210.12921v3",
                "arxiv_id": "2210.12921v3",
                "publicationDate": "2022-10-24T02:18:03Z"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1",
                "publicationDate": "2023-10-22T10:55:56Z"
            },
            {
                "Title": "Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition",
                "Authors": "Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  One of the major challenges for developing automatic speech recognition (ASR)\nfor low-resource languages is the limited access to labeled data with\ndomain-specific variations. In this study, we propose a pseudo-labeling\napproach to develop a large-scale domain-agnostic ASR dataset. With the\nproposed methodology, we developed a 20k+ hours labeled Bangla speech dataset\ncovering diverse topics, speaking styles, dialects, noisy environments, and\nconversational scenarios. We then exploited the developed corpus to design a\nconformer-based ASR system. We benchmarked the trained ASR with publicly\navailable datasets and compared it with other available models. To investigate\nthe efficacy, we designed and developed a human-annotated domain-agnostic test\nset composed of news, telephony, and conversational data among others. Our\nresults demonstrate the efficacy of the model trained on psuedo-label data for\nthe designed test-set along with publicly-available Bangla datasets. The\nexperimental resources will be publicly\navailable.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)\n",
                "Link": "http://arxiv.org/pdf/2311.03196v1",
                "arxiv_id": "2311.03196v1",
                "publicationDate": "2023-11-06T15:37:14Z"
            },
            {
                "Title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation",
                "Authors": "Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin",
                "Abstract": "  This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.\n",
                "Link": "http://arxiv.org/pdf/2306.06147v1",
                "arxiv_id": "2306.06147v1",
                "publicationDate": "2023-06-09T12:07:10Z"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1",
                "publicationDate": "2023-11-07T08:20:06Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1",
                "publicationDate": "2010-09-23T11:42:41Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition",
                "Authors": "Beiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-Hua Ling, Quan Liu",
                "Abstract": "  This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n",
                "Link": "http://arxiv.org/pdf/2203.03216v2",
                "arxiv_id": "2203.03216v2",
                "publicationDate": "2022-03-07T09:05:37Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1",
                "publicationDate": "2022-01-25T05:27:57Z"
            },
            {
                "Title": "A Harmony Search Based Wrapper Feature Selection Method for Holistic\n  Bangla word Recognition",
                "Authors": "Supratim Das, Pawan Kumar Singh, Showmik Bhowmik, Ram Sarkar, Mita Nasipuri",
                "Abstract": "  A lot of search approaches have been explored for the selection of features\nin pattern classification domain in order to discover significant subset of the\nfeatures which produces better accuracy. In this paper, we introduced a Harmony\nSearch (HS) algorithm based feature selection method for feature dimensionality\nreduction in handwritten Bangla word recognition problem. This algorithm has\nbeen implemented to reduce the feature dimensionality of a technique described\nin one of our previous papers by S. Bhowmik et al.[1]. In the said paper, a set\nof 65 elliptical features were computed for handwritten Bangla word recognition\npurpose and a recognition accuracy of 81.37% was achieved using Multi Layer\nPerceptron (MLP) classifier. In the present work, a subset containing 48\nfeatures (approximately 75% of said feature vector) has been selected by HS\nbased wrapper feature selection method which produces an accuracy rate of\n90.29%. Reasonable outcomes also validates that the introduced algorithm\nutilizes optimal number of features while showing higher classification\naccuracies when compared to two standard evolutionary algorithms like Genetic\nAlgorithm (GA), Particle Swarm Optimization (PSO) and statistical feature\ndimensionality reduction technique like Principal Component Analysis (PCA).\nThis confirms the suitability of HS algorithm to the holistic handwritten word\nrecognition problem.\n",
                "Link": "http://arxiv.org/pdf/1707.08398v1",
                "arxiv_id": "1707.08398v1",
                "publicationDate": "2017-07-26T12:03:39Z"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4",
                "publicationDate": "2021-01-01T09:28:45Z"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1",
                "publicationDate": "2012-03-05T12:22:23Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1",
                "publicationDate": "2010-09-25T06:55:27Z"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1",
                "publicationDate": "2012-01-10T10:33:18Z"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1",
                "publicationDate": "2014-10-08T10:01:47Z"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1",
                "publicationDate": "2012-03-05T12:06:54Z"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2",
                "publicationDate": "2022-08-20T15:21:35Z"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1",
                "publicationDate": "2010-09-26T02:09:41Z"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1",
                "publicationDate": "2021-09-24T05:31:01Z"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1",
                "publicationDate": "2021-05-31T20:39:35Z"
            },
            {
                "Title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language",
                "Authors": "Aunabil Chakma, Masum Hasan",
                "Abstract": "  This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023\n",
                "Link": "http://arxiv.org/pdf/2311.12735v1",
                "arxiv_id": "2311.12735v1",
                "publicationDate": "2023-11-21T17:21:15Z"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1",
                "publicationDate": "2022-04-21T08:40:17Z"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1",
                "publicationDate": "2021-06-14T08:26:50Z"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6",
                "publicationDate": "2021-07-12T16:09:22Z"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2",
                "publicationDate": "2010-02-22T02:58:49Z"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1",
                "publicationDate": "2023-11-10T17:38:46Z"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2",
                "publicationDate": "2024-01-16T01:08:19Z"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1",
                "publicationDate": "2017-01-27T18:43:31Z"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1",
                "publicationDate": "2023-10-17T02:12:12Z"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1",
                "publicationDate": "2014-10-02T08:26:38Z"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1",
                "publicationDate": "2021-03-14T12:11:21Z"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1",
                "publicationDate": "2021-11-12T09:38:15Z"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4",
                "publicationDate": "2022-05-23T06:54:56Z"
            }
        ]
    },
    {
        "topic_name": "Text Summarization",
        "summary": "default",
        "papers": [
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1",
                "publicationDate": "2019-11-15T08:22:33Z"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1",
                "publicationDate": "2021-09-24T05:31:01Z"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1",
                "publicationDate": "2014-01-06T20:25:26Z"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1",
                "publicationDate": "2014-10-08T10:01:47Z"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1",
                "publicationDate": "2023-11-22T08:25:15Z"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4",
                "publicationDate": "2021-01-01T09:28:45Z"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1",
                "publicationDate": "2012-03-05T12:22:23Z"
            },
            {
                "Title": "Pointer over Attention: An Improved Bangla Text Summarization Approach\n  Using Hybrid Pointer Generator Network",
                "Authors": "Nobel Dhar, Gaurob Saha, Prithwiraj Bhattacharjee, Avi Mallick, Md Saiful Islam",
                "Abstract": "  Despite the success of the neural sequence-to-sequence model for abstractive\ntext summarization, it has a few shortcomings, such as repeating inaccurate\nfactual details and tending to repeat themselves. We propose a hybrid pointer\ngenerator network to solve the shortcomings of reproducing factual details\ninadequately and phrase repetition. We augment the attention-based\nsequence-to-sequence using a hybrid pointer generator network that can generate\nOut-of-Vocabulary words and enhance accuracy in reproducing authentic details\nand a coverage mechanism that discourages repetition. It produces a\nreasonable-sized output text that preserves the conceptual integrity and\nfactual information of the input article. For evaluation, we primarily employed\n\"BANSData\" - a highly adopted publicly available Bengali dataset. Additionally,\nwe prepared a large-scale dataset called \"BANS-133\" which consists of 133k\nBangla news articles associated with human-generated summaries. Experimenting\nwith the proposed model, we achieved ROUGE-1 and ROUGE-2 scores of 0.66, 0.41\nfor the \"BANSData\" dataset and 0.67, 0.42 for the BANS-133k\" dataset,\nrespectively. We demonstrated that the proposed system surpasses previous\nstate-of-the-art Bengali abstractive summarization techniques and its stability\non a larger dataset. \"BANS-133\" datasets and code-base will be publicly\navailable for research.\n",
                "Link": "http://arxiv.org/pdf/2111.10269v2",
                "arxiv_id": "2111.10269v2",
                "publicationDate": "2021-11-19T15:18:12Z"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1",
                "publicationDate": "2021-05-31T20:39:35Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1",
                "publicationDate": "2022-05-28T15:39:09Z"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1",
                "publicationDate": "2017-01-27T18:43:31Z"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1",
                "publicationDate": "2021-11-12T09:38:15Z"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4",
                "publicationDate": "2022-05-23T06:54:56Z"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1",
                "publicationDate": "2022-09-13T17:59:21Z"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2",
                "publicationDate": "2016-10-02T23:45:23Z"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1",
                "publicationDate": "2023-11-06T13:02:07Z"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2",
                "publicationDate": "2024-01-25T18:06:19Z"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3",
                "publicationDate": "2021-07-08T13:49:46Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1",
                "publicationDate": "2020-05-29T15:38:54Z"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1",
                "publicationDate": "2024-01-30T17:47:07Z"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2",
                "publicationDate": "2023-03-16T13:31:31Z"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1",
                "publicationDate": "2023-10-22T10:55:56Z"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3",
                "publicationDate": "2017-12-28T14:31:56Z"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1",
                "publicationDate": "2023-08-21T22:18:09Z"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1",
                "publicationDate": "2010-02-21T19:48:16Z"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1",
                "publicationDate": "2012-06-02T13:23:18Z"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1",
                "publicationDate": "2022-05-29T22:56:26Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1",
                "publicationDate": "2023-11-25T13:47:34Z"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1",
                "publicationDate": "2023-11-07T08:20:06Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1",
                "publicationDate": "2022-10-19T21:53:49Z"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1",
                "publicationDate": "2021-12-03T13:35:18Z"
            },
            {
                "Title": "Authorship Attribution in Bangla literature using Character-level CNN",
                "Authors": "Aisha Khatun, Anisur Rahman, Md. Saiful Islam,  Marium-E-Jannat",
                "Abstract": "  Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results.\n",
                "Link": "http://arxiv.org/pdf/2001.05316v1",
                "arxiv_id": "2001.05316v1",
                "publicationDate": "2020-01-11T14:54:04Z"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1",
                "publicationDate": "2021-08-01T17:16:03Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1",
                "publicationDate": "2019-11-19T20:37:03Z"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1",
                "publicationDate": "2023-05-31T04:08:57Z"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1",
                "publicationDate": "2024-04-17T21:09:13Z"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1",
                "publicationDate": "2023-09-27T14:10:57Z"
            },
            {
                "Title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT",
                "Authors": "Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim",
                "Abstract": "  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n",
                "Link": "http://arxiv.org/pdf/2403.05519v1",
                "arxiv_id": "2403.05519v1",
                "publicationDate": "2024-03-08T18:42:59Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language\n  Models for Violence Inciting Text Detection",
                "Authors": "Saurabh Page, Sudeep Mangalvedhekar, Kshitij Deshpande, Tanmay Chavan, Sheetal Sonawane",
                "Abstract": "  This paper presents our work for the Violence Inciting Text Detection shared\ntask in the First Workshop on Bangla Language Processing. Social media has\naccelerated the propagation of hate and violence-inciting speech in society. It\nis essential to develop efficient mechanisms to detect and curb the propagation\nof such texts. The problem of detecting violence-inciting texts is further\nexacerbated in low-resource settings due to sparse research and less data. The\ndata provided in the shared task consists of texts in the Bangla language,\nwhere each example is classified into one of the three categories defined based\non the types of violence-inciting texts. We try and evaluate several BERT-based\nmodels, and then use an ensemble of the models as our final submission. Our\nsubmission is ranked 10th in the final leaderboard of the shared task with a\nmacro F1 score of 0.737.\n",
                "Link": "http://arxiv.org/pdf/2311.18778v1",
                "arxiv_id": "2311.18778v1",
                "publicationDate": "2023-11-30T18:23:38Z"
            },
            {
                "Title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation\n  Language Models",
                "Authors": "Mitodru Niyogi, Arnab Bhattacharya",
                "Abstract": "  We present Gyan AI Paramanu (\"atom\"), a family of novel language models for\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\nand multilingual Indic language models pretrained from scratch on a single GPU\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\npretrained with a context size of 1024 on a single GPU. The models are very\nefficient, small, fast, and powerful. We have also developed an efficient most\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\navoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we\npretrained on comparable corpora by typological grouping using the same script.\nWe performed human evaluation of our pretrained models for open end text\ngeneration on grammar, coherence, creativity, and factuality metrics for\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\nmodels on 23k instructions in respective languages. Our pretrained and\ninstruction-tuned models which are first of its kind, most powerful efficient\nsmall generative language models ever developed for Indic languages, and the\nvarious results lead to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and humongous number\nof parameters. We plan to release our models at https://www.bharatgpts.com.\n",
                "Link": "http://arxiv.org/pdf/2401.18034v1",
                "arxiv_id": "2401.18034v1",
                "publicationDate": "2024-01-31T17:58:10Z"
            },
            {
                "Title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP",
                "Authors": "Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque",
                "Abstract": "  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.\n",
                "Link": "http://arxiv.org/pdf/2309.13173v2",
                "arxiv_id": "2309.13173v2",
                "publicationDate": "2023-09-22T20:29:34Z"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1",
                "publicationDate": "2019-01-17T04:27:34Z"
            }
        ]
    },
    {
        "topic_name": "Opinion Mining",
        "summary": "default",
        "papers": [
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1",
                "publicationDate": "2010-09-23T11:42:41Z"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1",
                "publicationDate": "2010-09-25T06:55:27Z"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1",
                "publicationDate": "2010-09-26T02:09:41Z"
            },
            {
                "Title": "An Opinion Mining of Text in COVID-19 Issues along with Comparative\n  Study in ML, BERT & RNN",
                "Authors": "Md. Mahadi Hasan Sany, Mumenunnesa Keya, Sharun Akter Khushbu, Akm Shahariar Azad Rabby, Abu Kaisar Mohammad Masum",
                "Abstract": "  The global world is crossing a pandemic situation where this is a\ncatastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a\nglobal threat all over the 212 countries that people every day meet with mighty\nsituations. On the contrary, thousands of infected people live rich in\nmountains. Mental health is also affected by this worldwide coronavirus\nsituation. Due to this situation online sources made a communicative place that\ncommon people shares their opinion in any agenda. Such as affected news related\npositive and negative, financial issues, country and family crisis, lack of\nimport and export earning system etc. different kinds of circumstances are\nrecent trendy news in anywhere. Thus, vast amounts of text are produced within\nmoments therefore, in subcontinent areas the same as situation in other\ncountries and peoples opinion of text and situation also same but the language\nis different. This article has proposed some specific inputs along with Bangla\ntext comments from individual sources which can assure the goal of illustration\nthat machine learning outcome capable of building an assistive system. Opinion\nmining assistive system can be impactful in all language preferences possible.\nTo the best of our knowledge, the article predicted the Bangla input text on\nCOVID-19 issues proposed ML algorithms and deep learning models analysis also\ncheck the future reachability with a comparative analysis. Comparative analysis\nstates a report on text prediction accuracy is 91% along with ML algorithms and\n79% along with Deep Learning Models.\n",
                "Link": "http://arxiv.org/pdf/2201.02119v1",
                "arxiv_id": "2201.02119v1",
                "publicationDate": "2022-01-06T15:59:20Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Comparative Opinion Mining: A Review",
                "Authors": "Kasturi Dewi Varathan, Anastasia Giachanou, Fabio Crestani",
                "Abstract": "  Opinion mining refers to the use of natural language processing, text\nanalysis and computational linguistics to identify and extract subjective\ninformation in textual material. Opinion mining, also known as sentiment\nanalysis, has received a lot of attention in recent times, as it provides a\nnumber of tools to analyse the public opinion on a number of different topics.\nComparative opinion mining is a subfield of opinion mining that deals with\nidentifying and extracting information that is expressed in a comparative form\n(e.g.~\"paper X is better than the Y\"). Comparative opinion mining plays a very\nimportant role when ones tries to evaluate something, as it provides a\nreference point for the comparison. This paper provides a review of the area of\ncomparative opinion mining. It is the first review that cover specifically this\ntopic as all previous reviews dealt mostly with general opinion mining. This\nsurvey covers comparative opinion mining from two different angles. One from\nperspective of techniques and the other from perspective of comparative opinion\nelements. It also incorporates preprocessing tools as well as dataset that were\nused by the past researchers that can be useful to the future researchers in\nthe field of comparative opinion mining.\n",
                "Link": "http://arxiv.org/pdf/1712.08941v1",
                "arxiv_id": "1712.08941v1",
                "publicationDate": "2017-12-24T16:16:07Z"
            },
            {
                "Title": "Opinion Mining and Analysis: A survey",
                "Authors": "Arti Buche, Dr. M. B. Chandak, Akshay Zadgaonkar",
                "Abstract": "  The current research is focusing on the area of Opinion Mining also called as\nsentiment analysis due to sheer volume of opinion rich web resources such as\ndiscussion forums, review sites and blogs are available in digital form. One\nimportant problem in sentiment analysis of product reviews is to produce\nsummary of opinions based on product features. We have surveyed and analyzed in\nthis paper, various techniques that have been developed for the key tasks of\nopinion mining. We have provided an overall picture of what is involved in\ndeveloping a software system for opinion mining on the basis of our survey and\nanalysis.\n",
                "Link": "http://arxiv.org/pdf/1307.3336v1",
                "arxiv_id": "1307.3336v1",
                "publicationDate": "2013-07-12T06:20:36Z"
            },
            {
                "Title": "Opinion Mining Using Population-tuned Generative Language Models",
                "Authors": "Allmin Susaiyah, Abhinay Pandya, Aki H\u00e4rm\u00e4",
                "Abstract": "  We present a novel method for mining opinions from text collections using\ngenerative language models trained on data collected from different\npopulations. We describe the basic definitions, methodology and a generic\nalgorithm for opinion insight mining. We demonstrate the performance of our\nmethod in an experiment where a pre-trained generative model is fine-tuned\nusing specifically tailored content with unnatural and fully annotated\nopinions. We show that our approach can learn and transfer the opinions to the\nsemantic classes while maintaining the proportion of polarisation. Finally, we\ndemonstrate the usage of an insight mining system to scale up the discovery of\nopinion insights from a real text corpus.\n",
                "Link": "http://arxiv.org/pdf/2307.13173v1",
                "arxiv_id": "2307.13173v1",
                "publicationDate": "2023-07-24T23:42:32Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1",
                "publicationDate": "2021-08-01T17:16:03Z"
            },
            {
                "Title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by\n  Leveraging Hashtags and Sentiment Lexicon",
                "Authors": "Kar Wai Lim, Wray Buntine",
                "Abstract": "  Aspect-based opinion mining is widely applied to review data to aggregate or\nsummarize opinions of a product, and the current state-of-the-art is achieved\nwith Latent Dirichlet Allocation (LDA)-based model. Although social media data\nlike tweets are laden with opinions, their \"dirty\" nature (as natural language)\nhas discouraged researchers from applying LDA-based opinion model for product\nreview mining. Tweets are often informal, unstructured and lacking labeled data\nsuch as categories and ratings, making it challenging for product opinion\nmining. In this paper, we propose an LDA-based opinion model named Twitter\nOpinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM\nleverages hashtags, mentions, emoticons and strong sentiment words that are\npresent in tweets in its discovery process. It improves opinion prediction by\nmodeling the target-opinion interaction directly, thus discovering target\nspecific opinion words, neglected in existing approaches. Moreover, we propose\na new formulation of incorporating sentiment prior information into a topic\nmodel, by utilizing an existing public sentiment lexicon. This is novel in that\nit learns and updates with the data. We conduct experiments on 9 million tweets\non electronic products, and demonstrate the improved performance of TOTM in\nboth quantitative evaluations and qualitative analysis. We show that\naspect-based opinion analysis on massive volume of tweets provides useful\nopinions on products.\n",
                "Link": "http://arxiv.org/pdf/1609.06578v1",
                "arxiv_id": "1609.06578v1",
                "publicationDate": "2016-09-21T14:25:23Z"
            },
            {
                "Title": "Subjectivity Classification using Machine Learning Techniques for Mining\n  Feature-Opinion Pairs from Web Opinion Sources",
                "Authors": "Ahmad Kamal",
                "Abstract": "  Due to flourish of the Web 2.0, web opinion sources are rapidly emerging\ncontaining precious information useful for both customers and manufactures.\nRecently, feature based opinion mining techniques are gaining momentum in which\ncustomer reviews are processed automatically for mining product features and\nuser opinions expressed over them. However, customer reviews may contain both\nopinionated and factual sentences. Distillations of factual contents improve\nmining performance by preventing noisy and irrelevant extraction. In this\npaper, combination of both supervised machine learning and rule-based\napproaches are proposed for mining feasible feature-opinion pairs from\nsubjective review sentences. In the first phase of the proposed approach, a\nsupervised machine learning technique is applied for classifying subjective and\nobjective sentences from customer reviews. In the next phase, a rule based\nmethod is implemented which applies linguistic and semantic analysis of texts\nto mine feasible feature-opinion pairs from subjective sentences retained after\nthe first phase. The effectiveness of the proposed methods is established\nthrough experimentation over customer reviews on different electronic products.\n",
                "Link": "http://arxiv.org/pdf/1312.6962v1",
                "arxiv_id": "1312.6962v1",
                "publicationDate": "2013-12-25T12:38:17Z"
            },
            {
                "Title": "A Framework of Customer Review Analysis Using the Aspect-Based Opinion\n  Mining Approach",
                "Authors": "Subhasis Dasgupta, Jaydip Sen",
                "Abstract": "  Opinion mining is the branch of computation that deals with opinions,\nappraisals, attitudes, and emotions of people and their different aspects. This\nfield has attracted substantial research interest in recent years. Aspect-level\n(called aspect-based opinion mining) is often desired in practical applications\nas it provides detailed opinions or sentiments about different aspects of\nentities and entities themselves, which are usually required for action. Aspect\nextraction and entity extraction are thus two core tasks of aspect-based\nopinion mining. his paper has presented a framework of aspect-based opinion\nmining based on the concept of transfer learning. on real-world customer\nreviews available on the Amazon website. The model has yielded quite\nsatisfactory results in its task of aspect-based opinion mining.\n",
                "Link": "http://arxiv.org/pdf/2212.10051v1",
                "arxiv_id": "2212.10051v1",
                "publicationDate": "2022-12-20T07:54:58Z"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1",
                "publicationDate": "2023-08-21T22:18:09Z"
            },
            {
                "Title": "Sentiment Analysis: A Survey",
                "Authors": "Rahul Tejwani",
                "Abstract": "  Sentiment analysis (also known as opinion mining) refers to the use of\nnatural language processing, text analysis and computational linguistics to\nidentify and extract subjective information in source materials. Mining\nopinions expressed in the user generated content is a challenging yet\npractically very useful problem. This survey would cover various approaches and\nmethodology used in Sentiment Analysis and Opinion Mining in general. The focus\nwould be on Internet text like, Product review, tweets and other social media.\n",
                "Link": "http://arxiv.org/pdf/1405.2584v1",
                "arxiv_id": "1405.2584v1",
                "publicationDate": "2014-05-11T21:05:28Z"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1",
                "publicationDate": "2022-10-19T21:53:49Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "Constructing Explainable Opinion Graphs from Review",
                "Authors": "Nofar Carmeli, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, Yuliang Li, Jinfeng Li, Wang-Chiew Tan",
                "Abstract": "  The Web is a major resource of both factual and subjective information. While\nthere are significant efforts to organize factual information into knowledge\nbases, there is much less work on organizing opinions, which are abundant in\nsubjective data, into a structured format.\n  We present ExplainIt, a system that extracts and organizes opinions into an\nopinion graph, which are useful for downstream applications such as generating\nexplainable review summaries and facilitating search over opinion phrases. In\nsuch graphs, a node represents a set of semantically similar opinions extracted\nfrom reviews and an edge between two nodes signifies that one node explains the\nother. ExplainIt mines explanations in a supervised method and groups similar\nopinions together in a weakly supervised way before combining the clusters of\nopinions together with their explanation relationships into an opinion graph.\nWe experimentally demonstrate that the explanation relationships generated in\nthe opinion graph are of good quality and our labeled datasets for explanation\nmining and grouping opinions are publicly available.\n",
                "Link": "http://arxiv.org/pdf/2006.00119v2",
                "arxiv_id": "2006.00119v2",
                "publicationDate": "2020-05-29T23:11:48Z"
            },
            {
                "Title": "Review Mining for Feature Based Opinion Summarization and Visualization",
                "Authors": "Ahmad Kamal",
                "Abstract": "  The application and usage of opinion mining, especially for business\nintelligence, product recommendation, targeted marketing etc. have fascinated\nmany research attentions around the globe. Various research efforts attempted\nto mine opinions from customer reviews at different levels of granularity,\nincluding word-, sentence-, and document-level. However, development of a fully\nautomatic opinion mining and sentiment analysis system is still elusive. Though\nthe development of opinion mining and sentiment analysis systems are getting\nmomentum, most of them attempt to perform document-level sentiment analysis,\nclassifying a review document as positive, negative, or neutral. Such\ndocument-level opinion mining approaches fail to provide insight about users\nsentiment on individual features of a product or service. Therefore, it seems\nto be a great help for both customers and manufacturers, if the reviews could\nbe processed at a finer-grained level and presented in a summarized form\nthrough some visual means, highlighting individual features of a product and\nusers sentiment expressed over them. In this paper, the design of a unified\nopinion mining and sentiment analysis framework is presented at the\nintersection of both machine learning and natural language processing\napproaches. Also, design of a novel feature-level review summarization scheme\nis proposed to visualize mined features, opinions and their polarity values in\na comprehendible way.\n",
                "Link": "http://arxiv.org/pdf/1504.03068v2",
                "arxiv_id": "1504.03068v2",
                "publicationDate": "2015-04-13T05:53:59Z"
            },
            {
                "Title": "Opinion Mining from YouTube Captions Using ChatGPT: A Case Study of\n  Street Interviews Polling the 2023 Turkish Elections",
                "Authors": "Tu\u011frulcan Elmas, \u0130lker G\u00fcl",
                "Abstract": "  Opinion mining plays a critical role in understanding public sentiment and\npreferences, particularly in the context of political elections. Traditional\npolling methods, while useful, can be expensive and less scalable. Social media\noffers an alternative source of data for opinion mining but presents challenges\nsuch as noise, biases, and platform limitations in data collection. In this\npaper, we propose a novel approach for opinion mining, utilizing YouTube's\nauto-generated captions from public interviews as a data source, specifically\nfocusing on the 2023 Turkish elections as a case study. We introduce an opinion\nmining framework using ChatGPT to mass-annotate voting intentions and\nmotivations that represent the stance and frames prior to the election. We\nreport that ChatGPT can predict the preferred candidate with 97\\% accuracy and\nidentify the correct voting motivation out of 13 possible choices with 71\\%\naccuracy based on the data collected from 325 interviews. We conclude by\ndiscussing the robustness of our approach, accounting for factors such as\ncaptions quality, interview length, and channels. This new method will offer a\nless noisy and cost-effective alternative for opinion mining using social media\ndata.\n",
                "Link": "http://arxiv.org/pdf/2304.03434v1",
                "arxiv_id": "2304.03434v1",
                "publicationDate": "2023-04-07T01:25:22Z"
            },
            {
                "Title": "ECO v1: Towards Event-Centric Opinion Mining",
                "Authors": "Ruoxi Xu, Hongyu Lin, Meng Liao, Xianpei Han, Jin Xu, Wei Tan, Yingfei Sun, Le Sun",
                "Abstract": "  Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.\n",
                "Link": "http://arxiv.org/pdf/2203.12264v1",
                "arxiv_id": "2203.12264v1",
                "publicationDate": "2022-03-23T08:20:45Z"
            },
            {
                "Title": "Textual Toxicity in Social Media: Understanding the Bangla Toxic\n  Language Expressed in Facebook Comment",
                "Authors": "Mohammad Mamun Or Rashid",
                "Abstract": "  Social Media is a repository of digital literature including user-generated\ncontent. The users of social media are expressing their opinion with diverse\nmediums such as text, emojis, memes, and also through other visual and textual\nmediums. A major portion of these media elements could be treated as harmful to\nothers and they are known by many words including Cyberbullying and Toxic\nLanguage . The goal of this research paper is to analyze a curated and\nvalue-added dataset of toxic language titled ToxLex_bn . It is an exhaustive\nwordlist that can be used as classifier material to detect toxicity in social\nmedia. The toxic language/script used by the Bengali community as\ncyberbullying, hate speech and moral policing became major trends in social\nmedia culture in Bangladesh and West Bengal. The toxicity became so high that\nthe victims has to post as a counter or release explanation video for the\nhaters. Most cases are pointed to women celebrity and their relation, dress,\nlifestyle are became trolled and toxicity flooded in comments boxes. Not only\ncelebrity bashing but also hates occurred between Hindu Muslims,\nIndia-Bangladesh, Two opponents of 1971 and these are very common for virtual\nconflict in the comment thread. Even many times facebook comment causes sue and\nlegal matters in Bangladesh and thus it requires more study. In this study, a\nBangla toxic language dataset has been analyzed which was inputted by the user\nin Bengali script & language. For this, about 1968 unique bigrams or phrases as\nwordlists have been analyzed which are derived from 2207590 comments. It is\nassumed that this analysis will reinforce the detection of Bangla's toxic\nlanguage used in social media and thus cure this virtual disease.\n",
                "Link": "http://arxiv.org/pdf/2312.05467v1",
                "arxiv_id": "2312.05467v1",
                "publicationDate": "2023-12-09T05:04:34Z"
            },
            {
                "Title": "Jointly identifying opinion mining elements and fuzzy measurement of\n  opinion intensity to analyze product features",
                "Authors": "Haiqing Zhang, Aicha Sekhari, Yacine Ouzrout, Abdelaziz Bouras",
                "Abstract": "  Opinion mining mainly involves three elements: feature and feature-of\nrelations, opinion expressions and the related opinion attributes (e.g.\nPolarity), and feature-opinion relations. Although many works have emerged to\nachieve its aim of gaining information, the previous researches typically\nhandled each of the three elements in isolation, which cannot give sufficient\ninformation extraction results; hence, the complexity and the running time of\ninformation extraction is increased. In this paper, we propose an opinion\nmining extraction algorithm to jointly discover the main opinion mining\nelements. Specifically, the algorithm automatically builds kernels to combine\nclosely related words into new terms from word level to phrase level based on\ndependency relations; and we ensure the accuracy of opinion expressions and\npolarity based on: fuzzy measurements, opinion degree intensifiers, and opinion\npatterns. The 3458 analyzed reviews show that the proposed algorithm can\neffectively identify the main elements simultaneously and outperform the\nbaseline methods. The proposed algorithm is used to analyze the features among\nheterogeneous products in the same category. The feature-by-feature comparison\ncan help to select the weaker features and recommend the correct specifications\nfrom the beginning life of a product. From this comparison, some interesting\nobservations are revealed. For example, the negative polarity of video\ndimension is higher than the product usability dimension for a product. Yet,\nenhancing the dimension of product usability can more effectively improve the\nproduct (C) 2015 Elsevier Ltd. All rights reserved.\n",
                "Link": "http://arxiv.org/pdf/1811.05827v1",
                "arxiv_id": "1811.05827v1",
                "publicationDate": "2018-11-13T09:27:24Z"
            },
            {
                "Title": "Opinion-aware Answer Generation for Review-driven Question Answering in\n  E-Commerce",
                "Authors": "Yang Deng, Wenxuan Zhang, Wai Lam",
                "Abstract": "  Product-related question answering (QA) is an important but challenging task\nin E-Commerce. It leads to a great demand on automatic review-driven QA, which\naims at providing instant responses towards user-posted questions based on\ndiverse product reviews. Nevertheless, the rich information about personal\nopinions in product reviews, which is essential to answer those\nproduct-specific questions, is underutilized in current generation-based\nreview-driven QA studies. There are two main challenges when exploiting the\nopinion information from the reviews to facilitate the opinion-aware answer\ngeneration: (i) jointly modeling opinionated and interrelated information\nbetween the question and reviews to capture important information for answer\ngeneration, (ii) aggregating diverse opinion information to uncover the common\nopinion towards the given question. In this paper, we tackle opinion-aware\nanswer generation by jointly learning answer generation and opinion mining\ntasks with a unified model. Two kinds of opinion fusion strategies, namely,\nstatic and dynamic fusion, are proposed to distill and aggregate important\nopinion information learned from the opinion mining task into the answer\ngeneration process. Then a multi-view pointer-generator network is employed to\ngenerate opinion-aware answers for a given product-related question.\nExperimental results show that our method achieves superior performance in\nreal-world E-Commerce QA datasets, and effectively generate opinionated and\ninformative answers.\n",
                "Link": "http://arxiv.org/pdf/2008.11972v2",
                "arxiv_id": "2008.11972v2",
                "publicationDate": "2020-08-27T07:54:45Z"
            },
            {
                "Title": "Confirmatory Aspect-based Opinion Mining Processes",
                "Authors": "Jongho Im, Taikgun Song, Youngsu Lee, Jewoo Kim",
                "Abstract": "  A new opinion extraction method is proposed to summarize unstructured,\nuser-generated content (i.e., online customer reviews) in the fixed topic\ndomains. To differentiate the current approach from other opinion extraction\napproaches, which are often exposed to a sparsity problem and lack of sentiment\nscores, a confirmatory aspect-based opinion mining framework is introduced\nalong with its practical algorithm called DiSSBUS. In this procedure, 1) each\ncustomer review is disintegrated into a set of clauses; 2) each clause is\nsummarized to bi-terms-a topic word and an evaluation word-using a\npart-of-speech (POS) tagger; and 3) each bi-term is matched to a pre-specified\ntopic relevant to a specific domain. The proposed processes have two primary\nadvantages over existing methods: 1) they can decompose a single review into a\nset of bi-terms related to pre-specified topics in the domain of interest and,\ntherefore, 2) allow identification of the reviewer's opinions on the topics via\nevaluation words within the set of bi-terms. The proposed aspect-based opinion\nmining is applied to customer reviews of restaurants in Hawaii obtained from\nTripAdvisor, and the empirical findings validate the effectiveness of the\nmethod.\n  Keywords: Clause-based sentiment analysis, Customer review, Opinion mining,\nTopic modeling, User-generate-contents.\n",
                "Link": "http://arxiv.org/pdf/1907.12850v1",
                "arxiv_id": "1907.12850v1",
                "publicationDate": "2019-07-30T12:00:03Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "Over a Decade of Social Opinion Mining: A Systematic Review",
                "Authors": "Keith Cortis, Brian Davis",
                "Abstract": "  Social media popularity and importance is on the increase due to people using\nit for various types of social interaction across multiple channels. This\nsystematic review focuses on the evolving research area of Social Opinion\nMining, tasked with the identification of multiple opinion dimensions, such as\nsubjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from\nuser-generated content represented across multiple social media platforms and\nin various media formats, like text, image, video and audio. Through Social\nOpinion Mining, natural language can be understood in terms of the different\nopinion dimensions, as expressed by humans. This contributes towards the\nevolution of Artificial Intelligence which in turn helps the advancement of\nseveral real-world use cases, such as customer service and decision making. A\nthorough systematic review was carried out on Social Opinion Mining research\nwhich totals 485 published studies and spans a period of twelve years between\n2007 and 2018. The in-depth analysis focuses on the social media platforms,\ntechniques, social datasets, language, modality, tools and technologies, and\nother aspects derived. Social Opinion Mining can be utilised in many\napplication areas, ranging from marketing, advertising and sales for\nproduct/service management, and in multiple domains and industries, such as\npolitics, technology, finance, healthcare, sports and government. The latest\ndevelopments in Social Opinion Mining beyond 2018 are also presented together\nwith future research directions, with the aim of leaving a wider academic and\nsocietal impact in several real-world applications.\n",
                "Link": "http://arxiv.org/pdf/2012.03091v2",
                "arxiv_id": "2012.03091v2",
                "publicationDate": "2020-12-05T17:59:59Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Opinion Mining In Hindi Language: A Survey",
                "Authors": "Richa Sharma, Shweta Nigam, Rekha Jain",
                "Abstract": "  Opinions are very important in the life of human beings. These Opinions\nhelped the humans to carry out the decisions. As the impact of the Web is\nincreasing day by day, Web documents can be seen as a new source of opinion for\nhuman beings. Web contains a huge amount of information generated by the users\nthrough blogs, forum entries, and social networking websites and so on To\nanalyze this large amount of information it is required to develop a method\nthat automatically classifies the information available on the Web. This domain\nis called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment\nAnalysis is a natural language processing task that mine information from\nvarious text forms such as reviews, news, and blogs and classify them on the\nbasis of their polarity as positive, negative or neutral. But, from the last\nfew years, enormous increase has been seen in Hindi language on the Web.\nResearch in opinion mining mostly carried out in English language but it is\nvery important to perform the opinion mining in Hindi language also as large\namount of information in Hindi is also available on the Web. This paper gives\nan overview of the work that has been done Hindi language.\n",
                "Link": "http://arxiv.org/pdf/1404.4935v1",
                "arxiv_id": "1404.4935v1",
                "publicationDate": "2014-04-19T08:14:39Z"
            },
            {
                "Title": "Snippext: Semi-supervised Opinion Mining with Augmented Data",
                "Authors": "Zhengjie Miao, Yuliang Li, Xiaolan Wang, Wang-Chiew Tan",
                "Abstract": "  Online services are interested in solutions to opinion mining, which is the\nproblem of extracting aspects, opinions, and sentiments from text. One method\nto mine opinions is to leverage the recent success of pre-trained language\nmodels which can be fine-tuned to obtain high-quality extractions from reviews.\nHowever, fine-tuning language models still requires a non-trivial amount of\ntraining data. In this paper, we study the problem of how to significantly\nreduce the amount of labeled training data required in fine-tuning language\nmodels for opinion mining. We describe Snippext, an opinion mining system\ndeveloped over a language model that is fine-tuned through semi-supervised\nlearning with augmented data. A novelty of Snippext is its clever use of a\ntwo-prong approach to achieve state-of-the-art (SOTA) performance with little\nlabeled training data through: (1) data augmentation to automatically generate\nmore labeled training data from existing ones, and (2) a semi-supervised\nlearning technique to leverage the massive amount of unlabeled data in addition\nto the (limited amount of) labeled data. We show with extensive experiments\nthat Snippext performs comparably and can even exceed previous SOTA results on\nseveral opinion mining tasks with only half the training data required.\nFurthermore, it achieves new SOTA results when all training data are leveraged.\nBy comparison to a baseline pipeline, we found that Snippext extracts\nsignificantly more fine-grained opinions which enable new opportunities of\ndownstream applications.\n",
                "Link": "http://arxiv.org/pdf/2002.03049v1",
                "arxiv_id": "2002.03049v1",
                "publicationDate": "2020-02-07T23:54:23Z"
            },
            {
                "Title": "Architecture of Text Mining Application in Analyzing Public Sentiments\n  of West Java Governor Election using Naive Bayes Classification",
                "Authors": "Suryanto Nugroho,  Prihandoko",
                "Abstract": "  The selection of West Java governor is one event that seizes the attention of\nthe public is no exception to social media users. Public opinion on a\nprospective regional leader can help predict electability and tendency of\nvoters. Data that can be used by the opinion mining process can be obtained\nfrom Twitter. Because the data is very varied form and very unstructured, it\nmust be managed and uninformed using data pre-processing techniques into\nsemi-structured data. This semi-structured information is followed by a\nclassification stage to categorize the opinion into negative or positive\nopinions. The research methodology uses a literature study where the research\nwill examine previous research on a similar topic. The purpose of this study is\nto find the right architecture to develop it into the application of twitter\nopinion mining to know public sentiments toward the election of the governor of\nwest java. The result of this research is that Twitter opinion mining is part\nof text mining where opinions in Twitter if they want to be classified, must go\nthrough the preprocessing text stage first. The preprocessing step required\nfrom twitter data is cleansing, case folding, POS Tagging and stemming. The\nresulting text mining architecture is an architecture that can be used for text\nmining research with different topics.\n",
                "Link": "http://arxiv.org/pdf/1810.07767v1",
                "arxiv_id": "1810.07767v1",
                "publicationDate": "2018-09-20T07:14:10Z"
            },
            {
                "Title": "Sentiment Analysis Using Collaborated Opinion Mining",
                "Authors": "Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi",
                "Abstract": "  Opinion mining and Sentiment analysis have emerged as a field of study since\nthe widespread of World Wide Web and internet. Opinion refers to extraction of\nthose lines or phrase in the raw and huge data which express an opinion.\nSentiment analysis on the other hand identifies the polarity of the opinion\nbeing extracted. In this paper we propose the sentiment analysis in\ncollaboration with opinion extraction, summarization, and tracking the records\nof the students. The paper modifies the existing algorithm in order to obtain\nthe collaborated opinion about the students. The resultant opinion is\nrepresented as very high, high, moderate, low and very low. The paper is based\non a case study where teachers give their remarks about the students and by\napplying the proposed sentiment analysis algorithm the opinion is extracted and\nrepresented.\n",
                "Link": "http://arxiv.org/pdf/1401.2618v1",
                "arxiv_id": "1401.2618v1",
                "publicationDate": "2014-01-12T12:35:57Z"
            },
            {
                "Title": "From the Token to the Review: A Hierarchical Multimodal approach to\n  Opinion Mining",
                "Authors": "Alexandre Garcia, Pierre Colombo, Slim Essid, Florence d'Alch\u00e9-Buc, Chlo\u00e9 Clavel",
                "Abstract": "  The task of predicting fine grained user opinion based on spontaneous spoken\nlanguage is a key problem arising in the development of Computational Agents as\nwell as in the development of social network based opinion miners.\nUnfortunately, gathering reliable data on which a model can be trained is\nnotoriously difficult and existing works rely only on coarsely labeled\nopinions. In this work we aim at bridging the gap separating fine grained\nopinion models already developed for written language and coarse grained models\ndeveloped for spontaneous multimodal opinion mining. We take advantage of the\nimplicit hierarchical structure of opinions to build a joint fine and coarse\ngrained opinion model that exploits different views of the opinion expression.\nThe resulting model shares some properties with attention-based models and is\nshown to provide competitive results on a recently released multimodal fine\ngrained annotated corpus.\n",
                "Link": "http://arxiv.org/pdf/1908.11216v3",
                "arxiv_id": "1908.11216v3",
                "publicationDate": "2019-08-29T13:34:50Z"
            },
            {
                "Title": "Detecting Anchors' Opinion in Hinghlish News Delivery",
                "Authors": "Siddharth Sadhwani, Nishant Grover, Md Akhtar, Tanmoy Chakraborty",
                "Abstract": "  Humans like to express their opinions and crave the opinions of others.\nMining and detecting opinions from various sources are beneficial to\nindividuals, organisations, and even governments. One such organisation is news\nmedia, where a general norm is not to showcase opinions from their side.\nAnchors are the face of the digital media, and it is required for them not to\nbe opinionated. However, at times, they diverge from the accepted norm and\ninsert their opinions into otherwise straightforward news reports, either\npurposefully or unintentionally. This is primarily seen in debates as it\nrequires the anchors to be spontaneous, thus making them vulnerable to add\ntheir opinions. The consequence of such mishappening might lead to biased news\nor even supporting a certain agenda at the worst. To this end, we propose a\nnovel task of anchors' opinion detection in debates. We curate code-mixed news\ndebates and develop the ODIN dataset. A total of 2054 anchors' utterances in\nthe dataset are marked as opinionated or non-opinionated. Lastly, we propose\nDetONADe, an interactive attention-based framework for classifying anchors'\nutterances and obtain the best weighted-F1 score of 0.703. A thorough analysis\nand evaluation show many interesting patterns in the dataset and predictions.\n",
                "Link": "http://arxiv.org/pdf/2204.02155v1",
                "arxiv_id": "2204.02155v1",
                "publicationDate": "2022-04-05T12:26:46Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "Opinion Polarity Identification through Adjectives",
                "Authors": "Samaneh Moghaddam, Fred Popowich",
                "Abstract": "  \"What other people think\" has always been an important piece of information\nduring various decision-making processes. Today people frequently make their\nopinions available via the Internet, and as a result, the Web has become an\nexcellent source for gathering consumer opinions. There are now numerous Web\nresources containing such opinions, e.g., product reviews forums, discussion\ngroups, and Blogs. But, due to the large amount of information and the wide\nrange of sources, it is essentially impossible for a customer to read all of\nthe reviews and make an informed decision on whether to purchase the product.\nIt is also difficult for the manufacturer or seller of a product to accurately\nmonitor customer opinions. For this reason, mining customer reviews, or opinion\nmining, has become an important issue for research in Web information\nextraction. One of the important topics in this research area is the\nidentification of opinion polarity. The opinion polarity of a review is usually\nexpressed with values 'positive', 'negative' or 'neutral'. We propose a\ntechnique for identifying polarity of reviews by identifying the polarity of\nthe adjectives that appear in them. Our evaluation shows the technique can\nprovide accuracy in the area of 73%, which is well above the 58%-64% provided\nby naive Bayesian classifiers.\n",
                "Link": "http://arxiv.org/pdf/1011.4623v1",
                "arxiv_id": "1011.4623v1",
                "publicationDate": "2010-11-21T00:15:27Z"
            },
            {
                "Title": "Mining Public Opinion about Economic Issues: Twitter and the U.S.\n  Presidential Election",
                "Authors": "Amir Karami, London S. Bennett, Xiaoyun He",
                "Abstract": "  Opinion polls have been the bridge between public opinion and politicians in\nelections. However, developing surveys to disclose people's feedback with\nrespect to economic issues is limited, expensive, and time-consuming. In recent\nyears, social media such as Twitter has enabled people to share their opinions\nregarding elections. Social media has provided a platform for collecting a\nlarge amount of social media data. This paper proposes a computational public\nopinion mining approach to explore the discussion of economic issues in social\nmedia during an election. Current related studies use text mining methods\nindependently for election analysis and election prediction; this research\ncombines two text mining methods: sentiment analysis and topic modeling. The\nproposed approach has effectively been deployed on millions of tweets to\nanalyze economic concerns of people during the 2012 US presidential election.\n",
                "Link": "http://arxiv.org/pdf/1802.01786v1",
                "arxiv_id": "1802.01786v1",
                "publicationDate": "2018-02-06T03:55:37Z"
            },
            {
                "Title": "ARAACOM: ARAbic Algerian Corpus for Opinion Mining",
                "Authors": "Zitouni Abdelhafid, Hichem Rahab, Abdelhafid Zitouni, Mahieddine Djoudi",
                "Abstract": "  Nowadays, it is no more needed to do an enormous effort to distribute a lot\nof forms to thousands of people and collect them, then convert this from into\nelectronic format to track people opinion about some subjects. A lot of web\nsites can today reach a large spectrum with less effort. The majority of web\nsites suggest to their visitors to leave backups about their feeling of the\nsite or events. So, this makes for us a lot of data which need powerful mean to\nexploit. Opinion mining in the web becomes more and more an attracting task,\ndue the increasing need for individuals and societies to track the mood of\npeople against several subjects of daily life (sports, politics,\ntelevision,...). A lot of works in opinion mining was developed in western\nlanguages especially English, such works in Arabic language still very scarce.\nIn this paper, we propose our approach, for opinion mining in Arabic Algerian\nnews paper. CCS CONCEPTS $\\bullet$Information systems~Sentiment analysis\n$\\bullet$ Computing methodologies~Natural language processing\n",
                "Link": "http://arxiv.org/pdf/2001.08010v1",
                "arxiv_id": "2001.08010v1",
                "publicationDate": "2020-01-22T13:45:34Z"
            },
            {
                "Title": "A Machine Learning Approach For Opinion Holder Extraction In Arabic\n  Language",
                "Authors": "Mohamed Elarnaoty, Samir AbdelRahman, Aly Fahmy",
                "Abstract": "  Opinion mining aims at extracting useful subjective information from reliable\namounts of text. Opinion mining holder recognition is a task that has not been\nconsidered yet in Arabic Language. This task essentially requires deep\nunderstanding of clauses structures. Unfortunately, the lack of a robust,\npublicly available, Arabic parser further complicates the research. This paper\npresents a leading research for the opinion holder extraction in Arabic news\nindependent from any lexical parsers. We investigate constructing a\ncomprehensive feature set to compensate the lack of parsing structural\noutcomes. The proposed feature set is tuned from English previous works coupled\nwith our proposed semantic field and named entities features. Our feature\nanalysis is based on Conditional Random Fields (CRF) and semi-supervised\npattern recognition techniques. Different research models are evaluated via\ncross-validation experiments achieving 54.03 F-measure. We publicly release our\nown research outcome corpus and lexicon for opinion mining community to\nencourage further research.\n",
                "Link": "http://arxiv.org/pdf/1206.1011v1",
                "arxiv_id": "1206.1011v1",
                "publicationDate": "2012-04-06T20:50:59Z"
            },
            {
                "Title": "Robopinion: Opinion Mining Framework Inspired by Autonomous Robot\n  Navigation",
                "Authors": "M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny",
                "Abstract": "  Data association methods are used by autonomous robots to find matches\nbetween the current landmarks and the new set of observed features. We seek a\nframework for opinion mining to benefit from advancements in autonomous robot\nnavigation in both research and development\n",
                "Link": "http://arxiv.org/pdf/1209.0249v1",
                "arxiv_id": "1209.0249v1",
                "publicationDate": "2012-09-03T06:00:04Z"
            },
            {
                "Title": "Opinion Mining on Non-English Short Text",
                "Authors": "Esra Akbas",
                "Abstract": "  As the type and the number of such venues increase, automated analysis of\nsentiment on textual resources has become an essential data mining task. In\nthis paper, we investigate the problem of mining opinions on the collection of\ninformal short texts. Both positive and negative sentiment strength of texts\nare detected. We focus on a non-English language that has few resources for\ntext mining. This approach would help enhance the sentiment analysis in\nlanguages where a list of opinionated words does not exist. We propose a new\nmethod projects the text into dense and low dimensional feature vectors\naccording to the sentiment strength of the words. We detect the mixture of\npositive and negative sentiments on a multi-variant scale. Empirical evaluation\nof the proposed framework on Turkish tweets shows that our approach gets good\nresults for opinion mining.\n",
                "Link": "http://arxiv.org/pdf/1704.00016v2",
                "arxiv_id": "1704.00016v2",
                "publicationDate": "2017-03-31T18:05:44Z"
            },
            {
                "Title": "Argumentation Mining: Exploiting Multiple Sources and Background\n  Knowledge",
                "Authors": "Anastasios Lytos, Thomas Lagkas, Panagiotis Sarigiannidis, Kalina Bontcheva",
                "Abstract": "  The field of Argumentation Mining has arisen from the need of determining the\nunderlying causes from an expressed opinion and the urgency to develop the\nestablished fields of Opinion Mining and Sentiment Analysis. The recent\nprogress in the wider field of Artificial Intelligence in combination with the\navailable data through Social Web has create great potential for every\nsub-field of Natural Language Process including Argumentation Mining.\n",
                "Link": "http://arxiv.org/pdf/1809.06943v1",
                "arxiv_id": "1809.06943v1",
                "publicationDate": "2018-09-18T21:35:37Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "Legal Sentiment Analysis and Opinion Mining (LSAOM): Assimilating\n  Advances in Autonomous AI Legal Reasoning",
                "Authors": "Lance Eliot",
                "Abstract": "  An expanding field of substantive interest for the theory of the law and the\npractice-of-law entails Legal Sentiment Analysis and Opinion Mining (LSAOM),\nconsisting of two often intertwined phenomena and actions underlying legal\ndiscussions and narratives: (1) Sentiment Analysis (SA) for the detection of\nexpressed or implied sentiment about a legal matter within the context of a\nlegal milieu, and (2) Opinion Mining (OM) for the identification and\nillumination of explicit or implicit opinion accompaniments immersed within\nlegal discourse. Efforts to undertake LSAOM have historically been performed by\nhuman hand and cognition, and only thinly aided in more recent times by the use\nof computer-based approaches. Advances in Artificial Intelligence (AI)\ninvolving especially Natural Language Processing (NLP) and Machine Learning\n(ML) are increasingly bolstering how automation can systematically perform\neither or both of Sentiment Analysis and Opinion Mining, all of which is being\ninexorably carried over into engagement within a legal context for improving\nLSAOM capabilities. This research paper examines the evolving infusion of AI\ninto Legal Sentiment Analysis and Opinion Mining and proposes an alignment with\nthe Levels of Autonomy (LoA) of AI Legal Reasoning (AILR), plus provides\nadditional insights regarding AI LSAOM in its mechanizations and potential\nimpact to the study of law and the practicing of law.\n",
                "Link": "http://arxiv.org/pdf/2010.02726v1",
                "arxiv_id": "2010.02726v1",
                "publicationDate": "2020-10-02T04:15:21Z"
            },
            {
                "Title": "Facilitating on-line opinion dynamics by mining expressions of\n  causation. The case of climate change debates on The Guardian",
                "Authors": "Tom Willaert, Sven Banisch, Paul Van Eecke, Katrien Beuls",
                "Abstract": "  News website comment sections are spaces where potentially conflicting\nopinions and beliefs are voiced. Addressing questions of how to study such\ncultural and societal conflicts through technological means, the present\narticle critically examines possibilities and limitations of machine-guided\nexploration and potential facilitation of on-line opinion dynamics. These\ninvestigations are guided by a discussion of an experimental observatory for\nmining and analyzing opinions from climate change-related user comments on news\narticles from the TheGuardian.com. This observatory combines causal mapping\nmethods with computational text analysis in order to mine beliefs and visualize\nopinion landscapes based on expressions of causation. By (1) introducing\ndigital methods and open infrastructures for data exploration and analysis and\n(2) engaging in debates about the implications of such methods and\ninfrastructures, notably in terms of the leap from opinion observation to\ndebate facilitation, the article aims to make a practical and theoretical\ncontribution to the study of opinion dynamics and conflict in new media\nenvironments.\n",
                "Link": "http://arxiv.org/pdf/1912.01252v1",
                "arxiv_id": "1912.01252v1",
                "publicationDate": "2019-12-03T09:20:41Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1",
                "publicationDate": "2022-01-25T05:27:57Z"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4",
                "publicationDate": "2021-01-01T09:28:45Z"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1",
                "publicationDate": "2012-03-05T12:22:23Z"
            }
        ]
    },
    {
        "topic_name": "Language Modeling",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4",
                "publicationDate": "2021-01-01T09:28:45Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1",
                "publicationDate": "2021-09-24T05:31:01Z"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1",
                "publicationDate": "2021-06-14T08:26:50Z"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1",
                "publicationDate": "2021-05-31T20:39:35Z"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4",
                "publicationDate": "2022-05-23T06:54:56Z"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2",
                "publicationDate": "2022-08-20T15:21:35Z"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1",
                "publicationDate": "2022-05-28T15:39:09Z"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1",
                "publicationDate": "2022-09-13T17:59:21Z"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1",
                "publicationDate": "2017-01-27T18:43:31Z"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1",
                "publicationDate": "2023-10-17T02:12:12Z"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6",
                "publicationDate": "2021-07-12T16:09:22Z"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1",
                "publicationDate": "2018-09-02T14:03:30Z"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1",
                "publicationDate": "2023-11-10T17:38:46Z"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1",
                "publicationDate": "2019-11-15T08:22:33Z"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3",
                "publicationDate": "2021-07-08T13:49:46Z"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2",
                "publicationDate": "2023-10-13T16:46:38Z"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2",
                "publicationDate": "2023-11-01T15:32:50Z"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2",
                "publicationDate": "2016-10-02T23:45:23Z"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1",
                "publicationDate": "2023-11-22T08:25:15Z"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1",
                "publicationDate": "2021-11-12T09:38:15Z"
            },
            {
                "Title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models",
                "Authors": "Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman",
                "Abstract": "  Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.\n",
                "Link": "http://arxiv.org/pdf/1602.07803v1",
                "arxiv_id": "1602.07803v1",
                "publicationDate": "2016-02-25T05:35:16Z"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1",
                "publicationDate": "2023-10-13T13:25:16Z"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2",
                "publicationDate": "2023-03-16T13:31:31Z"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1",
                "publicationDate": "2023-09-24T15:51:39Z"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1",
                "publicationDate": "2023-10-22T10:55:56Z"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1",
                "publicationDate": "2010-03-30T18:54:57Z"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3",
                "publicationDate": "2023-05-11T06:27:38Z"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1",
                "publicationDate": "2024-01-30T17:47:07Z"
            },
            {
                "Title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
                "Authors": "Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam",
                "Abstract": "  Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.\n",
                "Link": "http://arxiv.org/pdf/2401.12210v1",
                "arxiv_id": "2401.12210v1",
                "publicationDate": "2024-01-22T18:52:51Z"
            },
            {
                "Title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation\n  Language Models",
                "Authors": "Mitodru Niyogi, Arnab Bhattacharya",
                "Abstract": "  We present Gyan AI Paramanu (\"atom\"), a family of novel language models for\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\nand multilingual Indic language models pretrained from scratch on a single GPU\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\npretrained with a context size of 1024 on a single GPU. The models are very\nefficient, small, fast, and powerful. We have also developed an efficient most\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\navoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we\npretrained on comparable corpora by typological grouping using the same script.\nWe performed human evaluation of our pretrained models for open end text\ngeneration on grammar, coherence, creativity, and factuality metrics for\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\nmodels on 23k instructions in respective languages. Our pretrained and\ninstruction-tuned models which are first of its kind, most powerful efficient\nsmall generative language models ever developed for Indic languages, and the\nvarious results lead to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and humongous number\nof parameters. We plan to release our models at https://www.bharatgpts.com.\n",
                "Link": "http://arxiv.org/pdf/2401.18034v1",
                "arxiv_id": "2401.18034v1",
                "publicationDate": "2024-01-31T17:58:10Z"
            },
            {
                "Title": "BDSL 49: A Comprehensive Dataset of Bangla Sign Language",
                "Authors": "Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein",
                "Abstract": "  Language is a method by which individuals express their thoughts. Each\nlanguage has its own set of alphabetic and numeric characters. People can\ncommunicate with one another through either oral or written communication.\nHowever, each language has a sign language counterpart. Individuals who are\ndeaf and/or mute communicate through sign language. The Bangla language also\nhas a sign language, which is called BDSL. The dataset is about Bangla hand\nsign images. The collection contains 49 individual Bangla alphabet images in\nsign language. BDSL49 is a dataset that consists of 29,490 images with 49\nlabels. Images of 14 different adult individuals, each with a distinct\nbackground and appearance, have been recorded during data collection. Several\nstrategies have been used to eliminate noise from datasets during preparation.\nThis dataset is available to researchers for free. They can develop automated\nsystems using machine learning, computer vision, and deep learning techniques.\nIn addition, two models were used in this dataset. The first is for detection,\nwhile the second is for recognition.\n",
                "Link": "http://arxiv.org/pdf/2208.06827v1",
                "arxiv_id": "2208.06827v1",
                "publicationDate": "2022-08-14T10:54:49Z"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2",
                "publicationDate": "2024-01-25T18:06:19Z"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3",
                "publicationDate": "2020-10-26T08:00:48Z"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2",
                "publicationDate": "2023-08-21T15:19:10Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
                "Authors": "MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee",
                "Abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
                "Link": "http://arxiv.org/pdf/2402.14179v1",
                "arxiv_id": "2402.14179v1",
                "publicationDate": "2024-02-21T23:43:04Z"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1",
                "publicationDate": "2022-10-19T21:53:49Z"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1",
                "publicationDate": "2023-09-27T14:10:57Z"
            },
            {
                "Title": "Vacaspati: A Diverse Corpus of Bangla Literature",
                "Authors": "Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya",
                "Abstract": "  Bangla (or Bengali) is the fifth most spoken language globally; yet, the\nstate-of-the-art NLP in Bangla is lagging for even simple tasks such as\nlemmatization, POS tagging, etc. This is partly due to lack of a varied quality\ncorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla\nliterature. The literary works are collected from various websites; only those\nworks that are publicly available without copyright violations or restrictions\nare collected. We believe that published literature captures the features of a\nlanguage much better than newspapers, blogs or social media posts which tend to\nfollow only a certain literary pattern and, therefore, miss out on language\nvariety. Our corpus Vacaspati is varied from multiple aspects, including type\nof composition, topic, author, time, space, etc. It contains more than 11\nmillion sentences and 115 million words. We also built a word embedding model,\nVac-FT, using FastText from Vacaspati as well as trained an Electra model,\nVac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only\na fraction of resources compared to other state-of-the-art transformer models\nand yet performs either better or similar on various downstream tasks. On\nmultiple downstream tasks, Vac-FT outperforms other FastText-based models. We\nalso demonstrate the efficacy of Vacaspati as a corpus by showing that similar\nmodels built from other corpora are not as effective. The models are available\nat https://bangla.iitk.ac.in/.\n",
                "Link": "http://arxiv.org/pdf/2307.05083v1",
                "arxiv_id": "2307.05083v1",
                "publicationDate": "2023-07-11T07:32:12Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1",
                "publicationDate": "2012-01-10T10:33:18Z"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1",
                "publicationDate": "2023-11-07T08:20:06Z"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            }
        ]
    },
    {
        "topic_name": "Sentiment Analysis",
        "summary": "default",
        "papers": [
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1",
                "publicationDate": "2023-11-25T13:58:58Z"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2",
                "publicationDate": "2024-01-25T18:06:19Z"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1",
                "publicationDate": "2023-10-22T10:55:56Z"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2",
                "publicationDate": "2023-10-13T16:46:38Z"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3",
                "publicationDate": "2023-05-11T06:27:38Z"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2",
                "publicationDate": "2016-10-02T23:45:23Z"
            },
            {
                "Title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation",
                "Authors": "Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin",
                "Abstract": "  This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.\n",
                "Link": "http://arxiv.org/pdf/2306.06147v1",
                "arxiv_id": "2306.06147v1",
                "publicationDate": "2023-06-09T12:07:10Z"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2",
                "publicationDate": "2023-08-21T15:19:10Z"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1",
                "publicationDate": "2023-09-27T14:10:57Z"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1",
                "publicationDate": "2020-11-19T21:06:28Z"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1",
                "publicationDate": "2020-11-09T14:12:07Z"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1",
                "publicationDate": "2022-10-19T21:53:49Z"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1",
                "publicationDate": "2019-07-18T01:00:42Z"
            },
            {
                "Title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language",
                "Authors": "Aunabil Chakma, Masum Hasan",
                "Abstract": "  This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023\n",
                "Link": "http://arxiv.org/pdf/2311.12735v1",
                "arxiv_id": "2311.12735v1",
                "publicationDate": "2023-11-21T17:21:15Z"
            },
            {
                "Title": "SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment\n  Analysis",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Antara Mahmud, Antonios Anastasopoulos, Marcos Zampieri",
                "Abstract": "  Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several datasets have been build with\nthe goal of training computational models for code-mixing. Although it is very\ncommon to observe code-mixing with multiple languages, most datasets available\ncontain code-mixed between only two languages. In this paper, we introduce\nSentMix-3L, a novel dataset for sentiment analysis containing code-mixed data\nbetween three languages Bangla, English, and Hindi. We carry out a\ncomprehensive evaluation using SentMix-3L. We show that zero-shot prompting\nwith GPT-3.5 outperforms all transformer-based models on SentMix-3L.\n",
                "Link": "http://arxiv.org/pdf/2310.18023v2",
                "arxiv_id": "2310.18023v2",
                "publicationDate": "2023-10-27T09:59:24Z"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1",
                "publicationDate": "2022-10-11T02:52:31Z"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1",
                "publicationDate": "2017-01-27T06:30:21Z"
            },
            {
                "Title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
                "Authors": "Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam",
                "Abstract": "  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n",
                "Link": "http://arxiv.org/pdf/2303.12772v1",
                "arxiv_id": "2303.12772v1",
                "publicationDate": "2023-03-22T17:35:35Z"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1",
                "publicationDate": "2023-11-18T18:36:16Z"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1",
                "publicationDate": "2022-10-26T20:57:45Z"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1",
                "publicationDate": "2022-05-28T15:39:09Z"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1",
                "publicationDate": "2023-10-17T02:12:12Z"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1",
                "publicationDate": "2023-11-10T17:38:46Z"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1",
                "publicationDate": "2012-04-05T12:28:11Z"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1",
                "publicationDate": "2023-10-13T13:25:16Z"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1",
                "publicationDate": "2020-05-29T15:38:54Z"
            },
            {
                "Title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis",
                "Authors": "Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, Feng Wu",
                "Abstract": "  Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta.\n",
                "Link": "http://arxiv.org/pdf/2005.05635v2",
                "arxiv_id": "2005.05635v2",
                "publicationDate": "2020-05-12T09:23:32Z"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1",
                "publicationDate": "2013-08-17T14:04:00Z"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2",
                "publicationDate": "2023-03-16T13:31:31Z"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1",
                "publicationDate": "2023-04-10T14:27:35Z"
            },
            {
                "Title": "SlangSD: Building and Using a Sentiment Dictionary of Slang Words for\n  Short-Text Sentiment Classification",
                "Authors": "Liang Wu, Fred Morstatter, Huan Liu",
                "Abstract": "  Sentiment in social media is increasingly considered as an important resource\nfor customer segmentation, market understanding, and tackling other\nsocio-economic issues. However, sentiment in social media is difficult to\nmeasure since user-generated content is usually short and informal. Although\nmany traditional sentiment analysis methods have been proposed, identifying\nslang sentiment words remains untackled. One of the reasons is that slang\nsentiment words are not available in existing dictionaries or sentiment\nlexicons. To this end, we propose to build the first sentiment dictionary of\nslang words to aid sentiment analysis of social media content. It is laborious\nand time-consuming to collect and label the sentiment polarity of a\ncomprehensive list of slang words. We present an approach to leverage web\nresources to construct an extensive Slang Sentiment word Dictionary (SlangSD)\nthat is easy to maintain and extend. SlangSD is publicly available for research\npurposes. We empirically show the advantages of using SlangSD, the newly-built\nslang sentiment word dictionary for sentiment classification, and provide\nexamples demonstrating its ease of use with an existing sentiment system.\n",
                "Link": "http://arxiv.org/pdf/1608.05129v1",
                "arxiv_id": "1608.05129v1",
                "publicationDate": "2016-08-17T23:32:57Z"
            },
            {
                "Title": "Sentiment Identification in Code-Mixed Social Media Text",
                "Authors": "Souvick Ghosh, Satanu Ghosh, Dipankar Das",
                "Abstract": "  Sentiment analysis is the Natural Language Processing (NLP) task dealing with\nthe detection and classification of sentiments in texts. While some tasks deal\nwith identifying the presence of sentiment in the text (Subjectivity analysis),\nother tasks aim at determining the polarity of the text categorizing them as\npositive, negative and neutral. Whenever there is a presence of sentiment in\nthe text, it has a source (people, group of people or any entity) and the\nsentiment is directed towards some entity, object, event or person. Sentiment\nanalysis tasks aim to determine the subject, the target and the polarity or\nvalence of the sentiment. In our work, we try to automatically extract\nsentiment (positive or negative) from Facebook posts using a machine learning\napproach.While some works have been done in code-mixed social media data and in\nsentiment analysis separately, our work is the first attempt (as of now) which\naims at performing sentiment analysis of code-mixed social media text. We have\nused extensive pre-processing to remove noise from raw text. Multilayer\nPerceptron model has been used to determine the polarity of the sentiment. We\nhave also developed the corpus for this task by manually labeling Facebook\nposts with their associated sentiments.\n",
                "Link": "http://arxiv.org/pdf/1707.01184v1",
                "arxiv_id": "1707.01184v1",
                "publicationDate": "2017-07-04T23:29:44Z"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3",
                "publicationDate": "2022-10-12T17:01:53Z"
            },
            {
                "Title": "A Deep Learning System for Sentiment Analysis of Service Calls",
                "Authors": "Yanan Jia, Sony SungChu",
                "Abstract": "  Sentiment analysis is crucial for the advancement of artificial intelligence\n(AI). Sentiment understanding can help AI to replicate human language and\ndiscourse. Studying the formation and response of sentiment state from\nwell-trained Customer Service Representatives (CSRs) can help make the\ninteraction between humans and AI more intelligent. In this paper, a sentiment\nanalysis pipeline is first carried out with respect to real-world multi-party\nconversations - that is, service calls. Based on the acoustic and linguistic\nfeatures extracted from the source information, a novel aggregated method for\nvoice sentiment recognition framework is built. Each party's sentiment pattern\nduring the communication is investigated along with the interaction sentiment\npattern between all parties.\n",
                "Link": "http://arxiv.org/pdf/2004.10320v1",
                "arxiv_id": "2004.10320v1",
                "publicationDate": "2020-04-21T22:02:43Z"
            },
            {
                "Title": "A Multimodal Sentiment Dataset for Video Recommendation",
                "Authors": "Hongxuan Tang, Hao Liu, Xinyan Xiao, Hua Wu",
                "Abstract": "  Recently, multimodal sentiment analysis has seen remarkable advance and a lot\nof datasets are proposed for its development. In general, current multimodal\nsentiment analysis datasets usually follow the traditional system of\nsentiment/emotion, such as positive, negative and so on. However, when applied\nin the scenario of video recommendation, the traditional sentiment/emotion\nsystem is hard to be leveraged to represent different contents of videos in the\nperspective of visual senses and language understanding. Based on this, we\npropose a multimodal sentiment analysis dataset, named baiDu Video Sentiment\ndataset (DuVideoSenti), and introduce a new sentiment system which is designed\nto describe the sentimental style of a video on recommendation scenery.\nSpecifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,\neach video is manually annotated with a sentimental style label which describes\nthe user's real feeling of a video. Furthermore, we propose UNIMO as our\nbaseline for DuVideoSenti. Experimental results show that DuVideoSenti brings\nnew challenges to multimodal sentiment analysis, and could be used as a new\nbenchmark for evaluating approaches designed for video understanding and\nmultimodal fusion. We also expect our proposed DuVideoSenti could further\nimprove the development of multimodal sentiment analysis and its application to\nvideo recommendations.\n",
                "Link": "http://arxiv.org/pdf/2109.08333v1",
                "arxiv_id": "2109.08333v1",
                "publicationDate": "2021-09-17T03:10:42Z"
            },
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1",
                "publicationDate": "2023-12-21T16:52:41Z"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1",
                "publicationDate": "2020-04-19T07:42:22Z"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1",
                "publicationDate": "2012-06-02T13:23:18Z"
            },
            {
                "Title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP",
                "Authors": "Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque",
                "Abstract": "  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.\n",
                "Link": "http://arxiv.org/pdf/2309.13173v2",
                "arxiv_id": "2309.13173v2",
                "publicationDate": "2023-09-22T20:29:34Z"
            },
            {
                "Title": "Sentiment analysis and opinion mining on E-commerce site",
                "Authors": "Fatema Tuz Zohra Anny, Oahidul Islam",
                "Abstract": "  Sentiment analysis or opinion mining help to illustrate the phrase NLP\n(Natural Language Processing). Sentiment analysis has been the most significant\ntopic in recent years. The goal of this study is to solve the sentiment\npolarity classification challenges in sentiment analysis. A broad technique for\ncategorizing sentiment opposition is presented, along with comprehensive\nprocess explanations. With the results of the analysis, both sentence-level\nclassification and review-level categorization are conducted. Finally, we\ndiscuss our plans for future sentiment analysis research.\n",
                "Link": "http://arxiv.org/pdf/2211.15536v2",
                "arxiv_id": "2211.15536v2",
                "publicationDate": "2022-11-28T16:43:33Z"
            },
            {
                "Title": "A Clustering Analysis of Tweet Length and its Relation to Sentiment",
                "Authors": "Matthew Mayo",
                "Abstract": "  Sentiment analysis of Twitter data is performed. The researcher has made the\nfollowing contributions via this paper: (1) an innovative method for deriving\nsentiment score dictionaries using an existing sentiment dictionary as seed\nwords is explored, and (2) an analysis of clustered tweet sentiment scores\nbased on tweet length is performed.\n",
                "Link": "http://arxiv.org/pdf/1406.3287v3",
                "arxiv_id": "1406.3287v3",
                "publicationDate": "2014-06-12T17:01:10Z"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1",
                "publicationDate": "2023-05-31T04:08:57Z"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1",
                "publicationDate": "2024-03-31T09:52:25Z"
            },
            {
                "Title": "Detecting Domain Polarity-Changes of Words in a Sentiment Lexicon",
                "Authors": "Shuai Wang, Guangyi Lv, Sahisnu Mazumder, Bing Liu",
                "Abstract": "  Sentiment lexicons are instrumental for sentiment analysis. One can use a set\nof sentiment words provided in a sentiment lexicon and a lexicon-based\nclassifier to perform sentiment classification. One major issue with this\napproach is that many sentiment words are domain dependent. That is, they may\nbe positive in some domains but negative in some others. We refer to this\nproblem as domain polarity-changes of words. Detecting such words and\ncorrecting their sentiment for an application domain is very important. In this\npaper, we propose a graph-based technique to tackle this problem. Experimental\nresults show its effectiveness on multiple real-world datasets.\n",
                "Link": "http://arxiv.org/pdf/2004.14357v1",
                "arxiv_id": "2004.14357v1",
                "publicationDate": "2020-04-29T17:35:05Z"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1",
                "publicationDate": "2010-02-21T19:48:16Z"
            },
            {
                "Title": "Learning Implicit Sentiment in Aspect-based Sentiment Analysis with\n  Supervised Contrastive Pre-Training",
                "Authors": "Zhengyan Li, Yicheng Zou, Chong Zhang, Qi Zhang, Zhongyu Wei",
                "Abstract": "  Aspect-based sentiment analysis aims to identify the sentiment polarity of a\nspecific aspect in product reviews. We notice that about 30% of reviews do not\ncontain obvious opinion words, but still convey clear human-aware sentiment\norientation, which is known as implicit sentiment. However, recent neural\nnetwork-based approaches paid little attention to implicit sentiment entailed\nin the reviews. To overcome this issue, we adopt Supervised Contrastive\nPre-training on large-scale sentiment-annotated corpora retrieved from\nin-domain language resources. By aligning the representation of implicit\nsentiment expressions to those with the same sentiment label, the pre-training\nprocess leads to better capture of both implicit and explicit sentiment\norientation towards aspects in reviews. Experimental results show that our\nmethod achieves state-of-the-art performance on SemEval2014 benchmarks, and\ncomprehensive analysis validates its effectiveness on learning implicit\nsentiment.\n",
                "Link": "http://arxiv.org/pdf/2111.02194v1",
                "arxiv_id": "2111.02194v1",
                "publicationDate": "2021-11-03T13:03:17Z"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1",
                "publicationDate": "2023-03-19T09:24:48Z"
            },
            {
                "Title": "Text Compression for Sentiment Analysis via Evolutionary Algorithms",
                "Authors": "Emmanuel Dufourq, Bruce A. Bassett",
                "Abstract": "  Can textual data be compressed intelligently without losing accuracy in\nevaluating sentiment? In this study, we propose a novel evolutionary\ncompression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),\nwhich makes use of Parts-of-Speech tags to compress text in a way that\nsacrifices minimal classification accuracy when used in conjunction with\nsentiment analysis algorithms. An analysis of PARSEC with eight commercial and\nnon-commercial sentiment analysis algorithms on twelve English sentiment data\nsets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss\nin sentiment classification accuracy for (20%, 50%, 75%) data compression with\nPARSEC using LingPipe, the most accurate of the sentiment algorithms. Other\nsentiment analysis algorithms are more severely affected by compression. We\nconclude that significant compression of text data is possible for sentiment\nanalysis depending on the accuracy demands of the specific application and the\nspecific sentiment analysis algorithm used.\n",
                "Link": "http://arxiv.org/pdf/1709.06990v1",
                "arxiv_id": "1709.06990v1",
                "publicationDate": "2017-09-20T17:57:16Z"
            },
            {
                "Title": "USA: Universal Sentiment Analysis Model & Construction of Japanese\n  Sentiment Text Classification and Part of Speech Dataset",
                "Authors": "Chengguang Gan, Qinghao Zhang, Tatsunori Mori",
                "Abstract": "  Sentiment analysis is a pivotal task in the domain of natural language\nprocessing. It encompasses both text-level sentiment polarity classification\nand word-level Part of Speech(POS) sentiment polarity determination. Such\nanalysis challenges models to understand text holistically while also\nextracting nuanced information. With the rise of Large Language Models(LLMs),\nnew avenues for sentiment analysis have opened. This paper proposes enhancing\nperformance by leveraging the Mutual Reinforcement Effect(MRE) between\nindividual words and the overall text. It delves into how word polarity\ninfluences the overarching sentiment of a passage. To support our research, we\nannotated four novel Sentiment Text Classification and Part of Speech(SCPOS)\ndatasets, building upon existing sentiment classification datasets.\nFurthermore, we developed a Universal Sentiment Analysis(USA) model, with a\n7-billion parameter size. Experimental results revealed that our model\nsurpassed the performance of gpt-3.5-turbo across all four datasets,\nunderscoring the significance of MRE in sentiment analysis.\n",
                "Link": "http://arxiv.org/pdf/2309.03787v2",
                "arxiv_id": "2309.03787v2",
                "publicationDate": "2023-09-07T15:35:00Z"
            }
        ]
    },
    {
        "topic_name": "Named Entity Disambiguation",
        "summary": "default",
        "papers": [
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3",
                "publicationDate": "2021-05-31T10:58:58Z"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1",
                "publicationDate": "2024-01-30T17:47:07Z"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2",
                "publicationDate": "2023-03-16T13:31:31Z"
            },
            {
                "Title": "Evaluating Entity Disambiguation and the Role of Popularity in\n  Retrieval-Based NLP",
                "Authors": "Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer Singh",
                "Abstract": "  Retrieval is a core component for open-domain NLP tasks. In open-domain\ntasks, multiple entities can share a name, making disambiguation an inherent\nyet under-explored problem. We propose an evaluation benchmark for assessing\nthe entity disambiguation capabilities of these retrievers, which we call\nAmbiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection\nof entities that share a name along with queries about those entities. By\ncovering the set of entities for polysemous names, AmbER sets act as a\nchallenging test of entity disambiguation. We create AmbER sets for three\npopular open-domain tasks: fact checking, slot filling, and question answering,\nand evaluate a diverse set of retrievers. We find that the retrievers exhibit\npopularity bias, significantly under-performing on rarer entities that share a\nname, e.g., they are twice as likely to retrieve erroneous documents on queries\nfor the less popular entity under the same name. These experiments on AmbER\nsets show their utility as an evaluation tool and highlight the weaknesses of\npopular retrieval systems.\n",
                "Link": "http://arxiv.org/pdf/2106.06830v1",
                "arxiv_id": "2106.06830v1",
                "publicationDate": "2021-06-12T18:27:18Z"
            },
            {
                "Title": "Bootleg: Chasing the Tail with Self-Supervised Named Entity\n  Disambiguation",
                "Authors": "Laurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao Ling, Christopher Re",
                "Abstract": "  A challenge for named entity disambiguation (NED), the task of mapping\ntextual mentions to entities in a knowledge base, is how to disambiguate\nentities that appear rarely in the training data, termed tail entities. Humans\nuse subtle reasoning patterns based on knowledge of entity facts, relations,\nand types to disambiguate unfamiliar entities. Inspired by these patterns, we\nintroduce Bootleg, a self-supervised NED system that is explicitly grounded in\nreasoning patterns for disambiguation. We define core reasoning patterns for\ndisambiguation, create a learning procedure to encourage the self-supervised\nmodel to learn the patterns, and show how to use weak supervision to enhance\nthe signals in the training data. Encoding the reasoning patterns in a simple\nTransformer architecture, Bootleg meets or exceeds state-of-the-art on three\nNED benchmarks. We further show that the learned representations from Bootleg\nsuccessfully transfer to other non-disambiguation tasks that require\nentity-based knowledge: we set a new state-of-the-art in the popular TACRED\nrelation extraction task by 1.0 F1 points and demonstrate up to 8% performance\nlift in highly optimized production search and assistant tasks at a major\ntechnology company\n",
                "Link": "http://arxiv.org/pdf/2010.10363v3",
                "arxiv_id": "2010.10363v3",
                "publicationDate": "2020-10-20T15:17:49Z"
            },
            {
                "Title": "An Unsupervised Language-Independent Entity Disambiguation Method and\n  its Evaluation on the English and Persian Languages",
                "Authors": "Majid Asgari-Bidhendi, Behrooz Janfada, Amir Havangi, Sayyed Ali Hossayni, Behrouz Minaei-Bidgoli",
                "Abstract": "  Entity Linking is one of the essential tasks of information extraction and\nnatural language understanding. Entity linking mainly consists of two tasks:\nrecognition and disambiguation of named entities. Most studies address these\ntwo tasks separately or focus only on one of them. Moreover, most of the\nstate-of-the -art entity linking algorithms are either supervised, which have\npoor performance in the absence of annotated corpora or language-dependent,\nwhich are not appropriate for multi-lingual applications. In this paper, we\nintroduce an Unsupervised Language-Independent Entity Disambiguation (ULIED),\nwhich utilizes a novel approach to disambiguate and link named entities.\nEvaluation of ULIED on different English entity linking datasets as well as the\nonly available Persian dataset illustrates that ULIED in most of the cases\noutperforms the state-of-the-art unsupervised multi-lingual approaches.\n",
                "Link": "http://arxiv.org/pdf/2102.00395v1",
                "arxiv_id": "2102.00395v1",
                "publicationDate": "2021-01-31T06:41:55Z"
            },
            {
                "Title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa",
                "Authors": "Rahul Mehta, Vasudeva Varma",
                "Abstract": "  Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n",
                "Link": "http://arxiv.org/pdf/2305.03300v1",
                "arxiv_id": "2305.03300v1",
                "publicationDate": "2023-05-05T06:05:45Z"
            },
            {
                "Title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition",
                "Authors": "Beiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-Hua Ling, Quan Liu",
                "Abstract": "  This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n",
                "Link": "http://arxiv.org/pdf/2203.03216v2",
                "arxiv_id": "2203.03216v2",
                "publicationDate": "2022-03-07T09:05:37Z"
            },
            {
                "Title": "Entity-aware ELMo: Learning Contextual Entity Representation for Entity\n  Disambiguation",
                "Authors": "Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Rasha Obeidat, Prasad Tadepalli",
                "Abstract": "  We present a new local entity disambiguation system. The key to our system is\na novel approach for learning entity representations. In our approach we learn\nan entity aware extension of Embedding for Language Model (ELMo) which we call\nEntity-ELMo (E-ELMo). Given a paragraph containing one or more named entity\nmentions, each mention is first defined as a function of the entire paragraph\n(including other mentions), then they predict the referent entities. Utilizing\nE-ELMo for local entity disambiguation, we outperform all of the\nstate-of-the-art local and global models on the popular benchmarks by improving\nabout 0.5\\% on micro average accuracy for AIDA test-b with Yago candidate set.\nThe evaluation setup of the training data and candidate set are the same as our\nbaselines for fair comparison.\n",
                "Link": "http://arxiv.org/pdf/1908.05762v2",
                "arxiv_id": "1908.05762v2",
                "publicationDate": "2019-08-14T03:51:25Z"
            },
            {
                "Title": "Bayesian Non-Exhaustive Classification A Case Study: Online Name\n  Disambiguation using Temporal Record Streams",
                "Authors": "Baichuan Zhang, Murat Dundar, Mohammad Al Hasan",
                "Abstract": "  The name entity disambiguation task aims to partition the records of multiple\nreal-life persons so that each partition contains records pertaining to a\nunique person. Most of the existing solutions for this task operate in a batch\nmode, where all records to be disambiguated are initially available to the\nalgorithm. However, more realistic settings require that the name\ndisambiguation task be performed in an online fashion, in addition to, being\nable to identify records of new ambiguous entities having no preexisting\nrecords. In this work, we propose a Bayesian non-exhaustive classification\nframework for solving online name disambiguation task. Our proposed method uses\na Dirichlet process prior with a Normal * Normal * Inverse Wishart data model\nwhich enables identification of new ambiguous entities who have no records in\nthe training data. For online classification, we use one sweep Gibbs sampler\nwhich is very efficient and effective. As a case study we consider\nbibliographic data in a temporal stream format and disambiguate authors by\npartitioning their papers into homogeneous groups. Our experimental results\ndemonstrate that the proposed method is better than existing methods for\nperforming online name disambiguation task.\n",
                "Link": "http://arxiv.org/pdf/1607.05746v3",
                "arxiv_id": "1607.05746v3",
                "publicationDate": "2016-07-19T20:17:10Z"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1",
                "publicationDate": "2022-04-21T08:40:17Z"
            },
            {
                "Title": "Scale-free collaboration networks: An author name disambiguation\n  perspective",
                "Authors": "Jinseok Kim",
                "Abstract": "  Several studies have found that collaboration networks are scale-free,\nproposing that such networks can be modeled by specific network evolution\nmechanisms like preferential attachment. This study argues that collaboration\nnetworks can look more or less scale-free depending on the methods for\nresolving author name ambiguity in bibliographic data. Analyzing networks\nconstructed from multiple datasets containing 3.4M ~ 9.6M publication records,\nthis study shows that collaboration networks in which author names are\ndisambiguated by the commonly used heuristic, i.e., forename-initial-based name\nmatching, tend to produce degree distributions better fitted to power-law\nslopes with the typical scaling parameter (2 < {\\alpha} < 3) than networks\ndisambiguated by more accurate algorithm-based methods. Such tendency is\nobserved across collaboration networks generated under various conditions such\nas cumulative years, 5- & 1-year sliding windows, and random sampling, and\nthrough simulation, found to arise due mainly to artefactual entities created\nby inaccurate disambiguation. This cautionary study calls for special attention\nfrom scholars analyzing network data in which entities such as people,\norganization, and gene can be merged or split by improper disambiguation.\n",
                "Link": "http://arxiv.org/pdf/1811.03030v1",
                "arxiv_id": "1811.03030v1",
                "publicationDate": "2018-11-07T17:30:00Z"
            },
            {
                "Title": "Distant Supervision for Entity Linking",
                "Authors": "Miao Fan, Qiang Zhou, Thomas Fang Zheng",
                "Abstract": "  Entity linking is an indispensable operation of populating knowledge\nrepositories for information extraction. It studies on aligning a textual\nentity mention to its corresponding disambiguated entry in a knowledge\nrepository. In this paper, we propose a new paradigm named distantly supervised\nentity linking (DSEL), in the sense that the disambiguated entities that belong\nto a huge knowledge repository (Freebase) are automatically aligned to the\ncorresponding descriptive webpages (Wiki pages). In this way, a large scale of\nweakly labeled data can be generated without manual annotation and fed to a\nclassifier for linking more newly discovered entities. Compared with\ntraditional paradigms based on solo knowledge base, DSEL benefits more via\njointly leveraging the respective advantages of Freebase and Wikipedia.\nSpecifically, the proposed paradigm facilitates bridging the disambiguated\nlabels (Freebase) of entities and their textual descriptions (Wikipedia) for\nWeb-scale entities. Experiments conducted on a dataset of 140,000 items and\n60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze\nthe feature performance and improve the F1-measure to 0.545.\n",
                "Link": "http://arxiv.org/pdf/1505.03823v3",
                "arxiv_id": "1505.03823v3",
                "publicationDate": "2015-05-14T18:15:49Z"
            },
            {
                "Title": "Cluster-based Mention Typing for Named Entity Disambiguation",
                "Authors": "Arda \u00c7elebi, Arzucan \u00d6zg\u00fcr",
                "Abstract": "  An entity mention in text such as \"Washington\" may correspond to many\ndifferent named entities such as the city \"Washington D.C.\" or the newspaper\n\"Washington Post.\" The goal of named entity disambiguation is to identify the\nmentioned named entity correctly among all possible candidates. If the type\n(e.g. location or person) of a mentioned entity can be correctly predicted from\nthe context, it may increase the chance of selecting the right candidate by\nassigning low probability to the unlikely ones. This paper proposes\ncluster-based mention typing for named entity disambiguation. The aim of\nmention typing is to predict the type of a given mention based on its context.\nGenerally, manually curated type taxonomies such as Wikipedia categories are\nused. We introduce cluster-based mention typing, where named entities are\nclustered based on their contextual similarities and the cluster ids are\nassigned as types. The hyperlinked mentions and their context in Wikipedia are\nused in order to obtain these cluster-based types. Then, mention typing models\nare trained on these mentions, which have been labeled with their cluster-based\ntypes through distant supervision. At the named entity disambiguation phase,\nfirst the cluster-based types of a given mention are predicted and then, these\ntypes are used as features in a ranking model to select the best entity among\nthe candidates. We represent entities at multiple contextual levels and obtain\ndifferent clusterings (and thus typing models) based on each level. As each\nclustering breaks the entity space differently, mention typing based on each\nclustering discriminates the mention differently. When predictions from all\ntyping models are used together, our system achieves better or comparable\nresults based on randomization tests with respect to the state-of-the-art\nlevels on four defacto test sets.\n",
                "Link": "http://arxiv.org/pdf/2109.11389v1",
                "arxiv_id": "2109.11389v1",
                "publicationDate": "2021-09-23T14:19:20Z"
            },
            {
                "Title": "Analysis of Named Entity Recognition and Linking for Tweets",
                "Authors": "Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp, Genevieve Gorrell, Rapha\u00ebl Troncy, Johann Petrak, Kalina Bontcheva",
                "Abstract": "  Applying natural language processing for mining and intelligent information\naccess to tweets (a form of microblog) is a challenging, emerging research\narea. Unlike carefully authored news text and other longer content, tweets pose\na number of new challenges, due to their short, noisy, context-dependent, and\ndynamic nature. Information extraction from tweets is typically performed in a\npipeline, comprising consecutive stages of language identification,\ntokenisation, part-of-speech tagging, named entity recognition and entity\ndisambiguation (e.g. with respect to DBpedia). In this work, we describe a new\nTwitter entity disambiguation dataset, and conduct an empirical analysis of\nnamed entity recognition and disambiguation, investigating how robust a number\nof state-of-the-art systems are on such noisy texts, what the main sources of\nerror are, and which problems should be further investigated to improve the\nstate of the art.\n",
                "Link": "http://arxiv.org/pdf/1410.7182v1",
                "arxiv_id": "1410.7182v1",
                "publicationDate": "2014-10-27T11:09:36Z"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1",
                "publicationDate": "2022-10-19T21:53:49Z"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2",
                "publicationDate": "2024-04-04T15:31:21Z"
            },
            {
                "Title": "Benchmarking Named Entity Disambiguation approaches for Streaming Graphs",
                "Authors": "Sutanay Choudhury, Chase Dowling",
                "Abstract": "  Named Entity Disambiaguation (NED) is a central task for applications dealing\nwith natural language text. Assume that we have a graph based knowledge base\n(subsequently referred as Knowledge Graph) where nodes represent various real\nworld entities such as people, location, organization and concepts. Given data\nsources such as social media streams and web pages Entity Linking is the task\nof mapping named entities that are extracted from the data to those present in\nthe Knowledge Graph. This is an inherently difficult task due to several\nreasons. Almost all these data sources are generated without any formal\nontology; the unstructured nature of the input, limited context and the\nambiguity involved when multiple entities are mapped to the same name make this\na hard task. This report looks at two state of the art systems employing two\ndistinctive approaches: graph based Accurate Online Disambiguation of Entities\n(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a\nstatistical inference approach. We compare both approaches using the data set\nand queries provided by the Knowledge Base Population (KBP) track at 2011 NIST\nText Analytics Conference (TAC). This report begins with an overview of the\nrespective approaches, followed by detailed description of the experimental\nsetup. It concludes with our findings from the benchmarking exercise.\n",
                "Link": "http://arxiv.org/pdf/1407.3751v1",
                "arxiv_id": "1407.3751v1",
                "publicationDate": "2014-07-14T18:01:08Z"
            },
            {
                "Title": "ProtagonistTagger -- a Tool for Entity Linkage of Persons in Texts from\n  Various Languages and Domains",
                "Authors": "Weronika Lajewska, Anna Wroblewska",
                "Abstract": "  Named entities recognition (NER) and disambiguation (NED) can add semantic\ncontext to the recognized named entities in texts. Named entity linkage in\ntexts, regardless of a domain, provides links between the entities mentioned in\nunstructured texts and individual instances of real-world objects. In this\nposter, we present a tool - protagonistTagger - for person NER and NED in\ntexts. The tool was tested on texts extracted from classic English novels and\nPolish Internet news. The tool's performance (both precision and recall)\nfluctuates between 78% and even 88%.\n",
                "Link": "http://arxiv.org/pdf/2203.06746v1",
                "arxiv_id": "2203.06746v1",
                "publicationDate": "2022-03-13T19:50:23Z"
            },
            {
                "Title": "BELHD: Improving Biomedical Entity Linking with Homonoym Disambiguation",
                "Authors": "Samuele Garda, Ulf Leser",
                "Abstract": "  Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base (KB). A popular approach to the task are name-based methods,\ni.e. those identifying the most appropriate name in the KB for a given mention,\neither via dense retrieval or autoregressive modeling. However, as these\nmethods directly return KB names, they cannot cope with homonyms, i.e.\ndifferent KB entities sharing the exact same name. This significantly affects\ntheir performance, especially for KBs where homonyms account for a large amount\nof entity mentions (e.g. UMLS and NCBI Gene). We therefore present BELHD\n(Biomedical Entity Linking with Homonym Disambiguation), a new name-based\nmethod that copes with this challenge. Specifically, BELHD builds upon the\nBioSyn (Sung et al.,2020) model introducing two crucial extensions. First, it\nperforms a preprocessing of the KB in which it expands homonyms with an\nautomatically chosen disambiguating string, thus enforcing unique linking\ndecisions. Second, we introduce candidate sharing, a novel strategy to select\ncandidates for contrastive learning that enhances the overall training signal.\nExperiments with 10 corpora and five entity types show that BELHD improves upon\nstate-of-the-art approaches, achieving the best results in 6 out 10 corpora\nwith an average improvement of 4.55pp recall@1. Furthermore, the KB\npreprocessing is orthogonal to the core prediction model and thus can also\nimprove other methods, which we exemplify for GenBioEL (Yuan et al, 2022), a\ngenerative name-based BEL approach. Code is available at: link added upon\npublication.\n",
                "Link": "http://arxiv.org/pdf/2401.05125v1",
                "arxiv_id": "2401.05125v1",
                "publicationDate": "2024-01-10T12:45:18Z"
            },
            {
                "Title": "A Large-Scale Multilingual Disambiguation of Glosses",
                "Authors": "Jos\u00e9 Camacho Collados, Claudio Delli Bovi, Alessandro Raganato, Roberto Navigli",
                "Abstract": "  Linking concepts and named entities to knowledge bases has become a crucial\nNatural Language Understanding task. In this respect, recent works have shown\nthe key advantage of exploiting textual definitions in various Natural Language\nProcessing applications. However, to date there are no reliable large-scale\ncorpora of sense-annotated textual definitions available to the research\ncommunity. In this paper we present a large-scale high-quality corpus of\ndisambiguated glosses in multiple languages, comprising sense annotations of\nboth concepts and named entities from a unified sense inventory. Our approach\nfor the construction and disambiguation of the corpus builds upon the structure\nof a large multilingual semantic network and a state-of-the-art disambiguation\nsystem; first, we gather complementary information of equivalent definitions\nacross different languages to provide context for disambiguation, and then we\ncombine it with a semantic similarity-based refinement. As a result we obtain a\nmultilingual corpus of textual definitions featuring over 38 million\ndefinitions in 263 languages, and we make it freely available at\nhttp://lcl.uniroma1.it/disambiguated-glosses. Experiments on Open Information\nExtraction and Sense Clustering show how two state-of-the-art approaches\nimprove their performance by integrating our disambiguated corpus into their\npipeline.\n",
                "Link": "http://arxiv.org/pdf/1608.06718v1",
                "arxiv_id": "1608.06718v1",
                "publicationDate": "2016-08-24T05:30:45Z"
            },
            {
                "Title": "Joint Learning of the Embedding of Words and Entities for Named Entity\n  Disambiguation",
                "Authors": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji",
                "Abstract": "  Named Entity Disambiguation (NED) refers to the task of resolving multiple\nnamed entity mentions in a document to their correct references in a knowledge\nbase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method\nspecifically designed for NED. The proposed method jointly maps words and\nentities into the same continuous vector space. We extend the skip-gram model\nby using two models. The KB graph model learns the relatedness of entities\nusing the link structure of the KB, whereas the anchor context model aims to\nalign vectors such that similar words and entities occur close to one another\nin the vector space by leveraging KB anchors and their context words. By\ncombining contexts based on the proposed embedding with standard NED features,\nwe achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset\nand 85.2% on the TAC 2010 dataset.\n",
                "Link": "http://arxiv.org/pdf/1601.01343v4",
                "arxiv_id": "1601.01343v4",
                "publicationDate": "2016-01-06T22:19:20Z"
            },
            {
                "Title": "Cross-Domain Data Integration for Named Entity Disambiguation in\n  Biomedical Text",
                "Authors": "Maya Varma, Laurel Orr, Sen Wu, Megan Leszczynski, Xiao Ling, Christopher R\u00e9",
                "Abstract": "  Named entity disambiguation (NED), which involves mapping textual mentions to\nstructured entities, is particularly challenging in the medical domain due to\nthe presence of rare entities. Existing approaches are limited by the presence\nof coarse-grained structural resources in biomedical knowledge bases as well as\nthe use of training datasets that provide low coverage over uncommon resources.\nIn this work, we address these issues by proposing a cross-domain data\nintegration method that transfers structural knowledge from a general text\nknowledge base to the medical domain. We utilize our integration scheme to\naugment structural resources and generate a large biomedical NED dataset for\npretraining. Our pretrained model with injected structural knowledge achieves\nstate-of-the-art performance on two benchmark medical NED datasets: MedMentions\nand BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57\naccuracy points.\n",
                "Link": "http://arxiv.org/pdf/2110.08228v1",
                "arxiv_id": "2110.08228v1",
                "publicationDate": "2021-10-15T17:38:16Z"
            },
            {
                "Title": "Harnessing Historical Corrections to build Test Collections for Named\n  Entity Disambiguation",
                "Authors": "Florian Reitz",
                "Abstract": "  Matching mentions of persons to the actual persons (the name disambiguation\nproblem) is central for several digital library applications. Scientists have\nbeen working on algorithms to create this matching for decades without finding\na universal solution. One problem is that test collections for this problem are\noften small and specific to a certain collection. In this work, we present an\napproach that can create large test collections from historical metadata with\nminimal extra cost. We apply this approach to the DBLP collection to generate\ntwo freely available test collections. One collection focuses on the properties\nof defects and one on the evaluation of disambiguation algorithms.\n",
                "Link": "http://arxiv.org/pdf/1808.08999v1",
                "arxiv_id": "1808.08999v1",
                "publicationDate": "2018-08-27T19:05:02Z"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1",
                "publicationDate": "2017-01-27T18:43:31Z"
            },
            {
                "Title": "Protagonists' Tagger in Literary Domain -- New Datasets and a Method for\n  Person Entity Linkage",
                "Authors": "Weronika \u0141ajewska, Anna Wr\u00f3blewska",
                "Abstract": "  Semantic annotation of long texts, such as novels, remains an open challenge\nin Natural Language Processing (NLP). This research investigates the problem of\ndetecting person entities and assigning them unique identities, i.e.,\nrecognizing people (especially main characters) in novels. We prepared a method\nfor person entity linkage (named entity recognition and disambiguation) and new\ntesting datasets. The datasets comprise 1,300 sentences from 13 classic novels\nof different genres that a novel reader had manually annotated. Our process of\nidentifying literary characters in a text, implemented in protagonistTagger,\ncomprises two stages: (1) named entity recognition (NER) of persons, (2) named\nentity disambiguation (NED) - matching each recognized person with the literary\ncharacter's full name, based on approximate text matching. The\nprotagonistTagger achieves both precision and recall of above 83% on the\nprepared testing sets. Finally, we gathered a corpus of 13 full-text novels\ntagged with protagonistTagger that comprises more than 35,000 mentions of\nliterary characters.\n",
                "Link": "http://arxiv.org/pdf/2110.01349v1",
                "arxiv_id": "2110.01349v1",
                "publicationDate": "2021-10-04T11:54:43Z"
            },
            {
                "Title": "Evaluating the word-expert approach for Named-Entity Disambiguation",
                "Authors": "Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, Eneko Agirre",
                "Abstract": "  Named Entity Disambiguation (NED) is the task of linking a named-entity\nmention to an instance in a knowledge-base, typically Wikipedia. This task is\nclosely related to word-sense disambiguation (WSD), where the supervised\nword-expert approach has prevailed. In this work we present the results of the\nword-expert approach to NED, where one classifier is built for each target\nentity mention string. The resources necessary to build the system, a\ndictionary and a set of training instances, have been automatically derived\nfrom Wikipedia. We provide empirical evidence of the value of this approach, as\nwell as a study of the differences between WSD and NED, including ambiguity and\nsynonymy statistics.\n",
                "Link": "http://arxiv.org/pdf/1603.04767v1",
                "arxiv_id": "1603.04767v1",
                "publicationDate": "2016-03-15T17:16:02Z"
            },
            {
                "Title": "Evaluating author name disambiguation for digital libraries: A case of\n  DBLP",
                "Authors": "Jinseok Kim",
                "Abstract": "  Author name ambiguity in a digital library may affect the findings of\nresearch that mines authorship data of the library. This study evaluates author\nname disambiguation in DBLP, a widely used but insufficiently evaluated digital\nlibrary for its disambiguation performance. In doing so, this study takes a\ntriangulation approach that author name disambiguation for a digital library\ncan be better evaluated when its performance is assessed on multiple labeled\ndatasets with comparison to baselines. Tested on three types of labeled data\ncontaining 5,000 ~ 700K disambiguated names and 6M pairs of disambiguated\nnames, DBLP is shown to assign author names quite accurately to distinct\nauthors, resulting in pairwise precision, recall, and F1 measures around 0.90\nor above overall. DBLP's author name disambiguation performs well even on large\nambiguous name blocks but deficiently on distinguishing authors with the same\nnames. When compared to other disambiguation algorithms, DBLP's disambiguation\nperformance is quite competitive, possibly due to its hybrid disambiguation\napproach combining algorithmic disambiguation and manual error correction. A\ndiscussion follows on strengths and weaknesses of labeled datasets used in this\nstudy for future efforts to evaluate author name disambiguation on a digital\nlibrary scale.\n",
                "Link": "http://arxiv.org/pdf/1806.10540v2",
                "arxiv_id": "1806.10540v2",
                "publicationDate": "2018-06-27T15:49:27Z"
            },
            {
                "Title": "Occurrence Statistics of Entities, Relations and Types on the Web",
                "Authors": "Aman Madaan, Sunita Sarawagi",
                "Abstract": "  The problem of collecting reliable estimates of occurrence of entities on the\nopen web forms the premise for this report. The models learned for tagging\nentities cannot be expected to perform well when deployed on the web. This is\nowing to the severe mismatch in the distributions of such entities on the web\nand in the relatively diminutive training data. In this report, we build up the\ncase for maximum mean discrepancy for estimation of occurrence statistics of\nentities on the web, taking a review of named entity disambiguation techniques\nand related concepts along the way.\n",
                "Link": "http://arxiv.org/pdf/1605.04359v1",
                "arxiv_id": "1605.04359v1",
                "publicationDate": "2016-05-14T01:13:48Z"
            },
            {
                "Title": "The Fellowship of the Authors: Disambiguating Names from Social Network\n  Context",
                "Authors": "Ryan Muther, David Smith",
                "Abstract": "  Most NLP approaches to entity linking and coreference resolution focus on\nretrieving similar mentions using sparse or dense text representations. The\ncommon \"Wikification\" task, for instance, retrieves candidate Wikipedia\narticles for each entity mention. For many domains, such as bibliographic\ncitations, authority lists with extensive textual descriptions for each entity\nare lacking and ambiguous named entities mostly occur in the context of other\nnamed entities. Unlike prior work, therefore, we seek to leverage the\ninformation that can be gained from looking at association networks of\nindividuals derived from textual evidence in order to disambiguate names. We\ncombine BERT-based mention representations with a variety of graph induction\nstrategies and experiment with supervised and unsupervised cluster inference\nmethods. We experiment with data consisting of lists of names from two domains:\nbibliographic citations from CrossRef and chains of transmission (isnads) from\nclassical Arabic histories. We find that in-domain language model pretraining\ncan significantly improve mention representations, especially for larger\ncorpora, and that the availability of bibliographic information, such as\npublication venue or title, can also increase performance on this task. We also\npresent a novel supervised cluster inference model which gives competitive\nperformance for little computational effort, making it ideal for situations\nwhere individuals must be identified without relying on an exhaustive authority\nlist.\n",
                "Link": "http://arxiv.org/pdf/2209.00133v1",
                "arxiv_id": "2209.00133v1",
                "publicationDate": "2022-08-31T21:51:55Z"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1",
                "publicationDate": "2017-02-22T07:57:14Z"
            },
            {
                "Title": "Entity Disambiguation via Fusion Entity Decoding",
                "Authors": "Junxiong Wang, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M. Rush, Umar Farooq Minhas, Yunyao Li",
                "Abstract": "  Entity disambiguation (ED), which links the mentions of ambiguous entities to\ntheir referent entities in a knowledge base, serves as a core component in\nentity linking (EL). Existing generative approaches demonstrate improved\naccuracy compared to classification approaches under the standardized ZELDA\nbenchmark. Nevertheless, generative approaches suffer from the need for\nlarge-scale pre-training and inefficient generation. Most importantly, entity\ndescriptions, which could contain crucial information to distinguish similar\nentities from each other, are often overlooked. We propose an encoder-decoder\nmodel to disambiguate entities with more detailed entity descriptions. Given\ntext and candidate entities, the encoder learns interactions between the text\nand each candidate entity, producing representations for each entity candidate.\nThe decoder then fuses the representations of entity candidates together and\nselects the correct entity. Our experiments, conducted on various entity\ndisambiguation benchmarks, demonstrate the strong and robust performance of\nthis model, particularly +1.5% in the ZELDA benchmark compared with GENRE.\nFurthermore, we integrate this approach into the retrieval/reader framework and\nobserve +1.5% improvements in end-to-end entity linking in the GERBIL benchmark\ncompared with EntQA.\n",
                "Link": "http://arxiv.org/pdf/2404.01626v1",
                "arxiv_id": "2404.01626v1",
                "publicationDate": "2024-04-02T04:27:54Z"
            },
            {
                "Title": "Biomedical Mention Disambiguation using a Deep Learning Approach",
                "Authors": "Chih-Hsuan Wei, Kyubum Lee, Robert Leaman, Zhiyong Lu",
                "Abstract": "  Automatically locating named entities in natural language text - named entity\nrecognition - is an important task in the biomedical domain. Many named entity\nmentions are ambiguous between several bioconcept types, however, causing text\nspans to be annotated as more than one type when simultaneously recognizing\nmultiple entity types. The straightforward solution is a rule-based approach\napplying a priority order based on the precision of each entity tagger (from\nhighest to lowest). While this method is straightforward and useful, imprecise\ndisambiguation remains a significant source of error. We address this issue by\ngenerating a partially labeled corpus of ambiguous concept mentions. We first\ncollect named entity mentions from multiple human-curated databases (e.g.\nCTDbase, gene2pubmed), then correlate them with the text mined span from\nPubTator to provide the context where the mention appears. Our corpus contains\nmore than 3 million concept mentions that ambiguous between one or more concept\ntypes in PubTator (about 3% of all mentions). We approached this task as a\nclassification problem and developed a deep learning-based method which uses\nthe semantics of the span being classified and the surrounding words to\nidentify the most likely bioconcept type. More specifically, we develop a\nconvolutional neural network (CNN) and along short-term memory (LSTM) network\nto respectively handle the semantic syntax features, then concatenate these\nwithin a fully connected layer for final classification. The priority ordering\nrule-based approach demonstrated F1-scores of 71.29% (micro-averaged) and\n41.19% (macro-averaged), while the new disambiguation method demonstrated\nF1-scores of 91.94% (micro-averaged) and 85.42% (macro-averaged), a very\nsubstantial increase.\n",
                "Link": "http://arxiv.org/pdf/1909.10416v1",
                "arxiv_id": "1909.10416v1",
                "publicationDate": "2019-09-23T15:14:56Z"
            },
            {
                "Title": "Improving Named Entity Recognition by Jointly Learning to Disambiguate\n  Morphological Tags",
                "Authors": "Onur G\u00fcng\u00f6r, Suzan \u00dcsk\u00fcdarl\u0131, Tunga G\u00fcng\u00f6r",
                "Abstract": "  Previous studies have shown that linguistic features of a word such as\npossession, genitive or other grammatical cases can be employed in word\nrepresentations of a named entity recognition (NER) tagger to improve the\nperformance for morphologically rich languages. However, these taggers require\nexternal morphological disambiguation (MD) tools to function which are hard to\nobtain or non-existent for many languages. In this work, we propose a model\nwhich alleviates the need for such disambiguators by jointly learning NER and\nMD taggers in languages for which one can provide a list of candidate\nmorphological analyses. We show that this can be done independent of the\nmorphological annotation schemes, which differ among languages. Our experiments\nemploying three different model architectures that join these two tasks show\nthat joint learning improves NER performance. Furthermore, the morphological\ndisambiguator's performance is shown to be competitive.\n",
                "Link": "http://arxiv.org/pdf/1807.06683v1",
                "arxiv_id": "1807.06683v1",
                "publicationDate": "2018-07-17T21:46:02Z"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2",
                "publicationDate": "2022-08-20T15:21:35Z"
            },
            {
                "Title": "Disambiguation of Company names via Deep Recurrent Networks",
                "Authors": "Alessandro Basile, Riccardo Crupi, Michele Grasso, Alessandro Mercanti, Daniele Regoli, Simone Scarsi, Shuyi Yang, Andrea Cosentini",
                "Abstract": "  Name Entity Disambiguation is the Natural Language Processing task of\nidentifying textual records corresponding to the same Named Entity, i.e.\nreal-world entities represented as a list of attributes (names, places,\norganisations, etc.). In this work, we face the task of disambiguating\ncompanies on the basis of their written names. We propose a Siamese LSTM\nNetwork approach to extract -- via supervised learning -- an embedding of\ncompany name strings in a (relatively) low dimensional vector space and use\nthis representation to identify pairs of company names that actually represent\nthe same company (i.e. the same Entity).\n  Given that the manual labelling of string pairs is a rather onerous task, we\nanalyse how an Active Learning approach to prioritise the samples to be\nlabelled leads to a more efficient overall learning pipeline.\n  With empirical investigations, we show that our proposed Siamese Network\noutperforms several benchmark approaches based on standard string matching\nalgorithms when enough labelled data are available. Moreover, we show that\nActive Learning prioritisation is indeed helpful when labelling resources are\nlimited, and let the learning models reach the out-of-sample performance\nsaturation with less labelled data with respect to standard (random) data\nlabelling approaches.\n",
                "Link": "http://arxiv.org/pdf/2303.05391v2",
                "arxiv_id": "2303.05391v2",
                "publicationDate": "2023-03-07T15:07:57Z"
            },
            {
                "Title": "DAMO-NLP at NLPCC-2022 Task 2: Knowledge Enhanced Robust NER for Speech\n  Entity Linking",
                "Authors": "Shen Huang, Yuchen Zhai, Xinwei Long, Yong Jiang, Xiaobin Wang, Yin Zhang, Pengjun Xie",
                "Abstract": "  Speech Entity Linking aims to recognize and disambiguate named entities in\nspoken languages. Conventional methods suffer gravely from the unfettered\nspeech styles and the noisy transcripts generated by ASR systems. In this\npaper, we propose a novel approach called Knowledge Enhanced Named Entity\nRecognition (KENER), which focuses on improving robustness through painlessly\nincorporating proper knowledge in the entity recognition stage and thus\nimproving the overall performance of entity linking. KENER first retrieves\ncandidate entities for a sentence without mentions, and then utilizes the\nentity descriptions as extra information to help recognize mentions. The\ncandidate entities retrieved by a dense retrieval module are especially useful\nwhen the input is short or noisy. Moreover, we investigate various data\nsampling strategies and design effective loss functions, in order to improve\nthe quality of retrieved entities in both recognition and disambiguation\nstages. Lastly, a linking with filtering module is applied as the final\nsafeguard, making it possible to filter out wrongly-recognized mentions. Our\nsystem achieves 1st place in Track 1 and 2nd place in Track 2 of NLPCC-2022\nShared Task 2.\n",
                "Link": "http://arxiv.org/pdf/2209.13187v2",
                "arxiv_id": "2209.13187v2",
                "publicationDate": "2022-09-27T06:43:56Z"
            },
            {
                "Title": "LATTE: Latent Type Modeling for Biomedical Entity Linking",
                "Authors": "Ming Zhu, Busra Celikkaya, Parminder Bhatia, Chandan K. Reddy",
                "Abstract": "  Entity linking is the task of linking mentions of named entities in natural\nlanguage text, to entities in a curated knowledge-base. This is of significant\nimportance in the biomedical domain, where it could be used to semantically\nannotate a large volume of clinical records and biomedical literature, to\nstandardized concepts described in an ontology such as Unified Medical Language\nSystem (UMLS). We observe that with precise type information, entity\ndisambiguation becomes a straightforward task. However, fine-grained type\ninformation is usually not available in biomedical domain. Thus, we propose\nLATTE, a LATent Type Entity Linking model, that improves entity linking by\nmodeling the latent fine-grained type information about mentions and entities.\nUnlike previous methods that perform entity linking directly between the\nmentions and the entities, LATTE jointly does entity disambiguation, and latent\nfine-grained type learning, without direct supervision. We evaluate our model\non two biomedical datasets: MedMentions, a large scale public dataset annotated\nwith UMLS concepts, and a de-identified corpus of dictated doctor's notes that\nhas been annotated with ICD concepts. Extensive experimental evaluation shows\nour model achieves significant performance improvements over several\nstate-of-the-art techniques.\n",
                "Link": "http://arxiv.org/pdf/1911.09787v2",
                "arxiv_id": "1911.09787v2",
                "publicationDate": "2019-11-21T23:55:15Z"
            },
            {
                "Title": "AmbigDocs: Reasoning across Documents on Different Entities under the\n  Same Name",
                "Authors": "Yoonsang Lee, Xi Ye, Eunsol Choi",
                "Abstract": "  Different entities with the same name can be difficult to distinguish.\nHandling confusing entity mentions is a crucial skill for language models\n(LMs). For example, given the question \"Where was Michael Jordan educated?\" and\na set of documents discussing different people named Michael Jordan, can LMs\ndistinguish entity mentions to generate a cohesive answer to the question? To\ntest this ability, we introduce a new benchmark, AmbigDocs. By leveraging\nWikipedia's disambiguation pages, we identify a set of documents, belonging to\ndifferent entities who share an ambiguous name. From these documents, we\ngenerate questions containing an ambiguous name and their corresponding sets of\nanswers. Our analysis reveals that current state-of-the-art models often yield\nambiguous answers or incorrectly merge information belonging to different\nentities. We establish an ontology categorizing four types of incomplete\nanswers and automatic evaluation metrics to identify such categories. We lay\nthe foundation for future work on reasoning across multiple documents with\nambiguous entities.\n",
                "Link": "http://arxiv.org/pdf/2404.12447v1",
                "arxiv_id": "2404.12447v1",
                "publicationDate": "2024-04-18T18:12:01Z"
            },
            {
                "Title": "Overview of the Ugglan Entity Discovery and Linking System",
                "Authors": "Marcus Klang, Firas Dib, Pierre Nugues",
                "Abstract": "  Ugglan is a system designed to discover named entities and link them to\nunique identifiers in a knowledge base. It is based on a combination of a name\nand nominal dictionary derived from Wikipedia and Wikidata, a named entity\nrecognition module (NER) using fixed ordinally-forgetting encoding (FOFE)\ntrained on the TAC EDL data from 2014-2016, a candidate generation module from\nthe Wikipedia link graph across multiple editions, a PageRank link and\ncooccurrence graph disambiguator, and finally a reranker trained on the TAC EDL\n2015-2016 data.\n",
                "Link": "http://arxiv.org/pdf/1903.05498v1",
                "arxiv_id": "1903.05498v1",
                "publicationDate": "2019-03-13T14:03:50Z"
            },
            {
                "Title": "Strong Heuristics for Named Entity Linking",
                "Authors": "Marko \u010culjak, Andreas Spitz, Robert West, Akhil Arora",
                "Abstract": "  Named entity linking (NEL) in news is a challenging endeavour due to the\nfrequency of unseen and emerging entities, which necessitates the use of\nunsupervised or zero-shot methods. However, such methods tend to come with\ncaveats, such as no integration of suitable knowledge bases (like Wikidata) for\nemerging entities, a lack of scalability, and poor interpretability. Here, we\nconsider person disambiguation in Quotebank, a massive corpus of\nspeaker-attributed quotations from the news, and investigate the suitability of\nintuitive, lightweight, and scalable heuristics for NEL in web-scale corpora.\nOur best performing heuristic disambiguates 94% and 63% of the mentions on\nQuotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the\nproposed heuristics compare favourably to the state-of-the-art unsupervised and\nzero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as\nstrong baselines for unsupervised and zero-shot entity linking.\n",
                "Link": "http://arxiv.org/pdf/2207.02824v1",
                "arxiv_id": "2207.02824v1",
                "publicationDate": "2022-07-06T17:29:30Z"
            },
            {
                "Title": "Improving Few-shot and Zero-shot Entity Linking with Coarse-to-Fine\n  Lexicon-based Retriever",
                "Authors": "Shijue Huang, Bingbing Wang, Libo Qin, Qin Zhao, Ruifeng Xu",
                "Abstract": "  Few-shot and zero-shot entity linking focus on the tail and emerging\nentities, which are more challenging but closer to real-world scenarios. The\nmainstream method is the ''retrieve and rerank'' two-stage framework. In this\npaper, we propose a coarse-to-fine lexicon-based retriever to retrieve entity\ncandidates in an effective manner, which operates in two layers. The first\nlayer retrieves coarse-grained candidates by leveraging entity names, while the\nsecond layer narrows down the search to fine-grained candidates within the\ncoarse-grained ones. In addition, this second layer utilizes entity\ndescriptions to effectively disambiguate tail or new entities that share names\nwith existing popular entities. Experimental results indicate that our approach\ncan obtain superior performance without requiring extensive finetuning in the\nretrieval stage. Notably, our approach ranks the 1st in NLPCC 2023 Shared Task\n6 on Chinese Few-shot and Zero-shot Entity Linking.\n",
                "Link": "http://arxiv.org/pdf/2308.03365v2",
                "arxiv_id": "2308.03365v2",
                "publicationDate": "2023-08-07T07:39:43Z"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1",
                "publicationDate": "2014-10-08T10:01:47Z"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6",
                "publicationDate": "2021-07-12T16:09:22Z"
            },
            {
                "Title": "Generating automatically labeled data for author name disambiguation: An\n  iterative clustering method",
                "Authors": "Jinseok Kim, Jinmo Kim, Jason Owen-Smith",
                "Abstract": "  To train algorithms for supervised author name disambiguation, many studies\nhave relied on hand-labeled truth data that are very laborious to generate.\nThis paper shows that labeled training data can be automatically generated\nusing information features such as email address, coauthor names, and cited\nreferences that are available from publication records. For this purpose,\nhigh-precision rules for matching name instances on each feature are decided\nusing an external-authority database. Then, selected name instances in target\nambiguous data go through the process of pairwise matching based on the rules.\nNext, they are merged into clusters by a generic entity resolution algorithm.\nThe clustering procedure is repeated over other features until further merging\nis impossible. Tested on 26,566 instances out of the population of 228K author\nname instances, this iterative clustering produced accurately labeled data with\npairwise F1 = 0.99. The labeled data represented the population data in terms\nof name ethnicity and co-disambiguating name group size distributions. In\naddition, trained on the labeled data, machine learning algorithms\ndisambiguated 24K names in test data with performance of pairwise F1 = 0.90 ~\n0.92. Several challenges are discussed for applying this method to resolving\nauthor name ambiguity in large-scale scholarly data.\n",
                "Link": "http://arxiv.org/pdf/2102.03272v1",
                "arxiv_id": "2102.03272v1",
                "publicationDate": "2021-02-05T16:24:25Z"
            },
            {
                "Title": "NASTyLinker: NIL-Aware Scalable Transformer-based Entity Linker",
                "Authors": "Nicolas Heist, Heiko Paulheim",
                "Abstract": "  Entity Linking (EL) is the task of detecting mentions of entities in text and\ndisambiguating them to a reference knowledge base. Most prevalent EL approaches\nassume that the reference knowledge base is complete. In practice, however, it\nis necessary to deal with the case of linking to an entity that is not\ncontained in the knowledge base (NIL entity). Recent works have shown that,\ninstead of focusing only on affinities between mentions and entities,\nconsidering inter-mention affinities can be used to represent NIL entities by\nproducing clusters of mentions. At the same time, inter-mention affinities can\nhelp to substantially improve linking performance for known entities. With\nNASTyLinker, we introduce an EL approach that is aware of NIL entities and\nproduces corresponding mention clusters while maintaining high linking\nperformance for known entities. The approach clusters mentions and entities\nbased on dense representations from Transformers and resolves conflicts (if\nmore than one entity is assigned to a cluster) by computing transitive\nmention-entity affinities. We show the effectiveness and scalability of\nNASTyLinker on NILK, a dataset that is explicitly constructed to evaluate EL\nwith respect to NIL entities. Further, we apply the presented approach to an\nactual EL task, namely to knowledge graph population by linking entities in\nWikipedia listings, and provide an analysis of the outcome.\n",
                "Link": "http://arxiv.org/pdf/2303.04426v2",
                "arxiv_id": "2303.04426v2",
                "publicationDate": "2023-03-08T08:08:57Z"
            },
            {
                "Title": "Story Disambiguation: Tracking Evolving News Stories across News and\n  Social Streams",
                "Authors": "Bichen Shi, Thanh-Binh Le, Neil Hurley, Georgiana Ifrim",
                "Abstract": "  Following a particular news story online is an important but difficult task,\nas the relevant information is often scattered across different domains/sources\n(e.g., news articles, blogs, comments, tweets), presented in various formats\nand language styles, and may overlap with thousands of other stories. In this\nwork we join the areas of topic tracking and entity disambiguation, and propose\na framework named Story Disambiguation - a cross-domain story tracking approach\nthat builds on real-time entity disambiguation and a learning-to-rank framework\nto represent and update the rich semantic structure of news stories. Given a\ntarget news story, specified by a seed set of documents, the goal is to\neffectively select new story-relevant documents from an incoming document\nstream. We represent stories as entity graphs and we model the story tracking\nproblem as a learning-to-rank task. This enables us to track content with high\naccuracy, from multiple domains, in real-time. We study a range of text, entity\nand graph based features to understand which type of features are most\neffective for representing stories. We further propose new semi-supervised\nlearning techniques to automatically update the story representation over time.\nOur empirical study shows that we outperform the accuracy of state-of-the-art\nmethods for tracking mixed-domain document streams, while requiring fewer\nlabeled data to seed the tracked stories. This is particularly the case for\nlocal news stories that are easily over shadowed by other trending stories, and\nfor complex news stories with ambiguous content in noisy stream environments.\n",
                "Link": "http://arxiv.org/pdf/1808.05906v1",
                "arxiv_id": "1808.05906v1",
                "publicationDate": "2018-08-16T09:02:37Z"
            },
            {
                "Title": "Collective Entity Disambiguation with Structured Gradient Tree Boosting",
                "Authors": "Yi Yang, Ozan Irsoy, Kazi Shefaet Rahman",
                "Abstract": "  We present a gradient-tree-boosting-based structured learning model for\njointly disambiguating named entities in a document. Gradient tree boosting is\na widely used machine learning algorithm that underlies many top-performing\nnatural language processing systems. Surprisingly, most works limit the use of\ngradient tree boosting as a tool for regular classification or regression\nproblems, despite the structured nature of language. To the best of our\nknowledge, our work is the first one that employs the structured gradient tree\nboosting (SGTB) algorithm for collective entity disambiguation. By defining\nglobal features over previous disambiguation decisions and jointly modeling\nthem with local features, our system is able to produce globally optimized\nentity assignments for mentions in a document. Exact inference is prohibitively\nexpensive for our globally normalized model. To solve this problem, we propose\nBidirectional Beam Search with Gold path (BiBSG), an approximate inference\nalgorithm that is a variant of the standard beam search algorithm. BiBSG makes\nuse of global information from both past and future to perform better local\nsearch. Experiments on standard benchmark datasets show that SGTB significantly\nimproves upon published results. Specifically, SGTB outperforms the previous\nstate-of-the-art neural system by near 1\\% absolute accuracy on the popular\nAIDA-CoNLL dataset.\n",
                "Link": "http://arxiv.org/pdf/1802.10229v2",
                "arxiv_id": "1802.10229v2",
                "publicationDate": "2018-02-28T02:01:30Z"
            },
            {
                "Title": "Term Relevance Feedback for Contextual Named Entity Retrieval",
                "Authors": "Sheikh Muhammad Sarwar, John Foley, James Allan",
                "Abstract": "  We address the role of a user in Contextual Named Entity Retrieval (CNER),\nshowing (1) that user identification of important context-bearing terms is\nsuperior to automated approaches, and (2) that further gains are possible if\nthe user indicates the relative importance of those terms. CNER is similar in\nspirit to List Question answering and Entity disambiguation. However, the main\nfocus of CNER is to obtain user feedback for constructing a profile for a class\nof entities on the fly and use that to retrieve entities from free text. Given\na sentence, and an entity selected from that sentence, CNER aims to retrieve\nsentences that have entities similar to query entity. This paper explores\nobtaining term relevance feedback and importance weighting from humans in order\nto improve a CNER system. We report our findings based on the efforts of IR\nresearchers as well as crowdsourced workers.\n",
                "Link": "http://arxiv.org/pdf/1801.02687v1",
                "arxiv_id": "1801.02687v1",
                "publicationDate": "2018-01-08T21:06:49Z"
            },
            {
                "Title": "Boosting Question Answering by Deep Entity Recognition",
                "Authors": "Piotr Przyby\u0142a",
                "Abstract": "  In this paper an open-domain factoid question answering system for Polish,\nRAFAEL, is presented. The system goes beyond finding an answering sentence; it\nalso extracts a single string, corresponding to the required entity. Herein the\nfocus is placed on different approaches to entity recognition, essential for\nretrieving information matching question constraints. Apart from traditional\napproach, including named entity recognition (NER) solutions, a novel\ntechnique, called Deep Entity Recognition (DeepER), is introduced and\nimplemented. It allows a comprehensive search of all forms of entity references\nmatching a given WordNet synset (e.g. an impressionist), based on a previously\nassembled entity library. It has been created by analysing the first sentences\nof encyclopaedia entries and disambiguation and redirect pages. DeepER also\nprovides automatic evaluation, which makes possible numerous experiments,\nincluding over a thousand questions from a quiz TV show answered on the grounds\nof Polish Wikipedia. The final results of a manual evaluation on a separate\nquestion set show that the strength of DeepER approach lies in its ability to\nanswer questions that demand answers beyond the traditional categories of named\nentities.\n",
                "Link": "http://arxiv.org/pdf/1605.08675v1",
                "arxiv_id": "1605.08675v1",
                "publicationDate": "2016-05-27T14:57:37Z"
            }
        ]
    },
    {
        "topic_name": "Core NLP Pipeline",
        "summary": "default",
        "papers": [
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "Potrika: Raw and Balanced Newspaper Datasets in the Bangla Language with\n  Eight Topics and Five Attributes",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Knowledge is central to human and scientific developments. Natural Language\nProcessing (NLP) allows automated analysis and creation of knowledge. Data is a\ncrucial NLP and machine learning ingredient. The scarcity of open datasets is a\nwell-known problem in machine and deep learning research. This is very much the\ncase for textual NLP datasets in English and other major world languages. For\nthe Bangla language, the situation is even more challenging and the number of\nlarge datasets for NLP research is practically nil. We hereby present Potrika,\na large single-label Bangla news article textual dataset curated for NLP\nresearch from six popular online news portals in Bangladesh (Jugantor,\nJaijaidin, Ittefaq, Kaler Kontho, Inqilab, and Somoyer Alo) for the period\n2014-2020. The articles are classified into eight distinct categories\n(National, Sports, International, Entertainment, Economy, Education, Politics,\nand Science \\& Technology) providing five attributes (News Article, Category,\nHeadline, publipublicationDate, and Newspaper Source). The raw dataset contains\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles. Moreover, using NLP augmentation techniques, we create from the raw\n(unbalanced) dataset another (balanced) dataset comprising 320,000 news\narticles with 40,000 articles in each of the eight news categories. Potrika\ncontains both the datasets (raw and balanced) to suit a wide range of NLP\nresearch. By far, to the best of our knowledge, Potrika is the largest and the\nmost extensive dataset for news classification.\n",
                "publicationDate": "2022-10-17T19:37:42Z",
                "Link": "http://arxiv.org/pdf/2210.09389v1",
                "arxiv_id": "2210.09389v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "publicationDate": "2020-04-19T07:42:22Z",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Antara Mahmud",
                "Abstract": "  One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n",
                "publicationDate": "2023-09-19T02:59:41Z",
                "Link": "http://arxiv.org/pdf/2309.10272v2",
                "arxiv_id": "2309.10272v2"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            },
            {
                "Title": "Vacaspati: A Diverse Corpus of Bangla Literature",
                "Authors": "Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya",
                "Abstract": "  Bangla (or Bengali) is the fifth most spoken language globally; yet, the\nstate-of-the-art NLP in Bangla is lagging for even simple tasks such as\nlemmatization, POS tagging, etc. This is partly due to lack of a varied quality\ncorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla\nliterature. The literary works are collected from various websites; only those\nworks that are publicly available without copyright violations or restrictions\nare collected. We believe that published literature captures the features of a\nlanguage much better than newspapers, blogs or social media posts which tend to\nfollow only a certain literary pattern and, therefore, miss out on language\nvariety. Our corpus Vacaspati is varied from multiple aspects, including type\nof composition, topic, author, time, space, etc. It contains more than 11\nmillion sentences and 115 million words. We also built a word embedding model,\nVac-FT, using FastText from Vacaspati as well as trained an Electra model,\nVac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only\na fraction of resources compared to other state-of-the-art transformer models\nand yet performs either better or similar on various downstream tasks. On\nmultiple downstream tasks, Vac-FT outperforms other FastText-based models. We\nalso demonstrate the efficacy of Vacaspati as a corpus by showing that similar\nmodels built from other corpora are not as effective. The models are available\nat https://bangla.iitk.ac.in/.\n",
                "publicationDate": "2023-07-11T07:32:12Z",
                "Link": "http://arxiv.org/pdf/2307.05083v1",
                "arxiv_id": "2307.05083v1"
            },
            {
                "Title": "Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition",
                "Authors": "Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  One of the major challenges for developing automatic speech recognition (ASR)\nfor low-resource languages is the limited access to labeled data with\ndomain-specific variations. In this study, we propose a pseudo-labeling\napproach to develop a large-scale domain-agnostic ASR dataset. With the\nproposed methodology, we developed a 20k+ hours labeled Bangla speech dataset\ncovering diverse topics, speaking styles, dialects, noisy environments, and\nconversational scenarios. We then exploited the developed corpus to design a\nconformer-based ASR system. We benchmarked the trained ASR with publicly\navailable datasets and compared it with other available models. To investigate\nthe efficacy, we designed and developed a human-annotated domain-agnostic test\nset composed of news, telephony, and conversational data among others. Our\nresults demonstrate the efficacy of the model trained on psuedo-label data for\nthe designed test-set along with publicly-available Bangla datasets. The\nexperimental resources will be publicly\navailable.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)\n",
                "publicationDate": "2023-11-06T15:37:14Z",
                "Link": "http://arxiv.org/pdf/2311.03196v1",
                "arxiv_id": "2311.03196v1"
            },
            {
                "Title": "A Pipeline For Discourse Circuits From CCG",
                "Authors": "Jonathon Liu, Razin A. Shaikh, Benjamin Rodatz, Richie Yeung, Bob Coecke",
                "Abstract": "  There is a significant disconnect between linguistic theory and modern NLP\npractice, which relies heavily on inscrutable black-box architectures.\nDisCoCirc is a newly proposed model for meaning that aims to bridge this\ndivide, by providing neuro-symbolic models that incorporate linguistic\nstructure. DisCoCirc represents natural language text as a `circuit' that\ncaptures the core semantic information of the text. These circuits can then be\ninterpreted as modular machine learning models. Additionally, DisCoCirc fulfils\nanother major aim of providing an NLP model that can be implemented on\nnear-term quantum computers.\n  In this paper we describe a software pipeline that converts English text to\nits DisCoCirc representation. The pipeline achieves coverage over a large\nfragment of the English language. It relies on Combinatory Categorial Grammar\n(CCG) parses of the input text as well as coreference resolution information.\nThis semantic and syntactic information is used in several steps to convert the\ntext into a simply-typed $\\lambda$-calculus term, and then into a circuit\ndiagram. This pipeline will enable the application of the DisCoCirc framework\nto NLP tasks, using both classical and quantum approaches.\n",
                "publicationDate": "2023-11-29T18:46:29Z",
                "Link": "http://arxiv.org/pdf/2311.17892v1",
                "arxiv_id": "2311.17892v1"
            },
            {
                "Title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
                "Authors": "Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam",
                "Abstract": "  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n",
                "publicationDate": "2023-03-22T17:35:35Z",
                "Link": "http://arxiv.org/pdf/2303.12772v1",
                "arxiv_id": "2303.12772v1"
            },
            {
                "Title": "Spark NLP: Natural Language Understanding at Scale",
                "Authors": "Veysel Kocaman, David Talby",
                "Abstract": "  Spark NLP is a Natural Language Processing (NLP) library built on top of\nApache Spark ML. It provides simple, performant and accurate NLP annotations\nfor machine learning pipelines that can scale easily in a distributed\nenvironment. Spark NLP comes with 1100 pre trained pipelines and models in more\nthan 192 languages. It supports nearly all the NLP tasks and modules that can\nbe used seamlessly in a cluster. Downloaded more than 2.7 million times and\nexperiencing nine times growth since January 2020, Spark NLP is used by 54% of\nhealthcare organizations as the worlds most widely used NLP library in the\nenterprise.\n",
                "publicationDate": "2021-01-26T15:11:52Z",
                "Link": "http://arxiv.org/pdf/2101.10848v1",
                "arxiv_id": "2101.10848v1"
            },
            {
                "Title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT",
                "Authors": "Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim",
                "Abstract": "  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n",
                "publicationDate": "2024-03-08T18:42:59Z",
                "Link": "http://arxiv.org/pdf/2403.05519v1",
                "arxiv_id": "2403.05519v1"
            },
            {
                "Title": "BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla",
                "Authors": "Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim",
                "Abstract": "  Initial work has been done to address fake news detection and\nmisrepresentation of news in the Bengali language. However, no work in Bengali\nyet addresses the identification of specific claims in social media news that\nfalsely manipulates a related news article. At this point, this problem has\nbeen tackled in English and a few other languages, but not in the Bengali\nlanguage. In this paper, we curate a dataset of social media content labeled\nwith information manipulation relative to reference articles, called BanMANI.\nThe dataset collection method we describe works around the limitations of the\navailable NLP tools in Bangla. We expect these techniques will carry over to\nbuilding similar datasets in other low-resource languages. BanMANI forms the\nbasis both for evaluating the capabilities of existing NLP systems and for\ntraining or fine-tuning new models specifically on this task. In our analysis,\nwe find that this task challenges current LLMs both under zero-shot and\nfine-tuned settings.\n",
                "publicationDate": "2023-11-05T05:49:57Z",
                "Link": "http://arxiv.org/pdf/2311.02570v1",
                "arxiv_id": "2311.02570v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "Offensive Language Identification in Transliterated and Code-Mixed\n  Bangla",
                "Authors": "Md Nishat Raihan, Umma Hani Tanmoy, Anika Binte Islam, Kai North, Tharindu Ranasinghe, Antonios Anastasopoulos, Marcos Zampieri",
                "Abstract": "  Identifying offensive content in social media is vital for creating safe\nonline communities. Several recent studies have addressed this problem by\ncreating datasets for various languages. In this paper, we explore offensive\nlanguage identification in texts with transliterations and code-mixing,\nlinguistic phenomena common in multilingual societies, and a known challenge\nfor NLP systems. We introduce TB-OLID, a transliterated Bangla offensive\nlanguage dataset containing 5,000 manually annotated comments. We train and\nfine-tune machine learning models on TB-OLID, and we evaluate their results on\nthis dataset. Our results show that English pre-trained transformer-based\nmodels, such as fBERT and HateBERT achieve the best performance on this\ndataset.\n",
                "publicationDate": "2023-11-25T13:27:22Z",
                "Link": "http://arxiv.org/pdf/2311.15023v1",
                "arxiv_id": "2311.15023v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "Vulgar Remarks Detection in Chittagonian Dialect of Bangla",
                "Authors": "Tanjim Mahmud, Michal Ptaszynski, Fumito Masui",
                "Abstract": "  The negative effects of online bullying and harassment are increasing with\nInternet popularity, especially in social media. One solution is using natural\nlanguage processing (NLP) and machine learning (ML) methods for the automatic\ndetection of harmful remarks, but these methods are limited in low-resource\nlanguages like the Chittagonian dialect of Bangla.This study focuses on\ndetecting vulgar remarks in social media using supervised ML and deep learning\nalgorithms.Logistic Regression achieved promising accuracy (0.91) while simple\nRNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the\nissue that NN algorithms require more data.\n",
                "publicationDate": "2023-08-29T17:19:32Z",
                "Link": "http://arxiv.org/pdf/2308.15448v1",
                "arxiv_id": "2308.15448v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Real-time Bangla License Plate Recognition System for Low Resource\n  Video-based Applications",
                "Authors": "Alif Ashrafee, Akib Mohammed Khan, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Automatic License Plate Recognition systems aim to provide a solution for\ndetecting, localizing, and recognizing license plate characters from vehicles\nappearing in video frames. However, deploying such systems in the real world\nrequires real-time performance in low-resource environments. In our paper, we\npropose a two-stage detection pipeline paired with Vision API that provides\nreal-time inference speed along with consistently accurate detection and\nrecognition performance. We used a haar-cascade classifier as a filter on top\nof our backbone MobileNet SSDv2 detection model. This reduces inference time by\nonly focusing on high confidence detections and using them for recognition. We\nalso impose a temporal frame separation strategy to distinguish between\nmultiple vehicle license plates in the same clip. Furthermore, there are no\npublicly available Bangla license plate datasets, for which we created an image\ndataset and a video dataset containing license plates in the wild. We trained\nour models on the image dataset and achieved an AP(0.5) score of 86% and tested\nour pipeline on the video dataset and observed reasonable detection and\nrecognition performance (82.7% detection rate, and 60.8% OCR F1 score) with\nreal-time processing speed (27.2 frames per second).\n",
                "publicationDate": "2021-08-18T18:31:01Z",
                "Link": "http://arxiv.org/pdf/2108.08339v3",
                "arxiv_id": "2108.08339v3"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "LatinCy: Synthetic Trained Pipelines for Latin NLP",
                "Authors": "Patrick J. Burns",
                "Abstract": "  This paper introduces LatinCy, a set of trained general purpose\nLatin-language \"core\" pipelines for use with the spaCy natural language\nprocessing framework. The models are trained on a large amount of available\nLatin data, including all five of the Latin Universal Dependency treebanks,\nwhich have been preprocessed to be compatible with each other. The result is a\nset of general models for Latin with good performance on a number of natural\nlanguage processing tasks (e.g. the top-performing model yields POS tagging,\n97.41% accuracy; lemmatization, 94.66% accuracy; morphological tagging 92.76%\naccuracy). The paper describes the model training, including its training data\nand parameterization, and presents the advantages to Latin-language researchers\nof having a spaCy model available for NLP work.\n",
                "publicationDate": "2023-05-07T19:59:01Z",
                "Link": "http://arxiv.org/pdf/2305.04365v1",
                "arxiv_id": "2305.04365v1"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            }
        ]
    },
    {
        "topic_name": "Entity Linking",
        "summary": "default",
        "papers": [
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa",
                "Authors": "Rahul Mehta, Vasudeva Varma",
                "Abstract": "  Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n",
                "publicationDate": "2023-05-05T06:05:45Z",
                "Link": "http://arxiv.org/pdf/2305.03300v1",
                "arxiv_id": "2305.03300v1"
            },
            {
                "Title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition",
                "Authors": "Beiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-Hua Ling, Quan Liu",
                "Abstract": "  This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n",
                "publicationDate": "2022-03-07T09:05:37Z",
                "Link": "http://arxiv.org/pdf/2203.03216v2",
                "arxiv_id": "2203.03216v2"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "publicationDate": "2022-04-21T08:40:17Z",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "On the Temporality of Priors in Entity Linking",
                "Authors": "Renato Stoffalette Joao",
                "Abstract": "  Entity linking is a fundamental task in natural language processing which\ndeals with the lexical ambiguity in texts. An important component in entity\nlinking approaches is the mention-to-entity prior probability. Even though\nthere is a large number of works in entity linking, the existing approaches do\nnot explicitly consider the time aspect, specifically the temporality of an\nentity's prior probability. We posit that this prior probability is temporal in\nnature and affects the performance of entity linking systems. In this paper we\nsystematically study the effect of the prior on the entity linking performance\nover the temporal validity of both texts and KBs.\n",
                "publicationDate": "2021-01-14T13:58:31Z",
                "Link": "http://arxiv.org/pdf/2101.05593v1",
                "arxiv_id": "2101.05593v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Empirical Evaluation of Pretraining Strategies for Supervised Entity\n  Linking",
                "Authors": "Thibault F\u00e9vry, Nicholas FitzGerald, Livio Baldini Soares, Tom Kwiatkowski",
                "Abstract": "  In this work, we present an entity linking model which combines a Transformer\narchitecture with large scale pretraining from Wikipedia links. Our model\nachieves the state-of-the-art on two commonly used entity linking datasets:\n96.7% on CoNLL and 94.9% on TAC-KBP. We present detailed analyses to understand\nwhat design choices are important for entity linking, including choices of\nnegative entity candidates, Transformer architecture, and input perturbations.\nLastly, we present promising results on more challenging settings such as\nend-to-end entity linking and entity linking without in-domain training data.\n",
                "publicationDate": "2020-05-28T19:32:52Z",
                "Link": "http://arxiv.org/pdf/2005.14253v1",
                "arxiv_id": "2005.14253v1"
            },
            {
                "Title": "Entity Disambiguation via Fusion Entity Decoding",
                "Authors": "Junxiong Wang, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M. Rush, Umar Farooq Minhas, Yunyao Li",
                "Abstract": "  Entity disambiguation (ED), which links the mentions of ambiguous entities to\ntheir referent entities in a knowledge base, serves as a core component in\nentity linking (EL). Existing generative approaches demonstrate improved\naccuracy compared to classification approaches under the standardized ZELDA\nbenchmark. Nevertheless, generative approaches suffer from the need for\nlarge-scale pre-training and inefficient generation. Most importantly, entity\ndescriptions, which could contain crucial information to distinguish similar\nentities from each other, are often overlooked. We propose an encoder-decoder\nmodel to disambiguate entities with more detailed entity descriptions. Given\ntext and candidate entities, the encoder learns interactions between the text\nand each candidate entity, producing representations for each entity candidate.\nThe decoder then fuses the representations of entity candidates together and\nselects the correct entity. Our experiments, conducted on various entity\ndisambiguation benchmarks, demonstrate the strong and robust performance of\nthis model, particularly +1.5% in the ZELDA benchmark compared with GENRE.\nFurthermore, we integrate this approach into the retrieval/reader framework and\nobserve +1.5% improvements in end-to-end entity linking in the GERBIL benchmark\ncompared with EntQA.\n",
                "publicationDate": "2024-04-02T04:27:54Z",
                "Link": "http://arxiv.org/pdf/2404.01626v1",
                "arxiv_id": "2404.01626v1"
            },
            {
                "Title": "Clustering-based Inference for Biomedical Entity Linking",
                "Authors": "Rico Angell, Nicholas Monath, Sunil Mohan, Nishant Yadav, Andrew McCallum",
                "Abstract": "  Due to large number of entities in biomedical knowledge bases, only a small\nfraction of entities have corresponding labelled training data. This\nnecessitates entity linking models which are able to link mentions of unseen\nentities using learned representations of entities. Previous approaches link\neach mention independently, ignoring the relationships within and across\ndocuments between the entity mentions. These relations can be very useful for\nlinking mentions in biomedical text where linking decisions are often difficult\ndue mentions having a generic or a highly specialized form. In this paper, we\nintroduce a model in which linking decisions can be made not merely by linking\nto a knowledge base entity but also by grouping multiple mentions together via\nclustering and jointly making linking predictions. In experiments on the\nlargest publicly available biomedical dataset, we improve the best independent\nprediction for entity linking by 3.0 points of accuracy, and our\nclustering-based inference model further improves entity linking by 2.3 points.\n",
                "publicationDate": "2020-10-21T19:16:27Z",
                "Link": "http://arxiv.org/pdf/2010.11253v2",
                "arxiv_id": "2010.11253v2"
            },
            {
                "Title": "Entity Linking with people entity on Wikipedia",
                "Authors": "Weiqian Yan, Kanchan Khurad",
                "Abstract": "  This paper introduces a new model that uses named entity recognition,\ncoreference resolution, and entity linking techniques, to approach the task of\nlinking people entities on Wikipedia people pages to their corresponding\nWikipedia pages if applicable. Our task is different from general and\ntraditional entity linking because we are working in a limited domain, namely,\npeople entities, and we are including pronouns as entities, whereas in the\npast, pronouns were never considered as entities in entity linking. We have\nbuilt 2 models, both outperforms our baseline model significantly. The purpose\nof our project is to build a model that could be use to generate cleaner data\nfor future entity linking tasks. Our contribution include a clean data set\nconsisting of 50Wikipedia people pages, and 2 entity linking models,\nspecifically tuned for this domain.\n",
                "publicationDate": "2017-05-02T16:06:03Z",
                "Link": "http://arxiv.org/pdf/1705.01042v1",
                "arxiv_id": "1705.01042v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout\n  Analysis",
                "Authors": "Shrestha Datta, Md Adith Mollah, Raisa Fairooz, Tariful Islam Fahim",
                "Abstract": "  Understanding digital documents is like solving a puzzle, especially\nhistorical ones. Document Layout Analysis (DLA) helps with this puzzle by\ndividing documents into sections like paragraphs, images, and tables. This is\ncrucial for machines to read and understand these documents. In the DL Sprint\n2.0 competition, we worked on understanding Bangla documents. We used a dataset\ncalled BaDLAD with lots of examples. We trained a special model called Mask\nR-CNN to help with this understanding. We made this model better by\nstep-by-step hyperparameter tuning, and we achieved a good dice score of 0.889.\nHowever, not everything went perfectly. We tried using a model trained for\nEnglish documents, but it didn't fit well with Bangla. This showed us that each\nlanguage has its own challenges. Our solution for the DL Sprint 2.0 is publicly\navailable at https://www.kaggle.com/competitions/dlsprint2/discussion/432201\nalong with notebooks, weights, and inference notebook.\n",
                "publicationDate": "2023-08-21T06:51:58Z",
                "Link": "http://arxiv.org/pdf/2308.10511v2",
                "arxiv_id": "2308.10511v2"
            },
            {
                "Title": "Fine-Grained Entity Typing for Domain Independent Entity Linking",
                "Authors": "Yasumasa Onoe, Greg Durrett",
                "Abstract": "  Neural entity linking models are very powerful, but run the risk of\noverfitting to the domain they are trained in. For this problem, a domain is\ncharacterized not just by genre of text but even by factors as specific as the\nparticular distribution of entities, as neural models tend to overfit by\nmemorizing properties of frequent entities in a dataset. We tackle the problem\nof building robust entity linking models that generalize effectively and do not\nrely on labeled entity linking data with a specific entity distribution. Rather\nthan predicting entities directly, our approach models fine-grained entity\nproperties, which can help disambiguate between even closely related entities.\nWe derive a large inventory of types (tens of thousands) from Wikipedia\ncategories, and use hyperlinked mentions in Wikipedia to distantly label data\nand train an entity typing model. At test time, we classify a mention with this\ntyping model and use soft type predictions to link the mention to the most\nsimilar candidate entity. We evaluate our entity linking system on the\nCoNLL-YAGO dataset (Hoffart et al., 2011) and show that our approach\noutperforms prior domain-independent entity linking systems. We also test our\napproach in a harder setting derived from the WikilinksNED dataset (Eshel et\nal., 2017) where all the mention-entity pairs are unseen during test time.\nResults indicate that our approach generalizes better than a state-of-the-art\nneural model on the dataset.\n",
                "publicationDate": "2019-09-12T16:29:24Z",
                "Link": "http://arxiv.org/pdf/1909.05780v2",
                "arxiv_id": "1909.05780v2"
            },
            {
                "Title": "Visual Named Entity Linking: A New Dataset and A Baseline",
                "Authors": "Wenxiang Sun, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng",
                "Abstract": "  Visual Entity Linking (VEL) is a task to link regions of images with their\ncorresponding entities in Knowledge Bases (KBs), which is beneficial for many\ncomputer vision tasks such as image retrieval, image caption, and visual\nquestion answering. While existing tasks in VEL either rely on textual data to\ncomplement a multi-modal linking or only link objects with general entities,\nwhich fails to perform named entity linking on large amounts of image data. In\nthis paper, we consider a purely Visual-based Named Entity Linking (VNEL) task,\nwhere the input only consists of an image. The task is to identify objects of\ninterest (i.e., visual entity mentions) in images and link them to\ncorresponding named entities in KBs. Since each entity often contains rich\nvisual and textual information in KBs, we thus propose three different\nsub-tasks, i.e., visual to visual entity linking (V2VEL), visual to textual\nentity linking (V2TEL), and visual to visual-textual entity linking (V2VTEL).\nIn addition, we present a high-quality human-annotated visual person linking\ndataset, named WIKIPerson. Based on WIKIPerson, we establish a series of\nbaseline algorithms for the solution of each sub-task, and conduct experiments\nto verify the quality of proposed datasets and the effectiveness of baseline\nmethods. We envision this work to be helpful for soliciting more works\nregarding VNEL in the future. The codes and datasets are publicly available at\nhttps://github.com/ict-bigdatalab/VNEL.\n",
                "publicationDate": "2022-11-09T13:27:50Z",
                "Link": "http://arxiv.org/pdf/2211.04872v1",
                "arxiv_id": "2211.04872v1"
            },
            {
                "Title": "SpEL: Structured Prediction for Entity Linking",
                "Authors": "Hassan S. Shavarani, Anoop Sarkar",
                "Abstract": "  Entity linking is a prominent thread of research focused on structured data\ncreation by linking spans of text to an ontology or knowledge source. We\nrevisit the use of structured prediction for entity linking which classifies\neach individual input token as an entity, and aggregates the token predictions.\nOur system, called SpEL (Structured prediction for Entity Linking) is a\nstate-of-the-art entity linking system that uses some new ideas to apply\nstructured prediction to the task of entity linking including: two refined\nfine-tuning steps; a context sensitive prediction aggregation strategy;\nreduction of the size of the model's output vocabulary, and; we address a\ncommon problem in entity-linking systems where there is a training vs.\ninference tokenization mismatch. Our experiments show that we can outperform\nthe state-of-the-art on the commonly used AIDA benchmark dataset for entity\nlinking to Wikipedia. Our method is also very compute efficient in terms of\nnumber of parameters and speed of inference.\n",
                "publicationDate": "2023-10-23T08:24:35Z",
                "Link": "http://arxiv.org/pdf/2310.14684v1",
                "arxiv_id": "2310.14684v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Personal Entity, Concept, and Named Entity Linking in Conversations",
                "Authors": "Hideaki Joko, Faegheh Hasibi",
                "Abstract": "  Building conversational agents that can have natural and knowledge-grounded\ninteractions with humans requires understanding user utterances. Entity Linking\n(EL) is an effective and widely used method for understanding natural language\ntext and connecting it to external knowledge. It is, however, shown that\nexisting EL methods developed for annotating documents are suboptimal for\nconversations, where personal entities (e.g., \"my cars\") and concepts are\nessential for understanding user utterances. In this paper, we introduce a\ncollection and a tool for entity linking in conversations. We collect EL\nannotations for 1327 conversational utterances, consisting of links to named\nentities, concepts, and personal entities. The dataset is used for training our\ntoolkit for conversational entity linking, CREL. Unlike existing EL methods,\nCREL is developed to identify both named entities and concepts. It also\nutilizes coreference resolution techniques to identify personal entities and\nreferences to the explicit entity mentions in the conversations. We compare\nCREL with state-of-the-art techniques and show that it outperforms all existing\nbaselines.\n",
                "publicationDate": "2022-06-15T22:32:29Z",
                "Link": "http://arxiv.org/pdf/2206.07836v3",
                "arxiv_id": "2206.07836v3"
            },
            {
                "Title": "Community Question Answering Entity Linking via Leveraging Auxiliary\n  Data",
                "Authors": "Yuhan Li, Wei Shen, Jianbo Gao, Yadong Wang",
                "Abstract": "  Community Question Answering (CQA) platforms contain plenty of CQA texts\n(i.e., questions and answers corresponding to the question) where named\nentities appear ubiquitously. In this paper, we define a new task of CQA entity\nlinking (CQAEL) as linking the textual entity mentions detected from CQA texts\nwith their corresponding entities in a knowledge base. This task can facilitate\nmany downstream applications including expert finding and knowledge base\nenrichment. Traditional entity linking methods mainly focus on linking entities\nin news documents, and are suboptimal over this new task of CQAEL since they\ncannot effectively leverage various informative auxiliary data involved in the\nCQA platform to aid entity linking, such as parallel answers and two types of\nmeta-data (i.e., topic tags and users). To remedy this crucial issue, we\npropose a novel transformer-based framework to effectively harness the\nknowledge delivered by different kinds of auxiliary data to promote the linking\nperformance. We validate the superiority of our framework through extensive\nexperiments over a newly released CQAEL data set against state-of-the-art\nentity linking methods.\n",
                "publicationDate": "2022-05-24T09:25:18Z",
                "Link": "http://arxiv.org/pdf/2205.11917v1",
                "arxiv_id": "2205.11917v1"
            },
            {
                "Title": "Learning to Select the Next Reasonable Mention for Entity Linking",
                "Authors": "Jian Sun, Yu Zhou, Chengqing Zong",
                "Abstract": "  Entity linking aims to establish a link between entity mentions in a document\nand the corresponding entities in knowledge graphs (KGs). Previous work has\nshown the effectiveness of global coherence for entity linking. However, most\nof the existing global linking methods based on sequential decisions focus on\nhow to utilize previously linked entities to enhance the later decisions. In\nthose methods, the order of mention is fixed, making the model unable to adjust\nthe subsequent linking targets according to the previously linked results,\nwhich will cause the previous information to be unreasonably utilized. To\naddress the problem, we propose a novel model, called DyMen, to dynamically\nadjust the subsequent linking target based on the previously linked entities\nvia reinforcement learning, enabling the model to select a link target that can\nfully use previously linked information. We sample mention by sliding window to\nreduce the action sampling space of reinforcement learning and maintain the\nsemantic coherence of mention. Experiments conducted on several benchmark\ndatasets have shown the effectiveness of the proposed model.\n",
                "publicationDate": "2021-12-08T04:12:50Z",
                "Link": "http://arxiv.org/pdf/2112.04104v1",
                "arxiv_id": "2112.04104v1"
            },
            {
                "Title": "Person Entity Profiling Framework: Identifying, Integrating and\n  Visualizing Online Freely Available Entity-Related Information",
                "Authors": "Saeed Amal, Einat Minkov, Tsvi Kuflik",
                "Abstract": "  When we consider our CV, it is full of entities that we are or were\nassociated with and that define us in some way(s). Such entities include where\nwe studied, where we worked, who we collaborated with on a project or on a\npaper etc. Entities we are linked to are part of who we are and may reveal\nabout what we are interested in. Hence, we can view any CV as a graph of\ninterlinked entities, where nodes are entities and edges are relations between\nthem. This study proposes a novel entity search framework that in response to a\nreal-time query about an entity, searches, crawls, analyzes and consolidates\nrelevant information that is freely available on the Web about the entity of\ninterest, culminating in the generation a profile of the searched entity.\nUnlike typical entity search settings, in which a ranked list of entities\nrelated to the target entity over a pre-specified relation is processed, we\npresent and visualize rich information about the entity of interest as a typed\nentity-relation graph without an apriori definition of the types of related\nentities and relations. This view is structured and compact, making it easy to\nunderstand as well as interpret. It enables the user to learn not only about\nthe entity in question, but also about related entities, thereby obtaining a\nbetter understanding of the entity in question. We evaluated each of the\nframeworks components separately and then performed an overall evaluation of\nthe framework, its visualization and the interest of users in the results. The\nresults show that the proposed framework performs entity searches, related\nentity identification and relation identification very well and that it\nsatisfies users needs.\n",
                "publicationDate": "2021-10-02T08:53:17Z",
                "Link": "http://arxiv.org/pdf/2110.00759v1",
                "arxiv_id": "2110.00759v1"
            },
            {
                "Title": "TempEL: Linking Dynamically Evolving and Newly Emerging Entities",
                "Authors": "Klim Zaporojets, Lucie-Aimee Kaffee, Johannes Deleu, Thomas Demeester, Chris Develder, Isabelle Augenstein",
                "Abstract": "  In our continuously evolving world, entities change over time and new,\npreviously non-existing or unknown, entities appear. We study how this\nevolutionary scenario impacts the performance on a well established entity\nlinking (EL) task. For that study, we introduce TempEL, an entity linking\ndataset that consists of time-stratified English Wikipedia snapshots from 2013\nto 2022, from which we collect both anchor mentions of entities, and these\ntarget entities' descriptions. By capturing such temporal aspects, our newly\nintroduced TempEL resource contrasts with currently existing entity linking\ndatasets, which are composed of fixed mentions linked to a single static\nversion of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA).\nIndeed, for each of our collected temporal snapshots, TempEL contains links to\nentities that are continual, i.e., occur in all of the years, as well as\ncompletely new entities that appear for the first time at some point. Thus, we\nenable to quantify the performance of current state-of-the-art EL models for:\n(i) entities that are subject to changes over time in their Knowledge Base\ndescriptions as well as their mentions' contexts, and (ii) newly created\nentities that were previously non-existing (e.g., at the time the EL model was\ntrained). Our experimental results show that in terms of temporal performance\ndegradation, (i) continual entities suffer a decrease of up to 3.1% EL\naccuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This\nhighlights the challenge of the introduced TempEL dataset and opens new\nresearch prospects in the area of time-evolving entity disambiguation.\n",
                "publicationDate": "2023-02-05T22:34:36Z",
                "Link": "http://arxiv.org/pdf/2302.02500v1",
                "arxiv_id": "2302.02500v1"
            },
            {
                "Title": "A Read-and-Select Framework for Zero-shot Entity Linking",
                "Authors": "Zhenran Xu, Yulin Chen, Baotian Hu, Min Zhang",
                "Abstract": "  Zero-shot entity linking (EL) aims at aligning entity mentions to unseen\nentities to challenge the generalization ability. Previous methods largely\nfocus on the candidate retrieval stage and ignore the essential candidate\nranking stage, which disambiguates among entities and makes the final linking\nprediction. In this paper, we propose a read-and-select (ReS) framework by\nmodeling the main components of entity disambiguation, i.e., mention-entity\nmatching and cross-entity comparison. First, for each candidate, the reading\nmodule leverages mention context to output mention-aware entity\nrepresentations, enabling mention-entity matching. Then, in the selecting\nmodule, we frame the choice of candidates as a sequence labeling problem, and\nall candidate representations are fused together to enable cross-entity\ncomparison. Our method achieves the state-of-the-art performance on the\nestablished zero-shot EL dataset ZESHEL with a 2.55% micro-average accuracy\ngain, with no need for laborious multi-phase pre-training used in most of the\nprevious work, showing the effectiveness of both mention-entity and\ncross-entity interaction.\n",
                "publicationDate": "2023-10-19T04:08:10Z",
                "Link": "http://arxiv.org/pdf/2310.12450v2",
                "arxiv_id": "2310.12450v2"
            },
            {
                "Title": "Improving Entity Linking through Semantic Reinforced Entity Embeddings",
                "Authors": "Feng Hou, Ruili Wang, Jun He, Yi Zhou",
                "Abstract": "  Entity embeddings, which represent different aspects of each entity with a\nsingle vector like word embeddings, are a key component of neural entity\nlinking models. Existing entity embeddings are learned from canonical Wikipedia\narticles and local contexts surrounding target entities. Such entity embeddings\nare effective, but too distinctive for linking models to learn contextual\ncommonality. We propose a simple yet effective method, FGS2EE, to inject\nfine-grained semantic information into entity embeddings to reduce the\ndistinctiveness and facilitate the learning of contextual commonality. FGS2EE\nfirst uses the embeddings of semantic type words to generate semantic\nembeddings, and then combines them with existing entity embeddings through\nlinear aggregation. Extensive experiments show the effectiveness of such\nembeddings. Based on our entity embeddings, we achieved new sate-of-the-art\nperformance on entity linking.\n",
                "publicationDate": "2021-06-16T00:27:56Z",
                "Link": "http://arxiv.org/pdf/2106.08495v1",
                "arxiv_id": "2106.08495v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Improving Fine-grained Entity Typing with Entity Linking",
                "Authors": "Hongliang Dai, Donghong Du, Xin Li, Yangqiu Song",
                "Abstract": "  Fine-grained entity typing is a challenging problem since it usually involves\na relatively large tag set and may require to understand the context of the\nentity mention. In this paper, we use entity linking to help with the\nfine-grained entity type classification process. We propose a deep neural model\nthat makes predictions based on both the context and the information obtained\nfrom entity linking results. Experimental results on two commonly used datasets\ndemonstrates the effectiveness of our approach. On both datasets, it achieves\nmore than 5\\% absolute strict accuracy improvement over the state of the art.\n",
                "publicationDate": "2019-09-26T13:20:10Z",
                "Link": "http://arxiv.org/pdf/1909.12079v1",
                "arxiv_id": "1909.12079v1"
            },
            {
                "Title": "Linking Graph Entities with Multiplicity and Provenance",
                "Authors": "Jixue Liu, Selasi Kwashie, Jiuyong Li, Lin Liu, Michael Bewong",
                "Abstract": "  Entity linking and resolution is a fundamental database problem with\napplications in data integration, data cleansing, information retrieval,\nknowledge fusion, and knowledge-base population. It is the task of accurately\nidentifying multiple, differing, and possibly contradicting representations of\nthe same real-world entity in data. In this work, we propose an entity linking\nand resolution system capable of linking entities across different databases\nand mentioned-entities extracted from text data. Our entity linking/resolution\nsolution, called Certus, uses a graph model to represent the profiles of\nentities. The graph model is versatile, thus, it is capable of handling\nmultiple values for an attribute or a relationship, as well as the provenance\ndescriptions of the values. Provenance descriptions of a value provide the\nsettings of the value, such as validity periods, sources, security\nrequirements, etc. This paper presents the architecture for the entity linking\nsystem, the logical, physical, and indexing models used in the system, and the\ngeneral linking process. Furthermore, we demonstrate the performance of update\noperations of the physical storage models when the system is implemented in two\nstate-of-the-art database management systems, HBase and Postgres.\n",
                "publicationDate": "2019-08-13T02:28:37Z",
                "Link": "http://arxiv.org/pdf/1908.04464v2",
                "arxiv_id": "1908.04464v2"
            },
            {
                "Title": "Learn to Not Link: Exploring NIL Prediction in Entity Linking",
                "Authors": "Fangwei Zhu, Jifan Yu, Hailong Jin, Juanzi Li, Lei Hou, Zhifang Sui",
                "Abstract": "  Entity linking models have achieved significant success via utilizing\npretrained language models to capture semantic features. However, the NIL\nprediction problem, which aims to identify mentions without a corresponding\nentity in the knowledge base, has received insufficient attention. We\ncategorize mentions linking to NIL into Missing Entity and Non-Entity Phrase,\nand propose an entity linking dataset NEL that focuses on the NIL prediction\nproblem. NEL takes ambiguous entities as seeds, collects relevant mention\ncontext in the Wikipedia corpus, and ensures the presence of mentions linking\nto NIL by human annotation and entity masking. We conduct a series of\nexperiments with the widely used bi-encoder and cross-encoder entity linking\nmodels, results show that both types of NIL mentions in training data have a\nsignificant influence on the accuracy of NIL prediction. Our code and dataset\ncan be accessed at https://github.com/solitaryzero/NIL_EL\n",
                "publicationDate": "2023-05-25T05:12:33Z",
                "Link": "http://arxiv.org/pdf/2305.15725v1",
                "arxiv_id": "2305.15725v1"
            }
        ]
    },
    {
        "topic_name": "Stemming",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Ranking the locations and predicting future crime occurrence by\n  retrieving news from different Bangla online newspapers",
                "Authors": "Jumman Hossain, Rajib Chandra Das, Md. Ruhul Amin, Md. Saiful Islam",
                "Abstract": "  There have thousands of crimes are happening daily all around. But people\nkeep statistics only few of them, therefore crime rates are increasing day by\nday. The reason behind can be less concern or less statistics of previous\ncrimes. It is much more important to observe the previous crime statistics for\ngeneral people to make their outing decision and police for catching the\ncriminals are taking steps to restrain the crimes and tourists to make their\ntravelling decision. National institute of justice releases crime survey data\nfor the country, but does not offer crime statistics up to Union or Thana\nlevel. Considering all of these cases we have come up with an approach which\ncan give an approximation to people about the safety of a specific location\nwith crime ranking of different areas locating the crimes on a map including a\nfuture crime occurrence prediction mechanism. Our approach relies on different\nonline Bangla newspapers for crawling the crime data, stemming and keyword\nextraction, location finding algorithm, cosine similarity, naive Bayes\nclassifier, and a custom crime prediction model\n",
                "publicationDate": "2023-05-18T04:19:26Z",
                "Link": "http://arxiv.org/pdf/2305.10698v1",
                "arxiv_id": "2305.10698v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051",
                "Authors": "Nasif Muslim, Md. Tanvir Adnan, Mohammad Zahidul Kabir, Md. Humayun Kabir, Sheikh Mominul Islam",
                "Abstract": "  In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance.\n",
                "publicationDate": "2012-08-05T09:22:06Z",
                "Link": "http://arxiv.org/pdf/1208.0995v1",
                "arxiv_id": "1208.0995v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            }
        ]
    },
    {
        "topic_name": "Text Generation",
        "summary": "default",
        "papers": [
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation\n  Language Models",
                "Authors": "Mitodru Niyogi, Arnab Bhattacharya",
                "Abstract": "  We present Gyan AI Paramanu (\"atom\"), a family of novel language models for\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\nand multilingual Indic language models pretrained from scratch on a single GPU\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\npretrained with a context size of 1024 on a single GPU. The models are very\nefficient, small, fast, and powerful. We have also developed an efficient most\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\navoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we\npretrained on comparable corpora by typological grouping using the same script.\nWe performed human evaluation of our pretrained models for open end text\ngeneration on grammar, coherence, creativity, and factuality metrics for\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\nmodels on 23k instructions in respective languages. Our pretrained and\ninstruction-tuned models which are first of its kind, most powerful efficient\nsmall generative language models ever developed for Indic languages, and the\nvarious results lead to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and humongous number\nof parameters. We plan to release our models at https://www.bharatgpts.com.\n",
                "publicationDate": "2024-01-31T17:58:10Z",
                "Link": "http://arxiv.org/pdf/2401.18034v1",
                "arxiv_id": "2401.18034v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Textual Toxicity in Social Media: Understanding the Bangla Toxic\n  Language Expressed in Facebook Comment",
                "Authors": "Mohammad Mamun Or Rashid",
                "Abstract": "  Social Media is a repository of digital literature including user-generated\ncontent. The users of social media are expressing their opinion with diverse\nmediums such as text, emojis, memes, and also through other visual and textual\nmediums. A major portion of these media elements could be treated as harmful to\nothers and they are known by many words including Cyberbullying and Toxic\nLanguage . The goal of this research paper is to analyze a curated and\nvalue-added dataset of toxic language titled ToxLex_bn . It is an exhaustive\nwordlist that can be used as classifier material to detect toxicity in social\nmedia. The toxic language/script used by the Bengali community as\ncyberbullying, hate speech and moral policing became major trends in social\nmedia culture in Bangladesh and West Bengal. The toxicity became so high that\nthe victims has to post as a counter or release explanation video for the\nhaters. Most cases are pointed to women celebrity and their relation, dress,\nlifestyle are became trolled and toxicity flooded in comments boxes. Not only\ncelebrity bashing but also hates occurred between Hindu Muslims,\nIndia-Bangladesh, Two opponents of 1971 and these are very common for virtual\nconflict in the comment thread. Even many times facebook comment causes sue and\nlegal matters in Bangladesh and thus it requires more study. In this study, a\nBangla toxic language dataset has been analyzed which was inputted by the user\nin Bengali script & language. For this, about 1968 unique bigrams or phrases as\nwordlists have been analyzed which are derived from 2207590 comments. It is\nassumed that this analysis will reinforce the detection of Bangla's toxic\nlanguage used in social media and thus cure this virtual disease.\n",
                "publicationDate": "2023-12-09T05:04:34Z",
                "Link": "http://arxiv.org/pdf/2312.05467v1",
                "arxiv_id": "2312.05467v1"
            },
            {
                "Title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform",
                "Authors": "Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar, Mita Nasipuri",
                "Abstract": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n",
                "publicationDate": "2020-09-17T03:14:27Z",
                "Link": "http://arxiv.org/pdf/2009.08037v1",
                "arxiv_id": "2009.08037v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "Pointer over Attention: An Improved Bangla Text Summarization Approach\n  Using Hybrid Pointer Generator Network",
                "Authors": "Nobel Dhar, Gaurob Saha, Prithwiraj Bhattacharjee, Avi Mallick, Md Saiful Islam",
                "Abstract": "  Despite the success of the neural sequence-to-sequence model for abstractive\ntext summarization, it has a few shortcomings, such as repeating inaccurate\nfactual details and tending to repeat themselves. We propose a hybrid pointer\ngenerator network to solve the shortcomings of reproducing factual details\ninadequately and phrase repetition. We augment the attention-based\nsequence-to-sequence using a hybrid pointer generator network that can generate\nOut-of-Vocabulary words and enhance accuracy in reproducing authentic details\nand a coverage mechanism that discourages repetition. It produces a\nreasonable-sized output text that preserves the conceptual integrity and\nfactual information of the input article. For evaluation, we primarily employed\n\"BANSData\" - a highly adopted publicly available Bengali dataset. Additionally,\nwe prepared a large-scale dataset called \"BANS-133\" which consists of 133k\nBangla news articles associated with human-generated summaries. Experimenting\nwith the proposed model, we achieved ROUGE-1 and ROUGE-2 scores of 0.66, 0.41\nfor the \"BANSData\" dataset and 0.67, 0.42 for the BANS-133k\" dataset,\nrespectively. We demonstrated that the proposed system surpasses previous\nstate-of-the-art Bengali abstractive summarization techniques and its stability\non a larger dataset. \"BANS-133\" datasets and code-base will be publicly\navailable for research.\n",
                "publicationDate": "2021-11-19T15:18:12Z",
                "Link": "http://arxiv.org/pdf/2111.10269v2",
                "arxiv_id": "2111.10269v2"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            }
        ]
    },
    {
        "topic_name": "Syntactic Parsing",
        "summary": "default",
        "papers": [
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Syntactic Structure Processing in the Brain while Listening",
                "Authors": "Subba Reddy Oota, Mounika Marreddy, Manish Gupta, Bapi Raju Surampud",
                "Abstract": "  Syntactic parsing is the task of assigning a syntactic structure to a\nsentence. There are two popular syntactic parsing methods: constituency and\ndependency parsing. Recent works have used syntactic embeddings based on\nconstituency trees, incremental top-down parsing, and other word syntactic\nfeatures for brain activity prediction given the text stimuli to study how the\nsyntax structure is represented in the brain's language network. However, the\neffectiveness of dependency parse trees or the relative predictive power of the\nvarious syntax parsers across brain areas, especially for the listening task,\nis yet unexplored. In this study, we investigate the predictive power of the\nbrain encoding models in three settings: (i) individual performance of the\nconstituency and dependency syntactic parsing based embedding methods, (ii)\nefficacy of these syntactic parsing based embedding methods when controlling\nfor basic syntactic signals, (iii) relative effectiveness of each of the\nsyntactic embedding methods when controlling for the other. Further, we explore\nthe relative importance of syntactic information (from these syntactic\nembedding methods) versus semantic information using BERT embeddings. We find\nthat constituency parsers help explain activations in the temporal lobe and\nmiddle-frontal gyrus, while dependency parsers better encode syntactic\nstructure in the angular gyrus and posterior cingulate cortex. Although\nsemantic signals from BERT are more effective compared to any of the syntactic\nfeatures or embedding methods, syntactic embedding methods explain additional\nvariance for a few brain regions.\n",
                "publicationDate": "2023-02-16T21:28:11Z",
                "Link": "http://arxiv.org/pdf/2302.08589v1",
                "arxiv_id": "2302.08589v1"
            },
            {
                "Title": "A Survey of Syntactic-Semantic Parsing Based on Constituent and\n  Dependency Structures",
                "Authors": "Meishan Zhang",
                "Abstract": "  Syntactic and semantic parsing has been investigated for decades, which is\none primary topic in the natural language processing community. This article\naims for a brief survey on this topic. The parsing community includes many\ntasks, which are difficult to be covered fully. Here we focus on two of the\nmost popular formalizations of parsing: constituent parsing and dependency\nparsing. Constituent parsing is majorly targeted to syntactic analysis, and\ndependency parsing can handle both syntactic and semantic analysis. This\narticle briefly reviews the representative models of constituent parsing and\ndependency parsing, and also dependency graph parsing with rich semantics.\nBesides, we also review the closely-related topics such as cross-domain,\ncross-lingual and joint parsing models, parser application as well as corpus\ndevelopment of parsing in the article.\n",
                "publicationDate": "2020-06-19T10:21:17Z",
                "Link": "http://arxiv.org/pdf/2006.11056v1",
                "arxiv_id": "2006.11056v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Syntactic representation learning for neural network based TTS with\n  syntactic parse tree traversal",
                "Authors": "Changhe Song, Jingbei Li, Yixuan Zhou, Zhiyong Wu, Helen Meng",
                "Abstract": "  Syntactic structure of a sentence text is correlated with the prosodic\nstructure of the speech that is crucial for improving the prosody and\nnaturalness of a text-to-speech (TTS) system. Nowadays TTS systems usually try\nto incorporate syntactic structure information with manually designed features\nbased on expert knowledge. In this paper, we propose a syntactic representation\nlearning method based on syntactic parse tree traversal to automatically\nutilize the syntactic structure information. Two constituent label sequences\nare linearized through left-first and right-first traversals from constituent\nparse tree. Syntactic representations are then extracted at word level from\neach constituent label sequence by a corresponding uni-directional gated\nrecurrent unit (GRU) network. Meanwhile, nuclear-norm maximization loss is\nintroduced to enhance the discriminability and diversity of the embeddings of\nconstituent labels. Upsampled syntactic representations and phoneme embeddings\nare concatenated to serve as the encoder input of Tacotron2. Experimental\nresults demonstrate the effectiveness of our proposed approach, with mean\nopinion score (MOS) increasing from 3.70 to 3.82 and ABX preference exceeding\nby 17% compared with the baseline. In addition, for sentences with multiple\nsyntactic parse trees, prosodic differences can be clearly perceived from the\nsynthesized speeches.\n",
                "publicationDate": "2020-12-13T05:52:07Z",
                "Link": "http://arxiv.org/pdf/2012.06971v1",
                "arxiv_id": "2012.06971v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Comparison of Syntactic Parsers on Biomedical Texts",
                "Authors": "Maria Biryukov",
                "Abstract": "  Syntactic parsing is an important step in the automated text analysis which\naims at information extraction. Quality of the syntactic parsing determines to\na large extent the recall and precision of the text mining results. In this\npaper we evaluate the performance of several popular syntactic parsers in\napplication to the biomedical text mining.\n",
                "publicationDate": "2020-08-17T10:07:23Z",
                "Link": "http://arxiv.org/pdf/2008.07189v1",
                "arxiv_id": "2008.07189v1"
            },
            {
                "Title": "Parsing All: Syntax and Semantics, Dependencies and Spans",
                "Authors": "Junru Zhou, Zuchao Li, Hai Zhao",
                "Abstract": "  Both syntactic and semantic structures are key linguistic contextual clues,\nin which parsing the latter has been well shown beneficial from parsing the\nformer. However, few works ever made an attempt to let semantic parsing help\nsyntactic parsing. As linguistic representation formalisms, both syntax and\nsemantics may be represented in either span (constituent/phrase) or dependency,\non both of which joint learning was also seldom explored. In this paper, we\npropose a novel joint model of syntactic and semantic parsing on both span and\ndependency representations, which incorporates syntactic information\neffectively in the encoder of neural network and benefits from two\nrepresentation formalisms in a uniform way. The experiments show that semantics\nand syntax can benefit each other by optimizing joint objectives. Our single\nmodel achieves new state-of-the-art or competitive results on both span and\ndependency semantic parsing on Propbank benchmarks and both dependency and\nconstituent syntactic parsing on Penn Treebank.\n",
                "publicationDate": "2019-08-30T03:49:19Z",
                "Link": "http://arxiv.org/pdf/1908.11522v3",
                "arxiv_id": "1908.11522v3"
            },
            {
                "Title": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing",
                "Authors": "Dominick Ng, Mohit Bansal, James R. Curran",
                "Abstract": "  We develop novel first- and second-order features for dependency parsing\nbased on the Google Syntactic Ngrams corpus, a collection of subtree counts of\nparsed sentences from scanned books. We also extend previous work on surface\n$n$-gram features from Web1T to the Google Books corpus and from first-order to\nsecond-order, comparing and analysing performance over newswire and web\ntreebanks.\n  Surface and syntactic $n$-grams both produce substantial and complementary\ngains in parsing accuracy across domains. Our best system combines the two\nfeature sets, achieving up to 0.8% absolute UAS improvements on newswire and\n1.4% on web text.\n",
                "publicationDate": "2015-02-25T03:27:38Z",
                "Link": "http://arxiv.org/pdf/1502.07038v1",
                "arxiv_id": "1502.07038v1"
            },
            {
                "Title": "Keystroke dynamics as signal for shallow syntactic parsing",
                "Authors": "Barbara Plank",
                "Abstract": "  Keystroke dynamics have been extensively used in psycholinguistic and writing\nresearch to gain insights into cognitive processing. But do keystroke logs\ncontain actual signal that can be used to learn better natural language\nprocessing models?\n  We postulate that keystroke dynamics contain information about syntactic\nstructure that can inform shallow syntactic parsing. To test this hypothesis,\nwe explore labels derived from keystroke logs as auxiliary task in a multi-task\nbidirectional Long Short-Term Memory (bi-LSTM). Our results show promising\nresults on two shallow syntactic parsing tasks, chunking and CCG supertagging.\nOur model is simple, has the advantage that data can come from distinct\nsources, and produces models that are significantly better than models trained\non the text annotations alone.\n",
                "publicationDate": "2016-10-11T13:20:52Z",
                "Link": "http://arxiv.org/pdf/1610.03321v1",
                "arxiv_id": "1610.03321v1"
            },
            {
                "Title": "Developing and Evaluating a Probabilistic LR Parser of Part-of-Speech\n  and Punctuation Labels",
                "Authors": "Ted Briscoe, John Carroll",
                "Abstract": "  We describe an approach to robust domain-independent syntactic parsing of\nunrestricted naturally-occurring (English) input. The technique involves\nparsing sequences of part-of-speech and punctuation labels using a\nunification-based grammar coupled with a probabilistic LR parser. We describe\nthe coverage of several corpora using this grammar and report the results of a\nparsing experiment using probabilities derived from bracketed training data. We\nreport the first substantial experiments to assess the contribution of\npunctuation to deriving an accurate syntactic analysis, by parsing identical\ntexts both with and without naturally-occurring punctuation marks.\n",
                "publicationDate": "1995-10-09T16:27:17Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9510005v1",
                "arxiv_id": "9510005v1"
            },
            {
                "Title": "Is Supervised Syntactic Parsing Beneficial for Language Understanding?\n  An Empirical Investigation",
                "Authors": "Goran Glava\u0161, Ivan Vuli\u0107",
                "Abstract": "  Traditional NLP has long held (supervised) syntactic parsing necessary for\nsuccessful higher-level semantic language understanding (LU). The recent advent\nof end-to-end neural models, self-supervised via language modeling (LM), and\ntheir success on a wide range of LU tasks, however, questions this belief. In\nthis work, we empirically investigate the usefulness of supervised parsing for\nsemantic LU in the context of LM-pretrained transformer networks. Relying on\nthe established fine-tuning paradigm, we first couple a pretrained transformer\nwith a biaffine parsing head, aiming to infuse explicit syntactic knowledge\nfrom Universal Dependencies treebanks into the transformer. We then fine-tune\nthe model for LU tasks and measure the effect of the intermediate parsing\ntraining (IPT) on downstream LU task performance. Results from both monolingual\nEnglish and zero-shot language transfer experiments (with intermediate\ntarget-language parsing) show that explicit formalized syntax, injected into\ntransformers through IPT, has very limited and inconsistent effect on\ndownstream LU performance. Our results, coupled with our analysis of\ntransformers' representation spaces before and after intermediate parsing, make\na significant step towards providing answers to an essential question: how\n(un)availing is supervised parsing for high-level semantic natural language\nunderstanding in the era of large neural models?\n",
                "publicationDate": "2020-08-15T21:03:36Z",
                "Link": "http://arxiv.org/pdf/2008.06788v2",
                "arxiv_id": "2008.06788v2"
            },
            {
                "Title": "Multi-Source Syntactic Neural Machine Translation",
                "Authors": "Anna Currey, Kenneth Heafield",
                "Abstract": "  We introduce a novel multi-source technique for incorporating source syntax\ninto neural machine translation using linearized parses. This is achieved by\nemploying separate encoders for the sequential and parsed versions of the same\nsource sentence; the resulting representations are then combined using a\nhierarchical attention mechanism. The proposed model improves over both seq2seq\nand parsed baselines by over 1 BLEU on the WMT17 English-German task. Further\nanalysis shows that our multi-source syntactic model is able to translate\nsuccessfully without any parsed input, unlike standard parsed methods. In\naddition, performance does not deteriorate as much on long sentences as for the\nbaselines.\n",
                "publicationDate": "2018-08-30T13:18:57Z",
                "Link": "http://arxiv.org/pdf/1808.10267v1",
                "arxiv_id": "1808.10267v1"
            },
            {
                "Title": "A Survey of Unsupervised Dependency Parsing",
                "Authors": "Wenjuan Han, Yong Jiang, Hwee Tou Ng, Kewei Tu",
                "Abstract": "  Syntactic dependency parsing is an important task in natural language\nprocessing. Unsupervised dependency parsing aims to learn a dependency parser\nfrom sentences that have no annotation of their correct parse trees. Despite\nits difficulty, unsupervised parsing is an interesting research direction\nbecause of its capability of utilizing almost unlimited unannotated text data.\nIt also serves as the basis for other research in low-resource parsing. In this\npaper, we survey existing approaches to unsupervised dependency parsing,\nidentify two major classes of approaches, and discuss recent trends. We hope\nthat our survey can provide insights for researchers and facilitate future\nresearch on this topic.\n",
                "publicationDate": "2020-10-04T10:51:22Z",
                "Link": "http://arxiv.org/pdf/2010.01535v1",
                "arxiv_id": "2010.01535v1"
            },
            {
                "Title": "Frame-Semantic Parsing with Softmax-Margin Segmental RNNs and a\n  Syntactic Scaffold",
                "Authors": "Swabha Swayamdipta, Sam Thomson, Chris Dyer, Noah A. Smith",
                "Abstract": "  We present a new, efficient frame-semantic parser that labels semantic\narguments to FrameNet predicates. Built using an extension to the segmental RNN\nthat emphasizes recall, our basic system achieves competitive performance\nwithout any calls to a syntactic parser. We then introduce a method that uses\nphrase-syntactic annotations from the Penn Treebank during training only,\nthrough a multitask objective; no parsing is required at training or test time.\nThis \"syntactic scaffold\" offers a cheaper alternative to traditional syntactic\npipelining, and achieves state-of-the-art performance.\n",
                "publicationDate": "2017-06-29T00:42:10Z",
                "Link": "http://arxiv.org/pdf/1706.09528v1",
                "arxiv_id": "1706.09528v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "A Conditional Splitting Framework for Efficient Constituency Parsing",
                "Authors": "Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li",
                "Abstract": "  We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.\n",
                "publicationDate": "2021-06-30T00:36:34Z",
                "Link": "http://arxiv.org/pdf/2106.15760v1",
                "arxiv_id": "2106.15760v1"
            },
            {
                "Title": "Penn-Helsinki Parsed Corpus of Early Modern English: First Parsing\n  Results and Analysis",
                "Authors": "Seth Kulick, Neville Ryant, Beatrice Santorini",
                "Abstract": "  We present the first parsing results on the Penn-Helsinki Parsed Corpus of\nEarly Modern English (PPCEME), a 1.9 million word treebank that is an important\nresource for research in syntactic change. We describe key features of PPCEME\nthat make it challenging for parsing, including a larger and more varied set of\nfunction tags than in the Penn Treebank. We present results for this corpus\nusing a modified version of the Berkeley Neural Parser and the approach to\nfunction tag recovery of Gabbard et al (2006). Despite its simplicity, this\napproach works surprisingly well, suggesting it is possible to recover the\noriginal structure with sufficient accuracy to support linguistic applications\n(e.g., searching for syntactic structures of interest). However, for a subset\nof function tags (e.g., the tag indicating direct speech), additional work is\nneeded, and we discuss some further limits of this approach. The resulting\nparser will be used to parse Early English Books Online, a 1.1 billion word\ncorpus whose utility for the study of syntactic change will be greatly\nincreased with the addition of accurate parse trees.\n",
                "publicationDate": "2021-12-15T23:56:21Z",
                "Link": "http://arxiv.org/pdf/2112.08532v1",
                "arxiv_id": "2112.08532v1"
            },
            {
                "Title": "Systematic Parsing of X.509: Eradicating Security Issues with a Parse\n  Tree",
                "Authors": "Alessandro Barenghi, Nicholas Mainardi, Gerardo Pelosi",
                "Abstract": "  X.509 certificate parsing and validation is a critical task which has shown\nconsistent lack of effectiveness, with practical attacks being reported with a\nsteady rate during the last 10 years. In this work we analyze the X.509\nstandard and provide a grammar description of it amenable to the automated\ngeneration of a parser with strong termination guarantees, providing\nunambiguous input parsing. We report the results of analyzing a 11M X.509\ncertificate dump of the HTTPS servers running on the entire IPv4 space, showing\nthat 21.5% of the certificates in use are syntactically invalid. We compare the\nresults of our parsing against 7 widely used TLS libraries showing that 631k to\n1,156k syntactically incorrect certificates are deemed valid by them\n(5.7%--10.5%), including instances with security critical mis-parsings. We\nprove the criticality of such mis-parsing exploiting one of the syntactic flaws\nfound in existing certificates to perform an impersonation attack.\n",
                "publicationDate": "2018-12-12T14:16:02Z",
                "Link": "http://arxiv.org/pdf/1812.04959v1",
                "arxiv_id": "1812.04959v1"
            },
            {
                "Title": "A Unifying Theory of Transition-based and Sequence Labeling Parsing",
                "Authors": "Carlos G\u00f3mez-Rodr\u00edguez, Michalina Strzyz, David Vilares",
                "Abstract": "  We define a mapping from transition-based parsing algorithms that read\nsentences from left to right to sequence labeling encodings of syntactic trees.\nThis not only establishes a theoretical relation between transition-based\nparsing and sequence-labeling parsing, but also provides a method to obtain new\nencodings for fast and simple sequence labeling parsing from the many existing\ntransition-based parsers for different formalisms. Applying it to dependency\nparsing, we implement sequence labeling versions of four algorithms, showing\nthat they are learnable and obtain comparable performance to existing\nencodings.\n",
                "publicationDate": "2020-11-01T18:25:15Z",
                "Link": "http://arxiv.org/pdf/2011.00584v1",
                "arxiv_id": "2011.00584v1"
            },
            {
                "Title": "Sparse Fuzzy Attention for Structured Sentiment Analysis",
                "Authors": "Letian Peng, Zuchao Li, Hai Zhao",
                "Abstract": "  Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n",
                "publicationDate": "2021-09-14T14:37:56Z",
                "Link": "http://arxiv.org/pdf/2109.06719v3",
                "arxiv_id": "2109.06719v3"
            },
            {
                "Title": "Encoder-Decoder Shift-Reduce Syntactic Parsing",
                "Authors": "Jiangming Liu, Yue Zhang",
                "Abstract": "  Starting from NMT, encoder-decoder neu- ral networks have been used for many\nNLP problems. Graph-based models and transition-based models borrowing the en-\ncoder components achieve state-of-the-art performance on dependency parsing and\nconstituent parsing, respectively. How- ever, there has not been work\nempirically studying the encoder-decoder neural net- works for transition-based\nparsing. We apply a simple encoder-decoder to this end, achieving comparable\nresults to the parser of Dyer et al. (2015) on standard de- pendency parsing,\nand outperforming the parser of Vinyals et al. (2015) on con- stituent parsing.\n",
                "publicationDate": "2017-06-24T04:08:11Z",
                "Link": "http://arxiv.org/pdf/1706.07905v1",
                "arxiv_id": "1706.07905v1"
            },
            {
                "Title": "Packrat Parsing: Simple, Powerful, Lazy, Linear Time",
                "Authors": "Bryan Ford",
                "Abstract": "  Packrat parsing is a novel technique for implementing parsers in a lazy\nfunctional programming language. A packrat parser provides the power and\nflexibility of top-down parsing with backtracking and unlimited lookahead, but\nnevertheless guarantees linear parse time. Any language defined by an LL(k) or\nLR(k) grammar can be recognized by a packrat parser, in addition to many\nlanguages that conventional linear-time algorithms do not support. This\nadditional power simplifies the handling of common syntactic idioms such as the\nwidespread but troublesome longest-match rule, enables the use of sophisticated\ndisambiguation strategies such as syntactic and semantic predicates, provides\nbetter grammar composition properties, and allows lexical analysis to be\nintegrated seamlessly into parsing. Yet despite its power, packrat parsing\nshares the same simplicity and elegance as recursive descent parsing; in fact\nconverting a backtracking recursive descent parser into a linear-time packrat\nparser often involves only a fairly straightforward structural change. This\npaper describes packrat parsing informally with emphasis on its use in\npractical applications, and explores its advantages and disadvantages with\nrespect to the more conventional alternatives.\n",
                "publicationDate": "2006-03-18T17:49:45Z",
                "Link": "http://arxiv.org/pdf/cs/0603077v1",
                "arxiv_id": "0603077v1"
            },
            {
                "Title": "Debugging Frame Semantic Role Labeling",
                "Authors": "Alexandre Kabbach",
                "Abstract": "  We propose a quantitative and qualitative analysis of the performances of\nstatistical models for frame semantic structure extraction. We report on a\nreplication study on FrameNet 1.7 data and show that preprocessing toolkits\nplay a major role in argument identification performances, observing gains\nsimilar in their order of magnitude to those reported by recent models for\nframe semantic parsing. We report on the robustness of a recent statistical\nclassifier for frame semantic parsing to lexical configurations of\npredicate-argument structures, relying on an artificially augmented dataset\ngenerated using a rule-based algorithm combining valence pattern matching and\nlexical substitution. We prove that syntactic pre-processing plays a major role\nin the performances of statistical classifiers to argument identification, and\ndiscuss the core reasons of syntactic mismatch between dependency parsers\noutput and FrameNet syntactic formalism. Finally, we suggest new leads for\nimproving statistical models for frame semantic parsing, including joint\nsyntax-semantic parsing relying on FrameNet syntactic formalism, latent classes\ninference via split-and-merge algorithms and neural network architectures\nrelying on rich input representations of words.\n",
                "publicationDate": "2019-01-22T17:17:02Z",
                "Link": "http://arxiv.org/pdf/1901.07475v1",
                "arxiv_id": "1901.07475v1"
            },
            {
                "Title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and\n  Semantic Parsing",
                "Authors": "Subhro Roy, Sam Thomson, Tongfei Chen, Richard Shin, Adam Pauls, Jason Eisner, Benjamin Van Durme",
                "Abstract": "  Recent work has shown that generation from a prompted or fine-tuned language\nmodel can perform well at semantic parsing when the output is constrained to be\na valid semantic representation. We introduce BenchCLAMP, a Benchmark to\nevaluate Constrained LAnguage Model Parsing, that includes context-free\ngrammars for seven semantic parsing datasets and two syntactic parsing datasets\nwith varied output representations, as well as a constrained decoding interface\nto generate only valid outputs covered by these grammars. We provide low,\nmedium, and high resource splits for each dataset, allowing accurate comparison\nof various language models under different data regimes. Our benchmark supports\nevaluation of language models using prompt-based learning as well as\nfine-tuning. We benchmark eight language models, including two GPT-3 variants\navailable only through an API. Our experiments show that encoder-decoder\npretrained language models can achieve similar performance or surpass\nstate-of-the-art methods for syntactic and semantic parsing when the model\noutput is constrained to be valid.\n",
                "publicationDate": "2022-06-21T18:34:11Z",
                "Link": "http://arxiv.org/pdf/2206.10668v2",
                "arxiv_id": "2206.10668v2"
            },
            {
                "Title": "Syntactic Topic Models",
                "Authors": "Jordan Boyd-Graber, David M. Blei",
                "Abstract": "  The syntactic topic model (STM) is a Bayesian nonparametric model of language\nthat discovers latent distributions of words (topics) that are both\nsemantically and syntactically coherent. The STM models dependency parsed\ncorpora where sentences are grouped into documents. It assumes that each word\nis drawn from a latent topic chosen by combining document-level features and\nthe local syntactic context. Each document has a distribution over latent\ntopics, as in topic models, which provides the semantic consistency. Each\nelement in the dependency parse tree also has a distribution over the topics of\nits children, as in latent-state syntax models, which provides the syntactic\nconsistency. These distributions are convolved so that the topic of each word\nis likely under both its document and syntactic context. We derive a fast\nposterior inference algorithm based on variational methods. We report\nqualitative and quantitative studies on both synthetic data and hand-parsed\ndocuments. We show that the STM is a more predictive model of language than\ncurrent models based only on syntax or only on topics.\n",
                "publicationDate": "2010-02-25T00:00:47Z",
                "Link": "http://arxiv.org/pdf/1002.4665v1",
                "arxiv_id": "1002.4665v1"
            },
            {
                "Title": "Semi-Supervised Methods for Out-of-Domain Dependency Parsing",
                "Authors": "Juntao Yu",
                "Abstract": "  Dependency parsing is one of the important natural language processing tasks\nthat assigns syntactic trees to texts. Due to the wider availability of\ndependency corpora and improved parsing and machine learning techniques,\nparsing accuracies of supervised learning-based systems have been significantly\nimproved. However, due to the nature of supervised learning, those parsing\nsystems highly rely on the manually annotated training corpora. They work\nreasonably good on the in-domain data but the performance drops significantly\nwhen tested on out-of-domain texts. To bridge the performance gap between\nin-domain and out-of-domain, this thesis investigates three semi-supervised\ntechniques for out-of-domain dependency parsing, namely co-training,\nself-training and dependency language models. Our approaches use easily\nobtainable unlabelled data to improve out-of-domain parsing accuracies without\nthe need of expensive corpora annotation. The evaluations on several English\ndomains and multi-lingual data show quite good improvements on parsing\naccuracy. Overall this work conducted a survey of semi-supervised methods for\nout-of-domain dependency parsing, where I extended and compared a number of\nimportant semi-supervised methods in a unified framework. The comparison\nbetween those techniques shows that self-training works equally well as\nco-training on out-of-domain parsing, while dependency language models can\nimprove both in- and out-of-domain accuracies.\n",
                "publicationDate": "2018-10-04T08:41:50Z",
                "Link": "http://arxiv.org/pdf/1810.02100v1",
                "arxiv_id": "1810.02100v1"
            },
            {
                "Title": "Learning Fault-tolerant Speech Parsing with SCREEN",
                "Authors": "Stefan Wermter, Volker Weber",
                "Abstract": "  This paper describes a new approach and a system SCREEN for fault-tolerant\nspeech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for\nNatural language. Speech parsing describes the syntactic and semantic analysis\nof spontaneous spoken language. The general approach is based on incremental\nimmediate flat analysis, learning of syntactic and semantic speech parsing,\nparallel integration of current hypotheses, and the consideration of various\nforms of speech related errors. The goal for this approach is to explore the\nparallel interactions between various knowledge sources for learning\nincremental fault-tolerant speech parsing. This approach is examined in a\nsystem SCREEN using various hybrid connectionist techniques. Hybrid\nconnectionist techniques are examined because of their promising properties of\ninherent fault tolerance, learning, gradedness and parallel constraint\nintegration. The input for SCREEN is hypotheses about recognized words of a\nspoken utterance potentially analyzed by a speech system, the output is\nhypotheses about the flat syntactic and semantic analysis of the utterance. In\nthis paper we focus on the general approach, the overall architecture, and\nexamples for learning flat syntactic speech parsing. Different from most other\nspeech language architectures SCREEN emphasizes an interactive rather than an\nautonomous position, learning rather than encoding, flat analysis rather than\nin-depth analysis, and fault-tolerant processing of phonetic, syntactic and\nsemantic knowledge.\n",
                "publicationDate": "1994-06-16T14:09:22Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9406024v1",
                "arxiv_id": "9406024v1"
            },
            {
                "Title": "A Robust Parser Based on Syntactic Information",
                "Authors": "Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, Gil Chang Kim",
                "Abstract": "  In this paper, we propose a robust parser which can parse extragrammatical\nsentences. This parser can recover them using only syntactic information. It\ncan be easily modified and extended because it utilize only syntactic\ninformation.\n",
                "publicationDate": "1995-02-21T01:57:32Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9502024v2",
                "arxiv_id": "9502024v2"
            },
            {
                "Title": "Concurrent Parsing of Constituency and Dependency",
                "Authors": "Junru Zhou, Shuailiang Zhang, Hai Zhao",
                "Abstract": "  Constituent and dependency representation for syntactic structure share a lot\nof linguistic and computational characteristics, this paper thus makes the\nfirst attempt by introducing a new model that is capable of parsing constituent\nand dependency at the same time, so that lets either of the parsers enhance\neach other. Especially, we evaluate the effect of different shared network\ncomponents and empirically verify that dependency parsing may be much more\nbeneficial from constituent parsing structure.\n  The proposed parser achieves new state-of-the-art performance for both\nparsing tasks, constituent and dependency on PTB and CTB benchmarks.\n",
                "publicationDate": "2019-08-18T05:10:59Z",
                "Link": "http://arxiv.org/pdf/1908.06379v2",
                "arxiv_id": "1908.06379v2"
            },
            {
                "Title": "ListOps: A Diagnostic Dataset for Latent Tree Learning",
                "Authors": "Nikita Nangia, Samuel R. Bowman",
                "Abstract": "  Latent tree learning models learn to parse a sentence without syntactic\nsupervision, and use that parse to build the sentence representation. Existing\nwork on such models has shown that, while they perform well on tasks like\nsentence classification, they do not learn grammars that conform to any\nplausible semantic or syntactic formalism (Williams et al., 2018a). Studying\nthe parsing ability of such models in natural language can be challenging due\nto the inherent complexities of natural language, like having several valid\nparses for a single sentence. In this paper we introduce ListOps, a toy dataset\ncreated to study the parsing ability of latent tree models. ListOps sequences\nare in the style of prefix arithmetic. The dataset is designed to have a single\ncorrect parsing strategy that a system needs to learn to succeed at the task.\nWe show that the current leading latent tree models are unable to learn to\nparse and succeed at ListOps. These models achieve accuracies worse than purely\nsequential RNNs.\n",
                "publicationDate": "2018-04-17T03:26:28Z",
                "Link": "http://arxiv.org/pdf/1804.06028v1",
                "arxiv_id": "1804.06028v1"
            },
            {
                "Title": "Graph Interpolation Grammars: a Rule-based Approach to the Incremental\n  Parsing of Natural Languages",
                "Authors": "John Larcheveque",
                "Abstract": "  Graph Interpolation Grammars are a declarative formalism with an operational\nsemantics. Their goal is to emulate salient features of the human parser, and\nnotably incrementality. The parsing process defined by GIGs incrementally\nbuilds a syntactic representation of a sentence as each successive lexeme is\nread. A GIG rule specifies a set of parse configurations that trigger its\napplication and an operation to perform on a matching configuration. Rules are\npartly context-sensitive; furthermore, they are reversible, meaning that their\noperations can be undone, which allows the parsing process to be\nnondeterministic. These two factors confer enough expressive power to the\nformalism for parsing natural languages.\n",
                "publicationDate": "1998-04-02T14:47:07Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9804001v1",
                "arxiv_id": "9804001v1"
            },
            {
                "Title": "On the Challenges of Fully Incremental Neural Dependency Parsing",
                "Authors": "Ana Ezquerro, Carlos G\u00f3mez-Rodr\u00edguez, David Vilares",
                "Abstract": "  Since the popularization of BiLSTMs and Transformer-based bidirectional\nencoders, state-of-the-art syntactic parsers have lacked incrementality,\nrequiring access to the whole sentence and deviating from human language\nprocessing. This paper explores whether fully incremental dependency parsing\nwith modern architectures can be competitive. We build parsers combining\nstrictly left-to-right neural encoders with fully incremental sequence-labeling\nand transition-based decoders. The results show that fully incremental parsing\nwith modern architectures considerably lags behind bidirectional parsing,\nnoting the challenges of psycholinguistically plausible parsing.\n",
                "publicationDate": "2023-09-28T08:44:08Z",
                "Link": "http://arxiv.org/pdf/2309.16254v1",
                "arxiv_id": "2309.16254v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Using a Diathesis Model for Semantic Parsing",
                "Authors": "Jordi Atserias, Irene Castellon, Montse Civit, German Rigau",
                "Abstract": "  This paper presents a semantic parsing approach for unrestricted texts.\nSemantic parsing is one of the major bottlenecks of Natural Language\nUnderstanding (NLU) systems and usually requires building expensive resources\nnot easily portable to other domains. Our approach obtains a case-role\nanalysis, in which the semantic roles of the verb are identified. In order to\ncover all the possible syntactic realisations of a verb, our system combines\ntheir argument structure with a set of general semantic labelled diatheses\nmodels. Combining them, the system builds a set of syntactic-semantic patterns\nwith their own role-case representation. Once the patterns are build, we use an\napproximate tree pattern-matching algorithm to identify the most reliable\npattern for a sentence. The pattern matching is performed between the\nsyntactic-semantic patterns and the feature-structure tree representing the\nmorphological, syntactical and semantic information of the analysed sentence.\nFor sentences assigned to the correct model, the semantic parsing system we are\npresenting identifies correctly more than 73% of possible semantic case-roles.\n",
                "publicationDate": "2000-06-29T07:44:16Z",
                "Link": "http://arxiv.org/pdf/cs/0006041v1",
                "arxiv_id": "0006041v1"
            },
            {
                "Title": "Transformer-Based Neural Text Generation with Syntactic Guidance",
                "Authors": "Yinghao Li, Rui Feng, Isaac Rehg, Chao Zhang",
                "Abstract": "  We study the problem of using (partial) constituency parse trees as syntactic\nguidance for controlled text generation. Existing approaches to this problem\nuse recurrent structures, which not only suffer from the long-term dependency\nproblem but also falls short in modeling the tree structure of the syntactic\nguidance. We propose to leverage the parallelism of Transformer to better\nincorporate parse trees. Our method first expands a partial template\nconstituency parse tree to a full-fledged parse tree tailored for the input\nsource text, and then uses the expanded tree to guide text generation. The\neffectiveness of our model in this process hinges upon two new attention\nmechanisms: 1) a path attention mechanism that forces one node to attend to\nonly other nodes located in its path in the syntax tree to better incorporate\nsyntax guidance; 2) a multi-encoder attention mechanism that allows the decoder\nto dynamically attend to information from multiple encoders. Our experiments in\nthe controlled paraphrasing task show that our method outperforms SOTA models\nboth semantically and syntactically, improving the best baseline's BLEU score\nfrom 11.83 to 26.27.\n",
                "publicationDate": "2020-10-05T01:33:58Z",
                "Link": "http://arxiv.org/pdf/2010.01737v1",
                "arxiv_id": "2010.01737v1"
            },
            {
                "Title": "Dynamic Syntax Mapping: A New Approach to Unsupervised Syntax Parsing",
                "Authors": "Buvarp Gohsh, Woods Ali, Anders Michael",
                "Abstract": "  The intricate hierarchical structure of syntax is fundamental to the\nintricate and systematic nature of human language. This study investigates the\npremise that language models, specifically their attention distributions, can\nencapsulate syntactic dependencies. We introduce Dynamic Syntax Mapping (DSM),\nan innovative approach for the agnostic induction of these structures. Our\nmethod diverges from traditional syntax models which rely on predefined\nannotation schemata. Instead, we focus on a core characteristic inherent in\ndependency relations: syntactic substitutability. This concept refers to the\ninterchangeability of words within the same syntactic category at either end of\na dependency. By leveraging this property, we generate a collection of\nsyntactically invariant sentences, which serve as the foundation for our\nparsing framework. Our findings reveal that the use of an increasing array of\nsubstitutions notably enhances parsing precision on natural language data.\nSpecifically, in the context of long-distance subject-verb agreement, DSM\nexhibits a remarkable advancement over prior methodologies. Furthermore, DSM's\nadaptability is demonstrated through its successful application in varied\nparsing scenarios, underscoring its broad applicability.\n",
                "publicationDate": "2023-12-18T10:34:29Z",
                "Link": "http://arxiv.org/pdf/2312.14966v1",
                "arxiv_id": "2312.14966v1"
            },
            {
                "Title": "Syntactic Substitutability as Unsupervised Dependency Syntax",
                "Authors": "Jasper Jian, Siva Reddy",
                "Abstract": "  Syntax is a latent hierarchical structure which underpins the robust and\ncompositional nature of human language. In this work, we explore the hypothesis\nthat syntactic dependencies can be represented in language model attention\ndistributions and propose a new method to induce these structures\ntheory-agnostically. Instead of modeling syntactic relations as defined by\nannotation schemata, we model a more general property implicit in the\ndefinition of dependency relations, syntactic substitutability. This property\ncaptures the fact that words at either end of a dependency can be substituted\nwith words from the same category. Substitutions can be used to generate a set\nof syntactically invariant sentences whose representations are then used for\nparsing. We show that increasing the number of substitutions used improves\nparsing accuracy on natural data. On long-distance subject-verb agreement\nconstructions, our method achieves 79.5% recall compared to 8.9% using a\nprevious method. Our method also provides improvements when transferred to a\ndifferent parsing setup, demonstrating that it generalizes.\n",
                "publicationDate": "2022-11-29T09:01:37Z",
                "Link": "http://arxiv.org/pdf/2211.16031v3",
                "arxiv_id": "2211.16031v3"
            },
            {
                "Title": "Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA",
                "Authors": "Daniel Christen",
                "Abstract": "  This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky\nissues of syntactic ambiguity reduction and word sense disambiguation as well\nas providing innovative and original solutions for constituent generation and\nconstraints management. To provide an insight into how it operates, the\nsystem's general architecture and components, as well as its lexical, syntactic\nand semantic resources are described. After that, the paper addresses the\nmechanism that performs selective parsing through an interaction between\nsyntactic and semantic information, leading the parser to a coherent and\naccurate interpretation of the input text.\n",
                "publicationDate": "2016-01-21T20:19:31Z",
                "Link": "http://arxiv.org/pdf/1601.05768v1",
                "arxiv_id": "1601.05768v1"
            },
            {
                "Title": "A General, Sound and Efficient Natural Language Parsing Algorithm based\n  on Syntactic Constraints Propagation",
                "Authors": "Jose F. Quesada",
                "Abstract": "  This paper presents a new context-free parsing algorithm based on a\nbidirectional strictly horizontal strategy which incorporates strong top-down\npredictions (derivations and adjacencies). From a functional point of view, the\nparser is able to propagate syntactic constraints reducing parsing ambiguity.\n  From a computational perspective, the algorithm includes different techniques\naimed at the improvement of the manipulation and representation of the\nstructures used.\n",
                "publicationDate": "1998-01-26T16:36:27Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9801005v1",
                "arxiv_id": "9801005v1"
            },
            {
                "Title": "Graphes param\u00e9tr\u00e9s et outils de lexicalisation",
                "Authors": "Eric Laporte, S\u00e9bastien Paumier",
                "Abstract": "  Shifting to a lexicalized grammar reduces the number of parsing errors and\nimproves application results. However, such an operation affects a syntactic\nparser in all its aspects. One of our research objectives is to design a\nrealistic model for grammar lexicalization. We carried out experiments for\nwhich we used a grammar with a very simple content and formalism, and a very\ninformative syntactic lexicon, the lexicon-grammar of French elaborated by the\nLADL. Lexicalization was performed by applying the parameterized-graph\napproach. Our results tend to show that most information in the lexicon-grammar\ncan be transferred into a grammar and exploited successfully for the syntactic\nparsing of sentences.\n",
                "publicationDate": "2007-11-21T20:44:04Z",
                "Link": "http://arxiv.org/pdf/0711.3454v1",
                "arxiv_id": "0711.3454v1"
            },
            {
                "Title": "Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank",
                "Authors": "Zhang Meishan, Zhang Yue, Fu Guohong",
                "Abstract": "  Treebank translation is a promising method for cross-lingual transfer of\nsyntactic dependency knowledge. The basic idea is to map dependency arcs from a\nsource treebank to its target translation according to word alignments. This\nmethod, however, can suffer from imperfect alignment between source and target\nwords. To address this problem, we investigate syntactic transfer by code\nmixing, translating only confident words in a source treebank. Cross-lingual\nword embeddings are leveraged for transferring syntactic knowledge to the\ntarget from the resulting code-mixed treebank. Experiments on University\nDependency Treebanks show that code-mixed treebanks are more effective than\ntranslated treebanks, giving highly competitive performances among\ncross-lingual parsing methods.\n",
                "publicationDate": "2019-09-05T07:10:44Z",
                "Link": "http://arxiv.org/pdf/1909.02235v1",
                "arxiv_id": "1909.02235v1"
            },
            {
                "Title": "Non-Projective Dependency Parsing via Latent Heads Representation (LHR)",
                "Authors": "Matteo Grella, Simone Cangialosi",
                "Abstract": "  In this paper, we introduce a novel approach based on a bidirectional\nrecurrent autoencoder to perform globally optimized non-projective dependency\nparsing via semi-supervised learning. The syntactic analysis is completed at\nthe end of the neural process that generates a Latent Heads Representation\n(LHR), without any algorithmic constraint and with a linear complexity. The\nresulting \"latent syntactic structure\" can be used directly in other semantic\ntasks. The LHR is transformed into the usual dependency tree computing a simple\nvectors similarity. We believe that our model has the potential to compete with\nmuch more complex state-of-the-art parsing architectures.\n",
                "publicationDate": "2018-02-06T18:28:45Z",
                "Link": "http://arxiv.org/pdf/1802.02116v1",
                "arxiv_id": "1802.02116v1"
            },
            {
                "Title": "Top-down Tree Structured Decoding with Syntactic Connections for Neural\n  Machine Translation and Parsing",
                "Authors": "Jetic G\u016b, Hassan S. Shavarani, Anoop Sarkar",
                "Abstract": "  The addition of syntax-aware decoding in Neural Machine Translation (NMT)\nsystems requires an effective tree-structured neural network, a syntax-aware\nattention model and a language generation model that is sensitive to sentence\nstructure. We exploit a top-down tree-structured model called DRNN\n(Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola\n(2017) to create an NMT model called Seq2DRNN that combines a sequential\nencoder with tree-structured decoding augmented with a syntax-aware attention\nmodel. Unlike previous approaches to syntax-based NMT which use dependency\nparsing models our method uses constituency parsing which we argue provides\nuseful information for translation. In addition, we use the syntactic structure\nof the sentence to add new connections to the tree-structured decoder neural\nnetwork (Seq2DRNN+SynC). We compare our NMT model with sequential and state of\nthe art syntax-based NMT models and show that our model produces more fluent\ntranslations with better reordering. Since our model is capable of doing\ntranslation and constituency parsing at the same time we also compare our\nparsing accuracy against other neural parsing models.\n",
                "publicationDate": "2018-09-06T07:33:48Z",
                "Link": "http://arxiv.org/pdf/1809.01854v1",
                "arxiv_id": "1809.01854v1"
            }
        ]
    },
    {
        "topic_name": "Part-of-Speech (POS) Tagging",
        "summary": "default",
        "papers": [
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Vacaspati: A Diverse Corpus of Bangla Literature",
                "Authors": "Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya",
                "Abstract": "  Bangla (or Bengali) is the fifth most spoken language globally; yet, the\nstate-of-the-art NLP in Bangla is lagging for even simple tasks such as\nlemmatization, POS tagging, etc. This is partly due to lack of a varied quality\ncorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla\nliterature. The literary works are collected from various websites; only those\nworks that are publicly available without copyright violations or restrictions\nare collected. We believe that published literature captures the features of a\nlanguage much better than newspapers, blogs or social media posts which tend to\nfollow only a certain literary pattern and, therefore, miss out on language\nvariety. Our corpus Vacaspati is varied from multiple aspects, including type\nof composition, topic, author, time, space, etc. It contains more than 11\nmillion sentences and 115 million words. We also built a word embedding model,\nVac-FT, using FastText from Vacaspati as well as trained an Electra model,\nVac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only\na fraction of resources compared to other state-of-the-art transformer models\nand yet performs either better or similar on various downstream tasks. On\nmultiple downstream tasks, Vac-FT outperforms other FastText-based models. We\nalso demonstrate the efficacy of Vacaspati as a corpus by showing that similar\nmodels built from other corpora are not as effective. The models are available\nat https://bangla.iitk.ac.in/.\n",
                "publicationDate": "2023-07-11T07:32:12Z",
                "Link": "http://arxiv.org/pdf/2307.05083v1",
                "arxiv_id": "2307.05083v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Part of Speech Tagging (POST) of a Low-resource Language using another\n  Language (Developing a POS-Tagged Lexicon for Kurdish (Sorani) using a Tagged\n  Persian (Farsi) Corpus)",
                "Authors": "Hossein Hassani",
                "Abstract": "  Tagged corpora play a crucial role in a wide range of Natural Language\nProcessing. The Part of Speech Tagging (POST) is essential in developing tagged\ncorpora. It is time-and-effort-consuming and costly, and therefore, it could be\nmore affordable if it is automated. The Kurdish language currently lacks\npublicly available tagged corpora of proper sizes. Tagging the publicly\navailable Kurdish corpora can leverage the capability of those resources to a\nhigher level than what raw or segmented corpora can provide. Developing\nPOS-tagged lexicons can assist the mentioned task. We use a tagged corpus\n(Bijankhan corpus) in Persian (Farsi) as a close language to Kurdish to develop\na POS-tagged lexicon. This paper presents the approach of leveraging the\nresource of a close language to Kurdish to enrich its resources. A partial\ndataset of the results is publicly available for non-commercial use under CC\nBY-NC-SA 4.0 license at https://kurdishblark.github.io/. We plan to make the\nwhole tagged corpus available after further investigation on the outcome. The\ndataset can help in developing POS-tagged lexicons for other Kurdish dialects\nand automated Kurdish corpora tagging.\n",
                "publicationDate": "2022-01-30T11:49:43Z",
                "Link": "http://arxiv.org/pdf/2201.12793v1",
                "arxiv_id": "2201.12793v1"
            },
            {
                "Title": "Unsupervised Part-of-Speech Induction",
                "Authors": "Omid Kashefi",
                "Abstract": "  Part-of-Speech (POS) tagging is an old and fundamental task in natural\nlanguage processing. While supervised POS taggers have shown promising\naccuracy, it is not always feasible to use supervised methods due to lack of\nlabeled data. In this project, we attempt to unsurprisingly induce POS tags by\niteratively looking for a recurring pattern of words through a hierarchical\nagglomerative clustering process. Our approach shows promising results when\ncompared to the tagging results of the state-of-the-art unsupervised POS\ntaggers.\n",
                "publicationDate": "2018-01-10T21:47:26Z",
                "Link": "http://arxiv.org/pdf/1801.03564v1",
                "arxiv_id": "1801.03564v1"
            },
            {
                "Title": "Turkish PoS Tagging by Reducing Sparsity with Morpheme Tags in Small\n  Datasets",
                "Authors": "Burcu Can, Ahmet \u00dcst\u00fcn, Murathan Kurfal\u0131",
                "Abstract": "  Sparsity is one of the major problems in natural language processing. The\nproblem becomes even more severe in agglutinating languages that are highly\nprone to be inflected. We deal with sparsity in Turkish by adopting\nmorphological features for part-of-speech tagging. We learn inflectional and\nderivational morpheme tags in Turkish by using conditional random fields (CRF)\nand we employ the morpheme tags in part-of-speech (PoS) tagging by using hidden\nMarkov models (HMMs) to mitigate sparsity. Results show that using morpheme\ntags in PoS tagging helps alleviate the sparsity in emission probabilities. Our\nmodel outperforms other hidden Markov model based PoS tagging models for small\ntraining datasets in Turkish. We obtain an accuracy of 94.1% in morpheme\ntagging and 89.2% in PoS tagging on a 5K training dataset.\n",
                "publicationDate": "2017-03-09T09:46:56Z",
                "Link": "http://arxiv.org/pdf/1703.03200v2",
                "arxiv_id": "1703.03200v2"
            },
            {
                "Title": "Joint Khmer Word Segmentation and Part-of-Speech Tagging Using Deep\n  Learning",
                "Authors": "Rina Buoy, Nguonly Taing, Sokchea Kor",
                "Abstract": "  Khmer text is written from left to right with optional space. Space is not\nserved as a word boundary but instead, it is used for readability or other\nfunctional purposes. Word segmentation is a prior step for downstream tasks\nsuch as part-of-speech (POS) tagging and thus, the robustness of POS tagging\nhighly depends on word segmentation. The conventional Khmer POS tagging is a\ntwo-stage process that begins with word segmentation and then actual tagging of\neach word, afterward. In this work, a joint word segmentation and POS tagging\napproach using a single deep learning model is proposed so that word\nsegmentation and POS tagging can be performed spontaneously. The proposed model\nwas trained and tested using the publicly available Khmer POS dataset. The\nvalidation suggested that the performance of the joint model is on par with the\nconventional two-stage POS tagging.\n",
                "publicationDate": "2021-03-31T04:26:54Z",
                "Link": "http://arxiv.org/pdf/2103.16801v1",
                "arxiv_id": "2103.16801v1"
            },
            {
                "Title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued\n  Prediction",
                "Authors": "Stefan Heid, Marcel Wever, Eyke H\u00fcllermeier",
                "Abstract": "  Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.\n",
                "publicationDate": "2020-08-04T07:21:36Z",
                "Link": "http://arxiv.org/pdf/2008.01377v3",
                "arxiv_id": "2008.01377v3"
            },
            {
                "Title": "Joint POS Tagging and Dependency Parsing with Transition-based Neural\n  Networks",
                "Authors": "Liner Yang, Meishan Zhang, Yang Liu, Nan Yu, Maosong Sun, Guohong Fu",
                "Abstract": "  While part-of-speech (POS) tagging and dependency parsing are observed to be\nclosely related, existing work on joint modeling with manually crafted feature\ntemplates suffers from the feature sparsity and incompleteness problems. In\nthis paper, we propose an approach to joint POS tagging and dependency parsing\nusing transition-based neural networks. Three neural network based classifiers\nare designed to resolve shift/reduce, tagging, and labeling conflicts.\nExperiments show that our approach significantly outperforms previous methods\nfor joint POS tagging and dependency parsing across a variety of natural\nlanguages.\n",
                "publicationDate": "2017-04-25T10:22:57Z",
                "Link": "http://arxiv.org/pdf/1704.07616v1",
                "arxiv_id": "1704.07616v1"
            },
            {
                "Title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
                "Authors": "Sahil Chopra",
                "Abstract": "  Part of speech tagging in zero-resource settings can be an effective approach\nfor low-resource languages when no labeled training data is available. Existing\nsystems use two main techniques for POS tagging i.e. pretrained multilingual\nlarge language models(LLM) or project the source language labels into the zero\nresource target language and train a sequence labeling model on it. We explore\nthe latter approach using the off-the-shelf alignment module and train a hidden\nMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setup\nwith English as a source language and French, German, and Spanish as target\nlanguages for part-of-speech tagging. Our conclusion is that projected\nalignment data in zero-resource language can be beneficial to predict POS tags.\n",
                "publicationDate": "2024-01-11T08:12:47Z",
                "Link": "http://arxiv.org/pdf/2401.05727v1",
                "arxiv_id": "2401.05727v1"
            },
            {
                "Title": "BNLP: Natural language processing toolkit for Bengali language",
                "Authors": "Sagor Sarker",
                "Abstract": "  BNLP is an open source language processing toolkit for Bengali language\nconsisting with tokenization, word embedding, POS tagging, NER tagging\nfacilities. BNLP provides pre-trained model with high accuracy to do model\nbased tokenization, embedding, POS tagging, NER tagging task for Bengali\nlanguage. BNLP pre-trained model achieves significant results in Bengali text\ntokenization, word embedding, POS tagging and NER tagging task. BNLP is using\nwidely in the Bengali research communities with 16K downloads, 119 stars and 31\nforks. BNLP is available at https://github.com/sagorbrur/bnlp.\n",
                "publicationDate": "2021-01-31T07:56:08Z",
                "Link": "http://arxiv.org/pdf/2102.00405v2",
                "arxiv_id": "2102.00405v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Joint PoS Tagging and Stemming for Agglutinative Languages",
                "Authors": "Necva B\u00f6l\u00fcc\u00fc, Burcu Can",
                "Abstract": "  The number of word forms in agglutinative languages is theoretically infinite\nand this variety in word forms introduces sparsity in many natural language\nprocessing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks\nthat often suffers from sparsity. In this paper, we present an unsupervised\nBayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and\nstemming for agglutinative languages. We use stemming to reduce sparsity in PoS\ntagging. Two tasks are jointly performed to provide a mutual benefit in both\ntasks. Our results show that joint POS tagging and stemming improves PoS\ntagging scores. We present results for Turkish and Finnish as agglutinative\nlanguages and English as a morphologically poor language.\n",
                "publicationDate": "2017-05-24T19:44:35Z",
                "Link": "http://arxiv.org/pdf/1705.08942v1",
                "arxiv_id": "1705.08942v1"
            },
            {
                "Title": "Cross-Register Projection for Headline Part of Speech Tagging",
                "Authors": "Adrian Benton, Hanyang Li, Igor Malioutov",
                "Abstract": "  Part of speech (POS) tagging is a familiar NLP task. State of the art taggers\nroutinely achieve token-level accuracies of over 97% on news body text,\nevidence that the problem is well understood. However, the register of English\nnews headlines, \"headlinese\", is very different from the register of long-form\ntext, causing POS tagging models to underperform on headlines. In this work, we\nautomatically annotate news headlines with POS tags by projecting predicted\ntags from corresponding sentences in news bodies. We train a multi-domain POS\ntagger on both long-form and headline text and show that joint training on both\nregisters improves over training on just one or naively concatenating training\nsets. We evaluate on a newly-annotated corpus of over 5,248 English news\nheadlines from the Google sentence compression corpus, and show that our model\nyields a 23% relative error reduction per token and 19% per headline. In\naddition, we demonstrate that better headline POS tags can improve the\nperformance of a syntax-based open information extraction system. We make POSH,\nthe POS-tagged Headline corpus, available to encourage research in improved NLP\nmodels for news headlines.\n",
                "publicationDate": "2021-09-15T18:00:02Z",
                "Link": "http://arxiv.org/pdf/2109.07483v1",
                "arxiv_id": "2109.07483v1"
            },
            {
                "Title": "Parsing linearizations appreciate PoS tags - but some are fussy about\n  errors",
                "Authors": "Alberto Mu\u00f1oz-Ortiz, Mark Anderson, David Vilares, Carlos G\u00f3mez-Rodr\u00edguez",
                "Abstract": "  PoS tags, once taken for granted as a useful resource for syntactic parsing,\nhave become more situational with the popularization of deep learning. Recent\nwork on the impact of PoS tags on graph- and transition-based parsers suggests\nthat they are only useful when tagging accuracy is prohibitively high, or in\nlow-resource scenarios. However, such an analysis is lacking for the emerging\nsequence labeling parsing paradigm, where it is especially relevant as some\nmodels explicitly use PoS tags for encoding and decoding. We undertake a study\nand uncover some trends. Among them, PoS tags are generally more useful for\nsequence labeling parsers than for other paradigms, but the impact of their\naccuracy is highly encoding-dependent, with the PoS-based head-selection\nencoding being best only when both tagging accuracy and resource availability\nare high.\n",
                "publicationDate": "2022-10-27T07:15:36Z",
                "Link": "http://arxiv.org/pdf/2210.15219v1",
                "arxiv_id": "2210.15219v1"
            },
            {
                "Title": "Semantic Tagging with Deep Residual Networks",
                "Authors": "Johannes Bjerva, Barbara Plank, Johan Bos",
                "Abstract": "  We propose a novel semantic tagging task, sem-tagging, tailored for the\npurpose of multilingual semantic parsing, and present the first tagger using\ndeep residual networks (ResNets). Our tagger uses both word and character\nrepresentations and includes a novel residual bypass architecture. We evaluate\nthe tagset both intrinsically on the new task of semantic tagging, as well as\non Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an\nauxiliary loss function predicting our semantic tags, significantly outperforms\nprior results on English Universal Dependencies POS tagging (95.71% accuracy on\nUD v1.2 and 95.67% accuracy on UD v1.3).\n",
                "publicationDate": "2016-09-22T16:34:00Z",
                "Link": "http://arxiv.org/pdf/1609.07053v2",
                "arxiv_id": "1609.07053v2"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Is POS Tagging Necessary or Even Helpful for Neural Dependency Parsing?",
                "Authors": "Houquan Zhou, Yu Zhang, Zhenghua Li, Min Zhang",
                "Abstract": "  In the pre deep learning era, part-of-speech tags have been considered as\nindispensable ingredients for feature engineering in dependency parsing. But\nquite a few works focus on joint tagging and parsing models to avoid error\npropagation. In contrast, recent studies suggest that POS tagging becomes much\nless important or even useless for neural parsing, especially when using\ncharacter-based word representations. Yet there are not enough investigations\nfocusing on this issue, both empirically and linguistically. To answer this, we\ndesign and compare three typical multi-task learning framework, i.e.,\nShare-Loose, Share-Tight, and Stack, for joint tagging and parsing based on the\nstate-of-the-art biaffine parser. Considering that it is much cheaper to\nannotate POS tags than parse trees, we also investigate the utilization of\nlarge-scale heterogeneous POS tag data. We conduct experiments on both English\nand Chinese datasets, and the results clearly show that POS tagging (both\nhomogeneous and heterogeneous) can still significantly improve parsing\nperformance when using the Stack joint framework. We conduct detailed analysis\nand gain more insights from the linguistic aspect.\n",
                "publicationDate": "2020-03-06T13:47:30Z",
                "Link": "http://arxiv.org/pdf/2003.03204v2",
                "arxiv_id": "2003.03204v2"
            },
            {
                "Title": "Bootstrapping Method for Developing Part-of-Speech Tagged Corpus in Low\n  Resource Languages Tagset - A Focus on an African Igbo",
                "Authors": "Onyenwe Ikechukwu E, Onyedinma Ebele G, Aniegwu Godwin E, Ezeani Ignatius M",
                "Abstract": "  Most languages, especially in Africa, have fewer or no established\npart-of-speech (POS) tagged corpus. However, POS tagged corpus is essential for\nnatural language processing (NLP) to support advanced researches such as\nmachine translation, speech recognition, etc. Even in cases where there is no\nPOS tagged corpus, there are some languages for which parallel texts are\navailable online. The task of POS tagging a new language corpus with a new\ntagset usually face a bootstrapping problem at the initial stages of the\nannotation process. The unavailability of automatic taggers to help the human\nannotator makes the annotation process to appear infeasible to quickly produce\nadequate amounts of POS tagged corpus for advanced NLP research and training\nthe taggers. In this paper, we demonstrate the efficacy of a POS annotation\nmethod that employed the services of two automatic approaches to assist POS\ntagged corpus creation for a novel language in NLP. The two approaches are\ncross-lingual and monolingual POS tags projection. We used cross-lingual to\nautomatically create an initial 'errorful' tagged corpus for a target language\nvia word-alignment. The resources for creating this are derived from a source\nlanguage rich in NLP resources. A monolingual method is applied to clean the\ninduce noise via an alignment process and to transform the source language tags\nto the target language tags. We used English and Igbo as our case study. This\nis possible because there are parallel texts that exist between English and\nIgbo, and the source language English has available NLP resources. The results\nof the experiment show a steady improvement in accuracy and rate of tags\ntransformation with score ranges of 6.13% to 83.79% and 8.67% to 98.37%\nrespectively. The rate of tags transformation evaluates the rate at which\nsource language tags are translated to target language tags.\n",
                "publicationDate": "2019-03-12T21:24:25Z",
                "Link": "http://arxiv.org/pdf/1903.05225v1",
                "arxiv_id": "1903.05225v1"
            },
            {
                "Title": "Improved POS tagging for spontaneous, clinical speech using data\n  augmentation",
                "Authors": "Seth Kulick, Neville Ryant, David J. Irwin, Naomi Nevler, Sunghye Cho",
                "Abstract": "  This paper addresses the problem of improving POS tagging of transcripts of\nspeech from clinical populations. In contrast to prior work on parsing and POS\ntagging of transcribed speech, we do not make use of an in domain treebank for\ntraining. Instead, we train on an out of domain treebank of newswire using data\naugmentation techniques to make these structures resemble natural, spontaneous\nspeech. We trained a parser with and without the augmented data and tested its\nperformance using manually validated POS tags in clinical speech produced by\npatients with various types of neurodegenerative conditions.\n",
                "publicationDate": "2023-07-11T20:42:06Z",
                "Link": "http://arxiv.org/pdf/2307.05796v1",
                "arxiv_id": "2307.05796v1"
            },
            {
                "Title": "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource\n  Languages",
                "Authors": "Ronald Cardenas, Ying Lin, Heng Ji, Jonathan May",
                "Abstract": "  Unsupervised part of speech (POS) tagging is often framed as a clustering\nproblem, but practical taggers need to \\textit{ground} their clusters as well.\nGrounding generally requires reference labeled data, a luxury a low-resource\nlanguage might not have. In this work, we describe an approach for low-resource\nunsupervised POS tagging that yields fully grounded output and requires no\nlabeled training data. We find the classic method of Brown et al. (1992)\nclusters well in our use case and employ a decipherment-based approach to\ngrounding. This approach presumes a sequence of cluster IDs is a `ciphertext'\nand seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We\nshow intrinsically that, despite the difficulty of the task, we obtain\nreasonable performance across a variety of languages. We also show\nextrinsically that incorporating our POS tagger into a name tagger leads to\nstate-of-the-art tagging performance in Sinhalese and Kinyarwanda, two\nlanguages with nearly no labeled POS data available. We further demonstrate our\ntagger's utility by incorporating it into a true `zero-resource' variant of the\nMalopa (Ammar et al., 2016) dependency parser model that removes the current\nreliance on multilingual resources and gold POS tags for new languages.\nExperiments show that including our tagger makes up much of the accuracy lost\nwhen gold POS tags are unavailable.\n",
                "publicationDate": "2019-04-10T20:22:31Z",
                "Link": "http://arxiv.org/pdf/1904.05426v1",
                "arxiv_id": "1904.05426v1"
            },
            {
                "Title": "Experiments with POS Tagging Code-mixed Indian Social Media Text",
                "Authors": "Prakash B. Pimpale, Raj Nath Patel",
                "Abstract": "  This paper presents Centre for Development of Advanced Computing Mumbai's\n(CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For\nCode-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON\n2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te)\nlanguages mixed with English (en). In this paper, we have described our\napproaches to the POS tagging techniques, we exploited for this task. Machine\nlearning has been used to POS tag the mixed language text. For POS tagging,\ndistributed representations of words in vector space (word2vec) for feature\nextraction and Log-linear models have been tried. We report our work on all\nthree languages hi, bn, and te mixed with en.\n",
                "publicationDate": "2016-10-31T06:13:31Z",
                "Link": "http://arxiv.org/pdf/1610.09799v1",
                "arxiv_id": "1610.09799v1"
            },
            {
                "Title": "Feature-Rich Part-of-speech Tagging for Morphologically Complex\n  Languages: Application to Bulgarian",
                "Authors": "Georgi Georgiev, Valentin Zhikov, Petya Osenova, Kiril Simov, Preslav Nakov",
                "Abstract": "  We present experiments with part-of-speech tagging for Bulgarian, a Slavic\nlanguage with rich inflectional and derivational morphology. Unlike most\nprevious work, which has used a small number of grammatical categories, we work\nwith 680 morpho-syntactic tags. We combine a large morphological lexicon with\nprior linguistic knowledge and guided learning from a POS-annotated corpus,\nachieving accuracy of 97.98%, which is a significant improvement over the\nstate-of-the-art for Bulgarian.\n",
                "publicationDate": "2019-11-26T13:05:33Z",
                "Link": "http://arxiv.org/pdf/1911.11503v1",
                "arxiv_id": "1911.11503v1"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "publicationDate": "2022-04-21T08:40:17Z",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1"
            },
            {
                "Title": "An improved neural network model for joint POS tagging and dependency\n  parsing",
                "Authors": "Dat Quoc Nguyen, Karin Verspoor",
                "Abstract": "  We propose a novel neural network model for joint part-of-speech (POS)\ntagging and dependency parsing. Our model extends the well-known BIST\ngraph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating\na BiLSTM-based tagging component to produce automatically predicted POS tags\nfor the parser. On the benchmark English Penn treebank, our model obtains\nstrong UAS and LAS scores at 94.51% and 92.87%, respectively, producing 1.5+%\nabsolute improvements to the BIST graph-based parser, and also obtaining a\nstate-of-the-art POS tagging accuracy at 97.97%. Furthermore, experimental\nresults on parsing 61 \"big\" Universal Dependencies treebanks from raw texts\nshow that our model outperforms the baseline UDPipe (Straka and Strakov\\'a,\n2017) with 0.8% higher average POS tagging score and 3.6% higher average LAS\nscore. In addition, with our model, we also obtain state-of-the-art downstream\ntask scores for biomedical event extraction and opinion analysis applications.\nOur code is available together with all pre-trained models at:\nhttps://github.com/datquocnguyen/jPTDP\n",
                "publicationDate": "2018-07-11T05:47:33Z",
                "Link": "http://arxiv.org/pdf/1807.03955v2",
                "arxiv_id": "1807.03955v2"
            },
            {
                "Title": "Development of POS tagger for English-Bengali Code-Mixed data",
                "Authors": "Tathagata Raha, Sainik Kumar Mahata, Dipankar Das, Sivaji Bandyopadhyay",
                "Abstract": "  Code-mixed texts are widespread nowadays due to the advent of social media.\nSince these texts combine two languages to formulate a sentence, it gives rise\nto various research problems related to Natural Language Processing. In this\npaper, we try to excavate one such problem, namely, Parts of Speech tagging of\ncode-mixed texts. We have built a system that can POS tag English-Bengali\ncode-mixed data where the Bengali words were written in Roman script. Our\napproach initially involves the collection and cleaning of English-Bengali\ncode-mixed tweets. These tweets were used as a development dataset for building\nour system. The proposed system is a modular approach that starts by tagging\nindividual tokens with their respective languages and then passes them to\ndifferent POS taggers, designed for different languages (English and Bengali,\nin our case). Tags given by the two systems are later joined together and the\nfinal result is then mapped to a universal POS tag set. Our system was checked\nusing 100 manually POS tagged code-mixed sentences and it returned an accuracy\nof 75.29%\n",
                "publicationDate": "2020-07-29T03:42:07Z",
                "Link": "http://arxiv.org/pdf/2007.14576v1",
                "arxiv_id": "2007.14576v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Development of Marathi Part of Speech Tagger Using Statistical Approach",
                "Authors": "Jyoti Singh, Nisheeth Joshi, Iti Mathur",
                "Abstract": "  Part-of-speech (POS) tagging is a process of assigning the words in a text\ncorresponding to a particular part of speech. A fundamental version of POS\ntagging is the identification of words as nouns, verbs, adjectives etc. For\nprocessing natural languages, Part of Speech tagging is a prominent tool. It is\none of the simplest as well as most constant and statistical model for many NLP\napplications. POS Tagging is an initial stage of linguistics, text analysis\nlike information retrieval, machine translator, text to speech synthesis,\ninformation extraction etc. In POS Tagging we assign a Part of Speech tag to\neach word in a sentence and literature. Various approaches have been proposed\nto implement POS taggers. In this paper we present a Marathi part of speech\ntagger. It is morphologically rich language. Marathi is spoken by the native\npeople of Maharashtra. The general approach used for development of tagger is\nstatistical using Unigram, Bigram, Trigram and HMM Methods. It presents a clear\nidea about all the algorithms with suitable examples. It also introduces a tag\nset for Marathi which can be used for tagging Marathi text. In this paper we\nhave shown the development of the tagger as well as compared to check the\naccuracy of taggers output. The three Marathi POS taggers viz. Unigram, Bigram,\nTrigram and HMM gives the accuracy of 77.38%, 90.30%, 91.46% and 93.82%\nrespectively.\n",
                "publicationDate": "2013-10-02T06:04:53Z",
                "Link": "http://arxiv.org/pdf/1310.0575v2",
                "arxiv_id": "1310.0575v2"
            },
            {
                "Title": "Colloquial Persian POS (CPPOS) Corpus: A Novel Corpus for Colloquial\n  Persian Part of Speech Tagging",
                "Authors": "Leyla Rabiei, Farzaneh Rahmani, Mohammad Khansari, Zeinab Rajabi, Moein Salimi",
                "Abstract": "  Introduction: Part-of-Speech (POS) Tagging, the process of classifying words\ninto their respective parts of speech (e.g., verb or noun), is essential in\nvarious natural language processing applications. POS tagging is a crucial\npreprocessing task for applications like machine translation, question\nanswering, sentiment analysis, etc. However, existing corpora for POS tagging\nin Persian mainly consist of formal texts, such as daily news and newspapers.\nAs a result, smart POS tools, machine learning models, and deep learning models\ntrained on these corpora may not perform optimally for processing colloquial\ntext in social network analysis. Method: This paper introduces a novel corpus,\n\"Colloquial Persian POS\" (CPPOS), specifically designed to support colloquial\nPersian text. The corpus includes formal and informal text collected from\nvarious domains such as political, social, and commercial on Telegram, Twitter,\nand Instagram more than 520K labeled tokens. After collecting posts from these\nsocial platforms for one year, special preprocessing steps were conducted,\nincluding normalization, sentence tokenizing, and word tokenizing for social\ntext. The tokens and sentences were then manually annotated and verified by a\nteam of linguistic experts. This study also defines a POS tagging guideline for\nannotating the data and conducting the annotation process. Results: To evaluate\nthe quality of CPPOS, various deep learning models, such as the RNN family,\nwere trained using the constructed corpus. A comparison with another well-known\nPersian POS corpus named \"Bijankhan\" and the Persian Hazm POS tool trained on\nBijankhan revealed that our model trained on CPPOS outperforms them. With the\nnew corpus and the BiLSTM deep neural model, we achieved a 14% improvement over\nthe previous dataset.\n",
                "publicationDate": "2023-10-01T05:06:33Z",
                "Link": "http://arxiv.org/pdf/2310.00572v1",
                "arxiv_id": "2310.00572v1"
            },
            {
                "Title": "g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation\n  in Mandarin",
                "Authors": "Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, Yi-Ren Yeh",
                "Abstract": "  Polyphone disambiguation is the most crucial task in Mandarin\ngrapheme-to-phoneme (g2p) conversion. Previous studies have approached this\nproblem using pre-trained language models, restricted output, and extra\ninformation from Part-Of-Speech (POS) tagging. Inspired by these strategies, we\npropose a novel approach, called g2pW, which adapts learnable softmax-weights\nto condition the outputs of BERT with the polyphonic character of interest and\nits POS tagging. Rather than using the hard mask as in previous works, our\nexperiments show that learning a soft-weighting function for the candidate\nphonemes benefits performance. In addition, our proposed g2pW does not require\nextra pre-trained POS tagging models while using POS tags as auxiliary features\nsince we train the POS tagging model simultaneously with the unified encoder.\nExperimental results show that our g2pW outperforms existing methods on the\npublic CPP dataset. All codes, model weights, and a user-friendly package are\npublicly available.\n",
                "publicationDate": "2022-03-20T02:28:25Z",
                "Link": "http://arxiv.org/pdf/2203.10430v5",
                "arxiv_id": "2203.10430v5"
            },
            {
                "Title": "A Novel Neural Network Model for Joint POS Tagging and Graph-based\n  Dependency Parsing",
                "Authors": "Dat Quoc Nguyen, Mark Dras, Mark Johnson",
                "Abstract": "  We present a novel neural network model that learns POS tagging and\ngraph-based dependency parsing jointly. Our model uses bidirectional LSTMs to\nlearn feature representations shared for both POS tagging and dependency\nparsing tasks, thus handling the feature-engineering problem. Our extensive\nexperiments, on 19 languages from the Universal Dependencies project, show that\nour model outperforms the state-of-the-art neural network-based\nStack-propagation model for joint POS tagging and transition-based dependency\nparsing, resulting in a new state of the art. Our code is open-source and\navailable together with pre-trained models at:\nhttps://github.com/datquocnguyen/jPTDP\n",
                "publicationDate": "2017-05-16T23:09:00Z",
                "Link": "http://arxiv.org/pdf/1705.05952v2",
                "arxiv_id": "1705.05952v2"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed\n  Social Media Text",
                "Authors": "Raj Nath Patel, Prakash B. Pimpale, M Sasikumar",
                "Abstract": "  This paper describes Centre for Development of Advanced Computing's (CDACM)\nsubmission to the shared task-'Tool Contest on POS tagging for Code-Mixed\nIndian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with\nICON-2016. The shared task was to predict Part of Speech (POS) tag at word\nlevel for a given text. The code-mixed text is generated mostly on social media\nby multilingual users. The presence of the multilingual words,\ntransliterations, and spelling variations make such content linguistically\ncomplex. In this paper, we propose an approach to POS tag code-mixed social\nmedia text using Recurrent Neural Network Language Model (RNN-LM) architecture.\nWe submitted the results for Hindi-English (hi-en), Bengali-English (bn-en),\nand Telugu-English (te-en) code-mixed data.\n",
                "publicationDate": "2016-11-15T19:02:35Z",
                "Link": "http://arxiv.org/pdf/1611.04989v2",
                "arxiv_id": "1611.04989v2"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Czech Text Processing with Contextual Embeddings: POS Tagging,\n  Lemmatization, Parsing and NER",
                "Authors": "Milan Straka, Jana Strakov\u00e1, Jan Haji\u010d",
                "Abstract": "  Contextualized embeddings, which capture appropriate word meaning depending\non context, have recently been proposed. We evaluate two meth ods for\nprecomputing such embeddings, BERT and Flair, on four Czech text processing\ntasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and\nnamed entity recognition (NER). The first three tasks, POS tagging,\nlemmatization and dependency parsing, are evaluated on two corpora: the Prague\nDependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity\nrecognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We\nreport state-of-the-art results for the above mentioned tasks and corpora.\n",
                "publicationDate": "2019-09-08T21:00:05Z",
                "Link": "http://arxiv.org/pdf/1909.03544v2",
                "arxiv_id": "1909.03544v2"
            },
            {
                "Title": "A neural joint model for Vietnamese word segmentation, POS tagging and\n  dependency parsing",
                "Authors": "Dat Quoc Nguyen",
                "Abstract": "  We propose the first multi-task learning model for joint Vietnamese word\nsegmentation, part-of-speech (POS) tagging and dependency parsing. In\nparticular, our model extends the BIST graph-based dependency parser\n(Kiperwasser and Goldberg, 2016) with BiLSTM-CRF-based neural layers (Huang et\nal., 2015) for word segmentation and POS tagging. On Vietnamese benchmark\ndatasets, experimental results show that our joint model obtains\nstate-of-the-art or competitive performances.\n",
                "publicationDate": "2018-12-30T03:03:28Z",
                "Link": "http://arxiv.org/pdf/1812.11459v3",
                "arxiv_id": "1812.11459v3"
            },
            {
                "Title": "A Review on Part-of-Speech Technologies",
                "Authors": "Onyenwe Ikechukwu, Onyedikachukwu Ikechukwu-Onyenwe, Onyedinma Ebele",
                "Abstract": "  Developing an automatic part-of-speech (POS) tagging for any new language is\nconsidered a necessary step for further computational linguistics methodology\nbeyond tagging, like chunking and parsing, to be fully applied to the language.\nMany POS disambiguation technologies have been developed for this type of\nresearch and there are factors that influence the choice of choosing one. This\ncould be either corpus-based or non-corpus-based. In this paper, we present a\nreview of POS tagging technologies.\n",
                "publicationDate": "2021-10-11T03:21:33Z",
                "Link": "http://arxiv.org/pdf/2110.04977v1",
                "arxiv_id": "2110.04977v1"
            },
            {
                "Title": "Modeling Composite Labels for Neural Morphological Tagging",
                "Authors": "Alexander Tkachenko, Kairit Sirts",
                "Abstract": "  Neural morphological tagging has been regarded as an extension to POS tagging\ntask, treating each morphological tag as a monolithic label and ignoring its\ninternal structure. We propose to view morphological tags as composite labels\nand explicitly model their internal structure in a neural sequence tagger. For\nthis, we explore three different neural architectures and compare their\nperformance with both CRF and simple neural multiclass baselines. We evaluate\nour models on 49 languages and show that the neural architecture that models\nthe morphological labels as sequences of morphological category values performs\nsignificantly better than both baselines establishing state-of-the-art results\nin morphological tagging for most languages.\n",
                "publicationDate": "2018-10-20T15:00:23Z",
                "Link": "http://arxiv.org/pdf/1810.08815v1",
                "arxiv_id": "1810.08815v1"
            },
            {
                "Title": "Creating a morphological and syntactic tagged corpus for the Uzbek\n  language",
                "Authors": "Maksud Sharipov, Jamolbek Mattiev, Jasur Sobirov, Rustam Baltayev",
                "Abstract": "  Nowadays, creation of the tagged corpora is becoming one of the most\nimportant tasks of Natural Language Processing (NLP). There are not enough\ntagged corpora to build machine learning models for the low-resource Uzbek\nlanguage. In this paper, we tried to fill that gap by developing a novel Part\nOf Speech (POS) and syntactic tagset for creating the syntactic and\nmorphologically tagged corpus of the Uzbek language. This work also includes\ndetailed description and presentation of a web-based application to work on a\ntagging as well. Based on the developed annotation tool and the software, we\nshare our experience results of the first stage of the tagged corpus creation\n",
                "publicationDate": "2022-10-27T07:44:12Z",
                "Link": "http://arxiv.org/pdf/2210.15234v1",
                "arxiv_id": "2210.15234v1"
            },
            {
                "Title": "Learning when to trust distant supervision: An application to\n  low-resource POS tagging using cross-lingual projection",
                "Authors": "Meng Fang, Trevor Cohn",
                "Abstract": "  Cross lingual projection of linguistic annotation suffers from many sources\nof bias and noise, leading to unreliable annotations that cannot be used\ndirectly. In this paper, we introduce a novel approach to sequence tagging that\nlearns to correct the errors from cross-lingual projection using an explicit\ndebiasing layer. This is framed as joint learning over two corpora, one tagged\nwith gold standard and the other with projected tags. We evaluated with only\n1,000 tokens tagged with gold standard tags, along with more plentiful parallel\ndata. Our system equals or exceeds the state-of-the-art on eight simulated\nlow-resource settings, as well as two real low-resource languages, Malagasy and\nKinyarwanda.\n",
                "publicationDate": "2016-07-05T07:31:22Z",
                "Link": "http://arxiv.org/pdf/1607.01133v1",
                "arxiv_id": "1607.01133v1"
            },
            {
                "Title": "Adversarial Transfer Learning for Punctuation Restoration",
                "Authors": "Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, Cunhang Fan",
                "Abstract": "  Previous studies demonstrate that word embeddings and part-of-speech (POS)\ntags are helpful for punctuation restoration tasks. However, two drawbacks\nstill exist. One is that word embeddings are pre-trained by unidirectional\nlanguage modeling objectives. Thus the word embeddings only contain\nleft-to-right context information. The other is that POS tags are provided by\nan external POS tagger. So computation cost will be increased and incorrect\npredicted tags may affect the performance of restoring punctuation marks during\ndecoding. This paper proposes adversarial transfer learning to address these\nproblems. A pre-trained bidirectional encoder representations from transformers\n(BERT) model is used to initialize a punctuation model. Thus the transferred\nmodel parameters carry both left-to-right and right-to-left representations.\nFurthermore, adversarial multi-task learning is introduced to learn task\ninvariant knowledge for punctuation prediction. We use an extra POS tagging\ntask to help the training of the punctuation predicting task. Adversarial\ntraining is utilized to prevent the shared parameters from containing task\nspecific information. We only use the punctuation predicting task to restore\nmarks during decoding stage. Therefore, it will not need extra computation and\nnot introduce incorrect tags from the POS tagger. Experiments are conducted on\nIWSLT2011 datasets. The results demonstrate that the punctuation predicting\nmodels obtain further performance improvement with task invariant knowledge\nfrom the POS tagging task. Our best model outperforms the previous\nstate-of-the-art model trained only with lexical features by up to 9.2%\nabsolute overall F_1-score on test set.\n",
                "publicationDate": "2020-04-01T06:19:56Z",
                "Link": "http://arxiv.org/pdf/2004.00248v1",
                "arxiv_id": "2004.00248v1"
            },
            {
                "Title": "Building a Kannada POS Tagger Using Machine Learning and Neural Network\n  Models",
                "Authors": "Ketan Kumar Todi, Pruthwik Mishra, Dipti Misra Sharma",
                "Abstract": "  POS Tagging serves as a preliminary task for many NLP applications. Kannada\nis a relatively poor Indian language with very limited number of quality NLP\ntools available for use. An accurate and reliable POS Tagger is essential for\nmany NLP tasks like shallow parsing, dependency parsing, sentiment analysis,\nnamed entity recognition. We present a statistical POS tagger for Kannada using\ndifferent machine learning and neural network models. Our Kannada POS tagger\noutperforms the state-of-the-art Kannada POS tagger by 6%. Our contribution in\nthis paper is three folds - building a generic POS Tagger, comparing the\nperformances of different modeling techniques, exploring the use of character\nand word embeddings together for Kannada POS Tagging.\n",
                "publicationDate": "2018-08-09T14:16:30Z",
                "Link": "http://arxiv.org/pdf/1808.03175v1",
                "arxiv_id": "1808.03175v1"
            },
            {
                "Title": "Is Word Sense Disambiguation just one more NLP task?",
                "Authors": "Yorick Wilks",
                "Abstract": "  This paper compares the tasks of part-of-speech (POS) tagging and\nword-sense-tagging or disambiguation (WSD), and argues that the tasks are not\nrelated by fineness of grain or anything like that, but are quite different\nkinds of task, particularly becuase there is nothing in POS corresponding to\nsense novelty. The paper also argues for the reintegration of sub-tasks that\nare being separated for evaluation\n",
                "publicationDate": "1999-02-25T14:41:32Z",
                "Link": "http://arxiv.org/pdf/cs/9902030v1",
                "arxiv_id": "9902030v1"
            },
            {
                "Title": "Machine Learning Approaches for Amharic Parts-of-speech Tagging",
                "Authors": "Ibrahim Gashaw, H L. Shashirekha",
                "Abstract": "  Part-of-speech (POS) tagging is considered as one of the basic but necessary\ntools which are required for many Natural Language Processing (NLP)\napplications such as word sense disambiguation, information retrieval,\ninformation processing, parsing, question answering, and machine translation.\nPerformance of the current POS taggers in Amharic is not as good as that of the\ncontemporary POS taggers available for English and other European languages.\nThe aim of this work is to improve POS tagging performance for the Amharic\nlanguage, which was never above 91%. Usage of morphological knowledge, an\nextension of the existing annotated data, feature extraction, parameter tuning\nby applying grid search and the tagging algorithms have been examined and\nobtained significant performance difference from the previous works. We have\nused three different datasets for POS experiments.\n",
                "publicationDate": "2020-01-10T06:40:49Z",
                "Link": "http://arxiv.org/pdf/2001.03324v1",
                "arxiv_id": "2001.03324v1"
            },
            {
                "Title": "Online Updating of Word Representations for Part-of-Speech Tagging",
                "Authors": "Wenpeng Yin, Tobias Schnabel, Hinrich Sch\u00fctze",
                "Abstract": "  We propose online unsupervised domain adaptation (DA), which is performed\nincrementally as data comes in and is applicable when batch DA is not possible.\nIn a part-of-speech (POS) tagging evaluation, we find that online unsupervised\nDA performs as well as batch DA.\n",
                "publicationDate": "2016-04-02T13:52:23Z",
                "Link": "http://arxiv.org/pdf/1604.00502v1",
                "arxiv_id": "1604.00502v1"
            },
            {
                "Title": "From Word Segmentation to POS Tagging for Vietnamese",
                "Authors": "Dat Quoc Nguyen, Thanh Vu, Dai Quoc Nguyen, Mark Dras, Mark Johnson",
                "Abstract": "  This paper presents an empirical comparison of two strategies for Vietnamese\nPart-of-Speech (POS) tagging from unsegmented text: (i) a pipeline strategy\nwhere we consider the output of a word segmenter as the input of a POS tagger,\nand (ii) a joint strategy where we predict a combined segmentation and POS tag\nfor each syllable. We also make a comparison between state-of-the-art (SOTA)\nfeature-based and neural network-based models. On the benchmark Vietnamese\ntreebank (Nguyen et al., 2009), experimental results show that the pipeline\nstrategy produces better scores of POS tagging from unsegmented text than the\njoint strategy, and the highest accuracy is obtained by using a feature-based\nmodel.\n",
                "publicationDate": "2017-11-14T05:19:45Z",
                "Link": "http://arxiv.org/pdf/1711.04951v1",
                "arxiv_id": "1711.04951v1"
            }
        ]
    },
    {
        "topic_name": "Dialog Systems",
        "summary": "default",
        "papers": [
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Conversation Learner -- A Machine Teaching Tool for Building Dialog\n  Managers for Task-Oriented Dialog Systems",
                "Authors": "Swadheen Shukla, Lars Liden, Shahin Shayandeh, Eslam Kamal, Jinchao Li, Matt Mazzola, Thomas Park, Baolin Peng, Jianfeng Gao",
                "Abstract": "  Traditionally, industry solutions for building a task-oriented dialog system\nhave relied on helping dialog authors define rule-based dialog managers,\nrepresented as dialog flows. While dialog flows are intuitively interpretable\nand good for simple scenarios, they fall short of performance in terms of the\nflexibility needed to handle complex dialogs. On the other hand, purely\nmachine-learned models can handle complex dialogs, but they are considered to\nbe black boxes and require large amounts of training data. In this\ndemonstration, we showcase Conversation Learner, a machine teaching tool for\nbuilding dialog managers. It combines the best of both approaches by enabling\ndialog authors to create a dialog flow using familiar tools, converting the\ndialog flow into a parametric model (e.g., neural networks), and allowing\ndialog authors to improve the dialog manager (i.e., the parametric model) over\ntime by leveraging user-system dialog logs as training data through a machine\nteaching interface.\n",
                "publicationDate": "2020-04-09T00:10:54Z",
                "Link": "http://arxiv.org/pdf/2004.04305v2",
                "arxiv_id": "2004.04305v2"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Task-Oriented Dialog Systems that Consider Multiple Appropriate\n  Responses under the Same Context",
                "Authors": "Yichi Zhang, Zhijian Ou, Zhou Yu",
                "Abstract": "  Conversations have an intrinsic one-to-many property, which means that\nmultiple responses can be appropriate for the same dialog context. In\ntask-oriented dialogs, this property leads to different valid dialog policies\ntowards task completion. However, none of the existing task-oriented dialog\ngeneration approaches takes this property into account. We propose a\nMulti-Action Data Augmentation (MADA) framework to utilize the one-to-many\nproperty to generate diverse appropriate dialog responses. Specifically, we\nfirst use dialog states to summarize the dialog history, and then discover all\npossible mappings from every dialog state to its different valid system\nactions. During dialog system training, we enable the current dialog state to\nmap to all valid system actions discovered in the previous process to create\nadditional state-action pairs. By incorporating these additional pairs, the\ndialog policy learns a balanced action distribution, which further guides the\ndialog model to generate diverse responses. Experimental results show that the\nproposed framework consistently improves dialog policy diversity, and results\nin improved response diversity and appropriateness. Our model obtains\nstate-of-the-art results on MultiWOZ.\n",
                "publicationDate": "2019-11-24T09:32:55Z",
                "Link": "http://arxiv.org/pdf/1911.10484v2",
                "arxiv_id": "1911.10484v2"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Spectral decomposition method of dialog state tracking via collective\n  matrix factorization",
                "Authors": "Julien Perez",
                "Abstract": "  The task of dialog management is commonly decomposed into two sequential\nsubtasks: dialog state tracking and dialog policy learning. In an end-to-end\ndialog system, the aim of dialog state tracking is to accurately estimate the\ntrue dialog state from noisy observations produced by the speech recognition\nand the natural language understanding modules. The state tracking task is\nprimarily meant to support a dialog policy. From a probabilistic perspective,\nthis is achieved by maintaining a posterior distribution over hidden dialog\nstates composed of a set of context dependent variables. Once a dialog policy\nis learned, it strives to select an optimal dialog act given the estimated\ndialog state and a defined reward function. This paper introduces a novel\nmethod of dialog state tracking based on a bilinear algebric decomposition\nmodel that provides an efficient inference schema through collective matrix\nfactorization. We evaluate the proposed approach on the second Dialog State\nTracking Challenge (DSTC-2) dataset and we show that the proposed tracker gives\nencouraging results compared to the state-of-the-art trackers that participated\nin this standard benchmark. Finally, we show that the prediction schema is\ncomputationally efficient in comparison to the previous approaches.\n",
                "publicationDate": "2016-06-16T17:31:13Z",
                "Link": "http://arxiv.org/pdf/1606.05286v1",
                "arxiv_id": "1606.05286v1"
            },
            {
                "Title": "Learning End-to-End Goal-Oriented Dialog with Multiple Answers",
                "Authors": "Janarthanan Rajendran, Jatin Ganhotra, Satinder Singh, Lazaros Polymenakos",
                "Abstract": "  In a dialog, there can be multiple valid next utterances at any point. The\npresent end-to-end neural methods for dialog do not take this into account.\nThey learn with the assumption that at any time there is only one correct next\nutterance. In this work, we focus on this problem in the goal-oriented dialog\nsetting where there are different paths to reach a goal. We propose a new\nmethod, that uses a combination of supervised learning and reinforcement\nlearning approaches to address this issue. We also propose a new and more\neffective testbed, permuted-bAbI dialog tasks, by introducing multiple valid\nnext utterances to the original-bAbI dialog tasks, which allows evaluation of\ngoal-oriented dialog systems in a more realistic setting. We show that there is\na significant drop in performance of existing end-to-end neural methods from\n81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on\npermuted-bAbI dialog tasks. We also show that our proposed method improves the\nperformance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog\ntasks.\n",
                "publicationDate": "2018-08-24T19:24:58Z",
                "Link": "http://arxiv.org/pdf/1808.09996v1",
                "arxiv_id": "1808.09996v1"
            },
            {
                "Title": "Context-Aware Dialog Re-Ranking for Task-Oriented Dialog Systems",
                "Authors": "Junki Ohmura, Maxine Eskenazi",
                "Abstract": "  Dialog response ranking is used to rank response candidates by considering\ntheir relation to the dialog history. Although researchers have addressed this\nconcept for open-domain dialogs, little attention has been focused on\ntask-oriented dialogs. Furthermore, no previous studies have analyzed whether\nresponse ranking can improve the performance of existing dialog systems in real\nhuman-computer dialogs with speech recognition errors. In this paper, we\npropose a context-aware dialog response re-ranking system. Our system reranks\nresponses in two steps: (1) it calculates matching scores for each candidate\nresponse and the current dialog context; (2) it combines the matching scores\nand a probability distribution of the candidates from an existing dialog system\nfor response re-ranking. By using neural word embedding-based models and\nhandcrafted or logistic regression-based ensemble models, we have improved the\nperformance of a recently proposed end-to-end task-oriented dialog system on\nreal dialogs with speech recognition errors.\n",
                "publicationDate": "2018-11-28T07:58:16Z",
                "Link": "http://arxiv.org/pdf/1811.11430v1",
                "arxiv_id": "1811.11430v1"
            },
            {
                "Title": "Contextual Data Augmentation for Task-Oriented Dialog Systems",
                "Authors": "Dustin Axman, Avik Ray, Shubham Garg, Jing Huang",
                "Abstract": "  Collection of annotated dialogs for training task-oriented dialog systems\nhave been one of the key bottlenecks in improving current models. While dialog\nresponse generation has been widely studied on the agent side, it is not\nevident if similar generative models can be used to generate a large variety\nof, and often unexpected, user inputs that real dialog systems encounter in\npractice. Existing data augmentation techniques such as paraphrase generation\ndo not take the dialog context into consideration. In this paper, we develop a\nnovel dialog augmentation model that generates a user turn, conditioning on\nfull dialog context. Additionally, with a new prompt design for language model,\nand output re-ranking, the dialogs generated from our model can be directly\nused to train downstream dialog systems. On common benchmark datasets MultiWoZ\nand SGD, we show that our dialog augmentation model generates high quality\ndialogs and improves dialog success rate by as much as $8\\%$ over baseline.\n",
                "publicationDate": "2023-10-16T13:22:34Z",
                "Link": "http://arxiv.org/pdf/2310.10380v1",
                "arxiv_id": "2310.10380v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "MSDF: A General Open-Domain Multi-Skill Dialog Framework",
                "Authors": "Yu Zhao, Xinshuo Hu, Yunxin Li, Baotian Hu, Dongfang Li, Sichao Chen, Xiaolong Wang",
                "Abstract": "  Dialog systems have achieved significant progress and have been widely used\nin various scenarios. The previous researches mainly focused on designing\ndialog generation models in a single scenario, while comprehensive abilities\nare required to handle tasks under various scenarios in the real world. In this\npaper, we propose a general Multi-Skill Dialog Framework, namely MSDF, which\ncan be applied in different dialog tasks (e.g. knowledge grounded dialog and\npersona based dialog). Specifically, we propose a transferable response\ngenerator pre-trained on diverse large-scale dialog corpora as the backbone of\nMSDF, consisting of BERT-based encoders and a GPT-based decoder. To select the\nresponse consistent with dialog history, we propose a consistency selector\ntrained through negative sampling. Moreover, the flexible copy mechanism of\nexternal knowledge is also employed to enhance the utilization of multiform\nknowledge in various scenarios. We conduct experiments on knowledge grounded\ndialog, recommendation dialog, and persona based dialog tasks. The experimental\nresults indicate that our MSDF outperforms the baseline models with a large\nmargin. In the Multi-skill Dialog of 2021 Language and Intelligence Challenge,\nour general MSDF won the 3rd prize, which proves our MSDF is effective and\ncompetitive.\n",
                "publicationDate": "2022-06-17T08:38:53Z",
                "Link": "http://arxiv.org/pdf/2206.08626v1",
                "arxiv_id": "2206.08626v1"
            },
            {
                "Title": "Dialog Acts for Task-Driven Embodied Agents",
                "Authors": "Spandana Gella, Aishwarya Padmakumar, Patrick Lange, Dilek Hakkani-Tur",
                "Abstract": "  Embodied agents need to be able to interact in natural language understanding\ntask descriptions and asking appropriate follow up questions to obtain\nnecessary information to be effective at successfully accomplishing tasks for a\nwide range of users. In this work, we propose a set of dialog acts for\nmodelling such dialogs and annotate the TEACh dataset that includes over 3,000\nsituated, task oriented conversations (consisting of 39.5k utterances in total)\nwith dialog acts. TEACh-DA is one of the first large scale dataset of dialog\nact annotations for embodied task completion. Furthermore, we demonstrate the\nuse of this annotated dataset in training models for tagging the dialog acts of\na given utterance, predicting the dialog act of the next response given a\ndialog history, and use the dialog acts to guide agent's non-dialog behaviour.\nIn particular, our experiments on the TEACh Execution from Dialog History task\nwhere the model predicts the sequence of low level actions to be executed in\nthe environment for embodied task completion, demonstrate that dialog acts can\nimprove end task success rate by up to 2 points compared to the system without\ndialog acts.\n",
                "publicationDate": "2022-09-26T18:41:28Z",
                "Link": "http://arxiv.org/pdf/2209.12953v1",
                "arxiv_id": "2209.12953v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "SPACE-3: Unified Dialog Model Pre-training for Task-Oriented Dialog\n  Understanding and Generation",
                "Authors": "Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang, Luo Si, Yongbin Li",
                "Abstract": "  Recently, pre-training methods have shown remarkable success in task-oriented\ndialog (TOD) systems. However, most existing pre-trained models for TOD focus\non either dialog understanding or dialog generation, but not both. In this\npaper, we propose SPACE-3, a novel unified semi-supervised pre-trained\nconversation model learning from large-scale dialog corpora with limited\nannotations, which can be effectively fine-tuned on a wide range of downstream\ndialog tasks. Specifically, SPACE-3 consists of four successive components in a\nsingle transformer to maintain a task-flow in TOD systems: (i) a dialog\nencoding module to encode dialog history, (ii) a dialog understanding module to\nextract semantic vectors from either user queries or system responses, (iii) a\ndialog policy module to generate a policy vector that contains high-level\nsemantics of the response, and (iv) a dialog generation module to produce\nappropriate responses. We design a dedicated pre-training objective for each\ncomponent. Concretely, we pre-train the dialog encoding module with span mask\nlanguage modeling to learn contextualized dialog information. To capture the\nstructured dialog semantics, we pre-train the dialog understanding module via a\nnovel tree-induced semi-supervised contrastive learning objective with the help\nof extra dialog annotations. In addition, we pre-train the dialog policy module\nby minimizing the L2 distance between its output policy vector and the semantic\nvector of the response for policy optimization. Finally, the dialog generation\nmodel is pre-trained by language modeling. Results show that SPACE-3 achieves\nstate-of-the-art performance on eight downstream dialog benchmarks, including\nintent prediction, dialog state tracking, and end-to-end dialog modeling. We\nalso show that SPACE-3 has a stronger few-shot ability than existing models\nunder the low-resource setting.\n",
                "publicationDate": "2022-09-14T14:17:57Z",
                "Link": "http://arxiv.org/pdf/2209.06664v1",
                "arxiv_id": "2209.06664v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Iterative Policy Learning in End-to-End Trainable Task-Oriented Neural\n  Dialog Models",
                "Authors": "Bing Liu, Ian Lane",
                "Abstract": "  In this paper, we present a deep reinforcement learning (RL) framework for\niterative dialog policy optimization in end-to-end task-oriented dialog\nsystems. Popular approaches in learning dialog policy with RL include letting a\ndialog agent to learn against a user simulator. Building a reliable user\nsimulator, however, is not trivial, often as difficult as building a good\ndialog agent. We address this challenge by jointly optimizing the dialog agent\nand the user simulator with deep RL by simulating dialogs between the two\nagents. We first bootstrap a basic dialog agent and a basic user simulator by\nlearning directly from dialog corpora with supervised training. We then improve\nthem further by letting the two agents to conduct task-oriented dialogs and\niteratively optimizing their policies with deep RL. Both the dialog agent and\nthe user simulator are designed with neural network models that can be trained\nend-to-end. Our experiment results show that the proposed method leads to\npromising improvements on task success rate and total task reward comparing to\nsupervised training and single-agent RL training baseline models.\n",
                "publicationDate": "2017-09-18T19:45:51Z",
                "Link": "http://arxiv.org/pdf/1709.06136v1",
                "arxiv_id": "1709.06136v1"
            },
            {
                "Title": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain\n  Task-Oriented Dialog",
                "Authors": "Ryuichi Takanobu, Hanlin Zhu, Minlie Huang",
                "Abstract": "  Dialog policy decides what and how a task-oriented dialog system will\nrespond, and plays a vital role in delivering effective conversations. Many\nstudies apply Reinforcement Learning to learn a dialog policy with the reward\nfunction which requires elaborate design and pre-specified user goals. With the\ngrowing needs to handle complex goals across multiple domains, such manually\ndesigned reward functions are not affordable to deal with the complexity of\nreal-world tasks. To this end, we propose Guided Dialog Policy Learning, a\nnovel algorithm based on Adversarial Inverse Reinforcement Learning for joint\nreward estimation and policy optimization in multi-domain task-oriented dialog.\nThe proposed approach estimates the reward signal and infers the user goal in\nthe dialog sessions. The reward estimator evaluates the state-action pairs so\nthat it can guide the dialog policy at each dialog turn. Extensive experiments\non a multi-domain dialog dataset show that the dialog policy guided by the\nlearned reward function achieves remarkably higher task success than\nstate-of-the-art baselines.\n",
                "publicationDate": "2019-08-28T13:36:25Z",
                "Link": "http://arxiv.org/pdf/1908.10719v1",
                "arxiv_id": "1908.10719v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with\n  Dialog-KB Inconsistencies",
                "Authors": "Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu,  Mausam",
                "Abstract": "  Task-oriented dialog (TOD) agents often ground their responses on external\nknowledge bases (KBs). These KBs can be dynamic and may be updated frequently.\nExisting approaches for learning TOD agents assume the KB snapshot contemporary\nto each individual dialog is available during training. However, in real-world\nscenarios, only the latest KB snapshot is available during training and as a\nresult, the train dialogs may contain facts conflicting with the latest KB.\nThese dialog-KB inconsistencies in the training data may potentially confuse\nthe TOD agent learning algorithm.\n  In this work, we define the novel problem of learning a TOD agent with\ndialog-KB inconsistencies in the training data. We propose a Dialog-KB\nArbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by\npredicting the contemporary KB snapshot for each train dialog. These predicted\nKB snapshots are then used for training downstream TOD agents. As there are no\nexisting datasets with dialog-KB inconsistencies, we systematically introduce\ninconsistencies in two publicly available dialog datasets. We show that TOD\nagents trained with DKAF perform better than existing baselines on both these\ndatasets\n",
                "publicationDate": "2023-05-26T07:36:23Z",
                "Link": "http://arxiv.org/pdf/2305.16697v1",
                "arxiv_id": "2305.16697v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Data Collection for Interactive Learning through the Dialog",
                "Authors": "Miroslav Vodol\u00e1n, Filip Jur\u010d\u00ed\u010dek",
                "Abstract": "  This paper presents a dataset collected from natural dialogs which enables to\ntest the ability of dialog systems to learn new facts from user utterances\nthroughout the dialog. This interactive learning will help with one of the most\nprevailing problems of open domain dialog system, which is the sparsity of\nfacts a dialog system can reason about. The proposed dataset, consisting of\n1900 collected dialogs, allows simulation of an interactive gaining of\ndenotations and questions explanations from users which can be used for the\ninteractive learning.\n",
                "publicationDate": "2016-03-31T15:13:51Z",
                "Link": "http://arxiv.org/pdf/1603.09631v2",
                "arxiv_id": "1603.09631v2"
            },
            {
                "Title": "Tracking of enriched dialog states for flexible conversational\n  information access",
                "Authors": "Yinpei Dai, Zhijian Ou, Dawei Ren, Pengfei Yu",
                "Abstract": "  Dialog state tracking (DST) is a crucial component in a task-oriented dialog\nsystem for conversational information access. A common practice in current\ndialog systems is to define the dialog state by a set of slot-value pairs. Such\nrepresentation of dialog states and the slot-filling based DST have been widely\nemployed, but suffer from three drawbacks. (1) The dialog state can contain\nonly a single value for a slot, and (2) can contain only users' affirmative\npreference over the values for a slot. (3) Current task-based dialog systems\nmainly focus on the searching task, while the enquiring task is also very\ncommon in practice. The above observations motivate us to enrich current\nrepresentation of dialog states and collect a brand new dialog dataset about\nmovies, based upon which we build a new DST, called enriched DST (EDST), for\nflexible accessing movie information. The EDST supports the searching task, the\nenquiring task and their mixed task. We show that the new EDST method not only\nachieves good results on Iqiyi dataset, but also outperforms other\nstate-of-the-art DST methods on the traditional dialog datasets, WOZ2.0 and\nDSTC2.\n",
                "publicationDate": "2017-11-09T14:10:45Z",
                "Link": "http://arxiv.org/pdf/1711.03381v2",
                "arxiv_id": "1711.03381v2"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Unsupervised Dialog Structure Learning",
                "Authors": "Weiyan Shi, Tiancheng Zhao, Zhou Yu",
                "Abstract": "  Learning a shared dialog structure from a set of task-oriented dialogs is an\nimportant challenge in computational linguistics. The learned dialog structure\ncan shed light on how to analyze human dialogs, and more importantly contribute\nto the design and evaluation of dialog systems. We propose to extract dialog\nstructures using a modified VRNN model with discrete latent vectors. Different\nfrom existing HMM-based models, our model is based on variational-autoencoder\n(VAE). Such model is able to capture more dynamics in dialogs beyond the\nsurface forms of the language. We find that qualitatively, our method extracts\nmeaningful dialog structure, and quantitatively, outperforms previous models on\nthe ability to predict unseen data. We further evaluate the model's\neffectiveness in a downstream task, the dialog system building task.\nExperiments show that, by integrating the learned dialog structure into the\nreward function design, the model converges faster and to a better outcome in a\nreinforcement learning setting.\n",
                "publicationDate": "2019-04-07T20:28:47Z",
                "Link": "http://arxiv.org/pdf/1904.03736v2",
                "arxiv_id": "1904.03736v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2",
                "Authors": "Yunyi Yang, Yunhao Li, Xiaojun Quan",
                "Abstract": "  This paper presents our task-oriented dialog system UBAR which models\ntask-oriented dialogs on a dialog session level. Specifically, UBAR is acquired\nby fine-tuning the large pre-trained unidirectional language model GPT-2 on the\nsequence of the entire dialog session which is composed of user utterance,\nbelief state, database result, system act, and system response of every dialog\nturn. Additionally, UBAR is evaluated in a more realistic setting, where its\ndialog context has access to user utterances and all content it generated such\nas belief states, system acts, and system responses. Experimental results on\nthe MultiWOZ datasets show that UBAR achieves state-of-the-art performances in\nmultiple settings, improving the combined score of response generation, policy\noptimization, and end-to-end modeling by 4.7, 3.5, and 9.4 points respectively.\nThorough analyses demonstrate that the session-level training sequence\nformulation and the generated dialog context are essential for UBAR to operate\nas a fully end-to-end task-oriented dialog system in real life. We also examine\nthe transfer ability of UBAR to new domains with limited data and provide\nvisualization and a case study to illustrate the advantages of UBAR in modeling\non a dialog session level.\n",
                "publicationDate": "2020-12-07T09:08:16Z",
                "Link": "http://arxiv.org/pdf/2012.03539v2",
                "arxiv_id": "2012.03539v2"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Toward More Accurate and Generalizable Evaluation Metrics for\n  Task-Oriented Dialogs",
                "Authors": "Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel, Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, Aram Galstyan",
                "Abstract": "  Measurement of interaction quality is a critical task for the improvement of\nspoken dialog systems. Existing approaches to dialog quality estimation either\nfocus on evaluating the quality of individual turns, or collect dialog-level\nquality measurements from end users immediately following an interaction. In\ncontrast to these approaches, we introduce a new dialog-level annotation\nworkflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate\nthe quality of dialogs as a whole, and also label dialogs for attributes such\nas goal completion and user sentiment. In this contribution, we show that: (i)\nwhile dialog quality cannot be completely decomposed into dialog-level\nattributes, there is a strong relationship between some objective dialog\nattributes and judgments of dialog quality; (ii) for the task of dialog-level\nquality estimation, a supervised model trained on dialog-level annotations\noutperforms methods based purely on aggregating turn-level features; and (iii)\nthe proposed evaluation model shows better domain generalization ability\ncompared to the baselines. On the basis of these results, we argue that having\nhigh-quality human-annotated data is an important component of evaluating\ninteraction quality for large industrial-scale voice assistant platforms.\n",
                "publicationDate": "2023-06-06T19:43:29Z",
                "Link": "http://arxiv.org/pdf/2306.03984v2",
                "arxiv_id": "2306.03984v2"
            },
            {
                "Title": "Domain Adaptive Dialog Generation via Meta Learning",
                "Authors": "Kun Qian, Zhou Yu",
                "Abstract": "  Domain adaptation is an essential task in dialog system building because\nthere are so many new dialog tasks created for different needs every day.\nCollecting and annotating training data for these new tasks is costly since it\ninvolves real user interactions. We propose a domain adaptive dialog generation\nmethod based on meta-learning (DAML). DAML is an end-to-end trainable dialog\nsystem model that learns from multiple rich-resource tasks and then adapts to\nnew domains with minimal training samples. We train a dialog system model using\nmultiple rich-resource single-domain dialog data by applying the model-agnostic\nmeta-learning algorithm to dialog domain. The model is capable of learning a\ncompetitive dialog system on a new domain with only a few training examples in\nan efficient manner. The two-step gradient updates in DAML enable the model to\nlearn general features across multiple tasks. We evaluate our method on a\nsimulated dialog dataset and achieve state-of-the-art performance, which is\ngeneralizable to new tasks.\n",
                "publicationDate": "2019-06-08T20:54:02Z",
                "Link": "http://arxiv.org/pdf/1906.03520v2",
                "arxiv_id": "1906.03520v2"
            },
            {
                "Title": "CookDial: A dataset for task-oriented dialogs grounded in procedural\n  documents",
                "Authors": "Yiwei Jiang, Klim Zaporojets, Johannes Deleu, Thomas Demeester, Chris Develder",
                "Abstract": "  This work presents a new dialog dataset, CookDial, that facilitates research\non task-oriented dialog systems with procedural knowledge understanding. The\ncorpus contains 260 human-to-human task-oriented dialogs in which an agent,\ngiven a recipe document, guides the user to cook a dish. Dialogs in CookDial\nexhibit two unique features: (i) procedural alignment between the dialog flow\nand supporting document; (ii) complex agent decision-making that involves\nsegmenting long sentences, paraphrasing hard instructions and resolving\ncoreference in the dialog context. In addition, we identify three challenging\n(sub)tasks in the assumed task-oriented dialog system: (1) User Question\nUnderstanding, (2) Agent Action Frame Prediction, and (3) Agent Response\nGeneration. For each of these tasks, we develop a neural baseline model, which\nwe evaluate on the CookDial dataset. We publicly release the CookDial dataset,\ncomprising rich annotations of both dialogs and recipe documents, to stimulate\nfurther research on domain-specific document-grounded dialog systems.\n",
                "publicationDate": "2022-06-17T12:23:53Z",
                "Link": "http://arxiv.org/pdf/2206.08723v1",
                "arxiv_id": "2206.08723v1"
            },
            {
                "Title": "MOSS: End-to-End Dialog System Framework with Modular Supervision",
                "Authors": "Weixin Liang, Youzhi Tian, Chengcai Chen, Zhou Yu",
                "Abstract": "  A major bottleneck in training end-to-end task-oriented dialog system is the\nlack of data. To utilize limited training data more efficiently, we propose\nModular Supervision Network (MOSS), an encoder-decoder training framework that\ncould incorporate supervision from various intermediate dialog system modules\nincluding natural language understanding, dialog state tracking, dialog policy\nlearning, and natural language generation. With only 60% of the training data,\nMOSS-all (i.e., MOSS with supervision from all four dialog modules) outperforms\nstate-of-the-art models on CamRest676. Moreover, introducing modular\nsupervision has even bigger benefits when the dialog task has a more complex\ndialog state and action space. With only 40% of the training data, MOSS-all\noutperforms the state-of-the-art model on a complex laptop network\ntroubleshooting dataset, LaptopNetwork, that we introduced. LaptopNetwork\nconsists of conversations between real customers and customer service agents in\nChinese. Moreover, MOSS framework can accommodate dialogs that have supervision\nfrom different dialog modules at both the framework level and model level.\nTherefore, MOSS is extremely flexible to update in a real-world deployment.\n",
                "publicationDate": "2019-09-12T09:27:37Z",
                "Link": "http://arxiv.org/pdf/1909.05528v1",
                "arxiv_id": "1909.05528v1"
            },
            {
                "Title": "A Survey on Dialog Management: Recent Advances and Challenges",
                "Authors": "Yinpei Dai, Huihua Yu, Yixuan Jiang, Chengguang Tang, Yongbin Li, Jian Sun",
                "Abstract": "  Dialog management (DM) is a crucial component in a task-oriented dialog\nsystem. Given the dialog history, DM predicts the dialog state and decides the\nnext action that the dialog agent should take. Recently, dialog policy learning\nhas been widely formulated as a Reinforcement Learning (RL) problem, and more\nworks focus on the applicability of DM. In this paper, we survey recent\nadvances and challenges within three critical topics for DM: (1) improving\nmodel scalability to facilitate dialog system modeling in new scenarios, (2)\ndealing with the data scarcity problem for dialog policy learning, and (3)\nenhancing the training efficiency to achieve better task-completion performance\n. We believe that this survey can shed a light on future research in dialog\nmanagement.\n",
                "publicationDate": "2020-05-05T14:31:24Z",
                "Link": "http://arxiv.org/pdf/2005.02233v3",
                "arxiv_id": "2005.02233v3"
            },
            {
                "Title": "Zero-Shot Dialog Generation with Cross-Domain Latent Actions",
                "Authors": "Tiancheng Zhao, Maxine Eskenazi",
                "Abstract": "  This paper introduces zero-shot dialog generation (ZSDG), as a step towards\nneural dialog systems that can instantly generalize to new situations with\nminimal data. ZSDG enables an end-to-end generative dialog system to generalize\nto a new domain for which only a domain description is provided and no training\ndialogs are available. Then a novel learning framework, Action Matching, is\nproposed. This algorithm can learn a cross-domain embedding space that models\nthe semantics of dialog responses which, in turn, lets a neural dialog\ngeneration model generalize to new domains. We evaluate our methods on a new\nsynthetic dialog dataset, and an existing human-human dialog dataset. Results\nshow that our method has superior performance in learning dialog models that\nrapidly adapt their behavior to new domains and suggests promising future\nresearch.\n",
                "publicationDate": "2018-05-13T01:07:32Z",
                "Link": "http://arxiv.org/pdf/1805.04803v1",
                "arxiv_id": "1805.04803v1"
            },
            {
                "Title": "\"Think Before You Speak\": Improving Multi-Action Dialog Policy by\n  Planning Single-Action Dialogs",
                "Authors": "Shuo Zhang, Junzhou Zhao, Pinghui Wang, Yu Li, Yi Huang, Junlan Feng",
                "Abstract": "  Multi-action dialog policy (MADP), which generates multiple atomic dialog\nactions per turn, has been widely applied in task-oriented dialog systems to\nprovide expressive and efficient system responses. Existing MADP models usually\nimitate action combinations from the labeled multi-action dialog samples. Due\nto data limitations, they generalize poorly toward unseen dialog flows. While\ninteractive learning and reinforcement learning algorithms can be applied to\nincorporate external data sources of real users and user simulators, they take\nsignificant manual effort to build and suffer from instability. To address\nthese issues, we propose Planning Enhanced Dialog Policy (PEDP), a novel\nmulti-task learning framework that learns single-action dialog dynamics to\nenhance multi-action prediction. Our PEDP method employs model-based planning\nfor conceiving what to express before deciding the current response through\nsimulating single-action dialogs. Experimental results on the MultiWOZ dataset\ndemonstrate that our fully supervised learning-based method achieves a solid\ntask success rate of 90.6%, improving 3% compared to the state-of-the-art\nmethods.\n",
                "publicationDate": "2022-04-25T07:55:53Z",
                "Link": "http://arxiv.org/pdf/2204.11481v1",
                "arxiv_id": "2204.11481v1"
            },
            {
                "Title": "Why Do Neural Dialog Systems Generate Short and Meaningless Replies? A\n  Comparison between Dialog and Translation",
                "Authors": "Bolin Wei, Shuai Lu, Lili Mou, Hao Zhou, Pascal Poupart, Ge Li, Zhi Jin",
                "Abstract": "  This paper addresses the question: Why do neural dialog systems generate\nshort and meaningless replies? We conjecture that, in a dialog system, an\nutterance may have multiple equally plausible replies, causing the deficiency\nof neural networks in the dialog application. We propose a systematic way to\nmimic the dialog scenario in a machine translation system, and manage to\nreproduce the phenomenon of generating short and less meaningful sentences in\nthe translation setting, showing evidence of our conjecture.\n",
                "publicationDate": "2017-12-06T16:00:45Z",
                "Link": "http://arxiv.org/pdf/1712.02250v1",
                "arxiv_id": "1712.02250v1"
            },
            {
                "Title": "Specifying and Staging Mixed-Initiative Dialogs with Program Generation\n  and Transformation",
                "Authors": "Saverio Perugini",
                "Abstract": "  Specifying and implementing flexible human-computer dialogs, such as those\nused in kiosks and smart phone apps, is challenging because of the numerous and\nvaried directions in which each user might steer a dialog. The objective of\nthis research is to improve dialog specification and implementation. To do so\nwe enriched a notation based on concepts from programming languages, especially\npartial evaluation, for specifying a variety of unsolicited reporting,\nmixed-initiative dialogs in a concise representation that serves as a design\nfor dialog implementation. We also built a dialog mining system that extracts a\nspecification in this notation from requirements. To demonstrate that such a\nspecification provides a design for dialog implementation, we built a system\nthat automatically generates an implementation of the dialog, called a stager,\nfrom it. These two components constitute a dialog modeling toolkit that\nautomates dialog specification and implementation. These results provide a\nproof of concept and demonstrate the study of dialog specification and\nimplementation from a programming languages perspective. The ubiquity of\ndialogs in domains such as travel, education, and health care combined with the\ndemand for smart phone apps provide a landscape for further investigation of\nthese results.\n",
                "publicationDate": "2011-08-02T03:00:09Z",
                "Link": "http://arxiv.org/pdf/1108.0476v5",
                "arxiv_id": "1108.0476v5"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "DialPort: Connecting the Spoken Dialog Research Community to Real User\n  Data",
                "Authors": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi",
                "Abstract": "  This paper describes a new spoken dialog portal that connects systems\nproduced by the spoken dialog academic research community and gives them access\nto real users. We introduce a distributed, multi-modal, multi-agent prototype\ndialog framework that affords easy integration with various remote resources,\nranging from end-to-end dialog systems to external knowledge APIs. To date, the\nDialPort portal has successfully connected to the multi-domain spoken dialog\nsystem at Cambridge University, the NOAA (National Oceanic and Atmospheric\nAdministration) weather API and the Yelp API.\n",
                "publicationDate": "2016-06-08T14:08:21Z",
                "Link": "http://arxiv.org/pdf/1606.02562v1",
                "arxiv_id": "1606.02562v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Adversarial Learning of Task-Oriented Neural Dialog Models",
                "Authors": "Bing Liu, Ian Lane",
                "Abstract": "  In this work, we propose an adversarial learning method for reward estimation\nin reinforcement learning (RL) based task-oriented dialog models. Most of the\ncurrent RL based task-oriented dialog systems require the access to a reward\nsignal from either user feedback or user ratings. Such user ratings, however,\nmay not always be consistent or available in practice. Furthermore, online\ndialog policy learning with RL typically requires a large number of queries to\nusers, suffering from sample efficiency problem. To address these challenges,\nwe propose an adversarial learning method to learn dialog rewards directly from\ndialog samples. Such rewards are further used to optimize the dialog policy\nwith policy gradient based RL. In the evaluation in a restaurant search domain,\nwe show that the proposed adversarial dialog learning method achieves advanced\ndialog success rate comparing to strong baseline methods. We further discuss\nthe covariate shift problem in online adversarial dialog learning and show how\nwe can address that with partial access to user feedback.\n",
                "publicationDate": "2018-05-30T00:48:44Z",
                "Link": "http://arxiv.org/pdf/1805.11762v1",
                "arxiv_id": "1805.11762v1"
            },
            {
                "Title": "SUMBT+LaRL: Effective Multi-domain End-to-end Neural Task-oriented\n  Dialog System",
                "Authors": "Hwaran Lee, Seokhwan Jo, HyungJun Kim, Sangkeun Jung, Tae-Yoon Kim",
                "Abstract": "  The recent advent of neural approaches for developing each dialog component\nin task-oriented dialog systems has remarkably improved, yet optimizing the\noverall system performance remains a challenge. Besides, previous research on\nmodeling complicated multi-domain goal-oriented dialogs in end-to-end fashion\nhas been limited. In this paper, we present an effective multi-domain\nend-to-end trainable neural dialog system SUMBT+LaRL that incorporates two\nprevious strong models and facilitates them to be fully differentiable.\nSpecifically, the SUMBT+ estimates user-acts as well as dialog belief states,\nand the LaRL models latent system action spaces and generates responses given\nthe estimated contexts. We emphasize that the training framework of three steps\nsignificantly and stably increase dialog success rates: separately pretraining\nthe SUMBT+ and LaRL, fine-tuning the entire system, and then reinforcement\nlearning of dialog policy. We also introduce new reward criteria of\nreinforcement learning for dialog policy training. Then, we discuss\nexperimental results depending on the reward criteria and different dialog\nevaluation methods. Consequently, our model achieved the new state-of-the-art\nsuccess rate of 85.4% on corpus-based evaluation, and a comparable success rate\nof 81.40% on simulator-based evaluation provided by the DSTC8 challenge. To our\nbest knowledge, our work is the first comprehensive study of a modularized E2E\nmulti-domain dialog system that learning from each component to the entire\ndialog policy for task success.\n",
                "publicationDate": "2020-09-22T11:02:21Z",
                "Link": "http://arxiv.org/pdf/2009.10447v3",
                "arxiv_id": "2009.10447v3"
            },
            {
                "Title": "End-to-end Conversation Modeling Track in DSTC6",
                "Authors": "Chiori Hori, Takaaki Hori",
                "Abstract": "  End-to-end training of neural networks is a promising approach to automatic\nconstruction of dialog systems using a human-to-human dialog corpus. Recently,\nVinyals et al. tested neural conversation models using OpenSubtitles. Lowe et\nal. released the Ubuntu Dialogue Corpus for researching unstructured multi-turn\ndialogue systems. Furthermore, the approach has been extended to accomplish\ntask oriented dialogs to provide information properly with natural\nconversation. For example, Ghazvininejad et al. proposed a knowledge grounded\nneural conversation model [3], where the research is aiming at combining\nconversational dialogs with task-oriented knowledge using unstructured data\nsuch as Twitter data for conversation and Foursquare data for external\nknowledge.However, the task is still limited to a restaurant information\nservice, and has not yet been tested with a wide variety of dialog tasks. In\naddition, it is still unclear how to create intelligent dialog systems that can\nrespond like a human agent.\n  In consideration of these problems, we proposed a challenge track to the 6th\ndialog system technology challenges (DSTC6) using human-to-human dialog data to\nmimic human dialog behaviors. The focus of the challenge track is to train\nend-to-end conversation models from human-to-human conversation and accomplish\nend-to-end dialog tasks in various situations assuming a customer service, in\nwhich a system plays a role of human agent and generates natural and\ninformative sentences in response to user's questions or comments given dialog\ncontext.\n",
                "publicationDate": "2017-06-22T18:00:34Z",
                "Link": "http://arxiv.org/pdf/1706.07440v2",
                "arxiv_id": "1706.07440v2"
            },
            {
                "Title": "Psychological Metrics for Dialog System Evaluation",
                "Authors": "Salvatore Giorgi, Shreya Havaldar, Farhan Ahmed, Zuhaib Akhtar, Shalaka Vaidya, Gary Pan, Lyle H. Ungar, H. Andrew Schwartz, Joao Sedoc",
                "Abstract": "  We present metrics for evaluating dialog systems through a\npsychologically-grounded \"human\" lens in which conversational agents express a\ndiversity of both states (e.g., emotion) and traits (e.g., personality), just\nas people do. We present five interpretable metrics from established psychology\nthat are fundamental to human communication and relationships: emotional\nentropy, linguistic style and emotion matching, agreeableness, and empathy.\nThese metrics can be applied (1) across dialogs and (2) on turns within\ndialogs. The psychological metrics are compared against seven state-of-the-art\ntraditional metrics (e.g., BARTScore and BLEURT) on seven standard dialog\nsystem data sets. We also introduce a novel data set, the Three Bot Dialog\nEvaluation Corpus, which consists of annotated conversations from ChatGPT,\nGPT-3, and BlenderBot. We demonstrate that our proposed metrics offer novel\ninformation; they are uncorrelated with traditional metrics, can be used to\nmeaningfully compare dialog systems, and lead to increased accuracy (beyond\nexisting traditional metrics) in predicting crowd-sourced dialog judgements.\nThe interpretability and unique signal of our psychological metrics make them a\nvaluable tool for evaluating and improving dialog systems.\n",
                "publicationDate": "2023-05-24T06:02:32Z",
                "Link": "http://arxiv.org/pdf/2305.14757v2",
                "arxiv_id": "2305.14757v2"
            },
            {
                "Title": "Recent Advances and Challenges in Task-oriented Dialog System",
                "Authors": "Zheng Zhang, Ryuichi Takanobu, Qi Zhu, Minlie Huang, Xiaoyan Zhu",
                "Abstract": "  Due to the significance and value in human-computer interaction and natural\nlanguage processing, task-oriented dialog systems are attracting more and more\nattention in both academic and industrial communities. In this paper, we survey\nrecent advances and challenges in task-oriented dialog systems. We also discuss\nthree critical topics for task-oriented dialog systems: (1) improving data\nefficiency to facilitate dialog modeling in low-resource settings, (2) modeling\nmulti-turn dynamics for dialog policy learning to achieve better\ntask-completion performance, and (3) integrating domain ontology knowledge into\nthe dialog model. Besides, we review the recent progresses in dialog evaluation\nand some widely-used corpora. We believe that this survey, though incomplete,\ncan shed a light on future research in task-oriented dialog systems.\n",
                "publicationDate": "2020-03-17T01:34:56Z",
                "Link": "http://arxiv.org/pdf/2003.07490v3",
                "arxiv_id": "2003.07490v3"
            },
            {
                "Title": "Deep Reinforcement Learning for Inquiry Dialog Policies with Logical\n  Formula Embeddings",
                "Authors": "Takuya Hiraoka, Masaaki Tsuchida, Yotaro Watanabe",
                "Abstract": "  This paper is the first attempt to learn the policy of an inquiry dialog\nsystem (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks\nrepresent dialog states and dialog acts with logical formulae. In order to make\nlearning inquiry dialog policies more effective, we introduce a logical formula\nembedding framework based on a recursive neural network. The results of\nexperiments to evaluate the effect of 1) the DRL and 2) the logical formula\nembedding framework show that the combination of the two are as effective or\neven better than existing rule-based methods for inquiry dialog policies.\n",
                "publicationDate": "2017-08-02T09:40:42Z",
                "Link": "http://arxiv.org/pdf/1708.00667v1",
                "arxiv_id": "1708.00667v1"
            }
        ]
    },
    {
        "topic_name": "Semantic Parsing",
        "summary": "default",
        "papers": [
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Context Dependent Semantic Parsing: A Survey",
                "Authors": "Zhuang Li, Lizhen Qu, Gholamreza Haffari",
                "Abstract": "  Semantic parsing is the task of translating natural language utterances into\nmachine-readable meaning representations. Currently, most semantic parsing\nmethods are not able to utilize contextual information (e.g. dialogue and\ncomments history), which has a great potential to boost semantic parsing\nperformance. To address this issue, context dependent semantic parsing has\nrecently drawn a lot of attention. In this survey, we investigate progress on\nthe methods for the context dependent semantic parsing, together with the\ncurrent datasets and tasks. We then point out open problems and challenges for\nfuture research in this area. The collected resources for this topic are\navailable\nat:https://github.com/zhuang-li/Contextual-Semantic-Parsing-Paper-List.\n",
                "publicationDate": "2020-11-02T07:51:05Z",
                "Link": "http://arxiv.org/pdf/2011.00797v1",
                "arxiv_id": "2011.00797v1"
            },
            {
                "Title": "Discourse Representation Structure Parsing for Chinese",
                "Authors": "Chunliu Wang, Xiao Zhang, Johan Bos",
                "Abstract": "  Previous work has predominantly focused on monolingual English semantic\nparsing. We, instead, explore the feasibility of Chinese semantic parsing in\nthe absence of labeled data for Chinese meaning representations. We describe\nthe pipeline of automatically collecting the linearized Chinese meaning\nrepresentation data for sequential-to sequential neural networks. We further\npropose a test suite designed explicitly for Chinese semantic parsing, which\nprovides fine-grained evaluation for parsing performance, where we aim to study\nChinese parsing difficulties. Our experimental results show that the difficulty\nof Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese\nparsing through machine translation and an English parser yields slightly lower\nperformance than training a model directly on Chinese data.\n",
                "publicationDate": "2023-06-16T09:47:45Z",
                "Link": "http://arxiv.org/pdf/2306.09725v1",
                "arxiv_id": "2306.09725v1"
            },
            {
                "Title": "A Survey of Syntactic-Semantic Parsing Based on Constituent and\n  Dependency Structures",
                "Authors": "Meishan Zhang",
                "Abstract": "  Syntactic and semantic parsing has been investigated for decades, which is\none primary topic in the natural language processing community. This article\naims for a brief survey on this topic. The parsing community includes many\ntasks, which are difficult to be covered fully. Here we focus on two of the\nmost popular formalizations of parsing: constituent parsing and dependency\nparsing. Constituent parsing is majorly targeted to syntactic analysis, and\ndependency parsing can handle both syntactic and semantic analysis. This\narticle briefly reviews the representative models of constituent parsing and\ndependency parsing, and also dependency graph parsing with rich semantics.\nBesides, we also review the closely-related topics such as cross-domain,\ncross-lingual and joint parsing models, parser application as well as corpus\ndevelopment of parsing in the article.\n",
                "publicationDate": "2020-06-19T10:21:17Z",
                "Link": "http://arxiv.org/pdf/2006.11056v1",
                "arxiv_id": "2006.11056v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Cornell SPF: Cornell Semantic Parsing Framework",
                "Authors": "Yoav Artzi",
                "Abstract": "  The Cornell Semantic Parsing Framework (SPF) is a learning and inference\nframework for mapping natural language to formal representation of its meaning.\n",
                "publicationDate": "2013-11-13T03:58:38Z",
                "Link": "http://arxiv.org/pdf/1311.3011v2",
                "arxiv_id": "1311.3011v2"
            },
            {
                "Title": "Parsing All: Syntax and Semantics, Dependencies and Spans",
                "Authors": "Junru Zhou, Zuchao Li, Hai Zhao",
                "Abstract": "  Both syntactic and semantic structures are key linguistic contextual clues,\nin which parsing the latter has been well shown beneficial from parsing the\nformer. However, few works ever made an attempt to let semantic parsing help\nsyntactic parsing. As linguistic representation formalisms, both syntax and\nsemantics may be represented in either span (constituent/phrase) or dependency,\non both of which joint learning was also seldom explored. In this paper, we\npropose a novel joint model of syntactic and semantic parsing on both span and\ndependency representations, which incorporates syntactic information\neffectively in the encoder of neural network and benefits from two\nrepresentation formalisms in a uniform way. The experiments show that semantics\nand syntax can benefit each other by optimizing joint objectives. Our single\nmodel achieves new state-of-the-art or competitive results on both span and\ndependency semantic parsing on Propbank benchmarks and both dependency and\nconstituent syntactic parsing on Penn Treebank.\n",
                "publicationDate": "2019-08-30T03:49:19Z",
                "Link": "http://arxiv.org/pdf/1908.11522v3",
                "arxiv_id": "1908.11522v3"
            },
            {
                "Title": "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via\n  Synchronous Semantic Decoding",
                "Authors": "Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun, Weipeng Zhang, Jiansong Chen, Fan Yang, Xunliang Cai",
                "Abstract": "  Semantic parsing is challenging due to the structure gap and the semantic gap\nbetween utterances and logical forms. In this paper, we propose an unsupervised\nsemantic parsing method - Synchronous Semantic Decoding (SSD), which can\nsimultaneously resolve the semantic gap and the structure gap by jointly\nleveraging paraphrasing and grammar constrained decoding. Specifically, we\nreformulate semantic parsing as a constrained paraphrasing problem: given an\nutterance, our model synchronously generates its canonical utterance and\nmeaning representation. During synchronous decoding: the utterance paraphrasing\nis constrained by the structure of the logical form, therefore the canonical\nutterance can be paraphrased controlledly; the semantic decoding is guided by\nthe semantics of the canonical utterance, therefore its logical form can be\ngenerated unsupervisedly. Experimental results show that SSD is a promising\napproach and can achieve competitive unsupervised semantic parsing performance\non multiple datasets.\n",
                "publicationDate": "2021-06-11T08:16:35Z",
                "Link": "http://arxiv.org/pdf/2106.06228v1",
                "arxiv_id": "2106.06228v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Progressive refinement: a method of coarse-to-fine image parsing using\n  stacked network",
                "Authors": "Jiagao Hu, Zhengxing Sun, Yunhan Sun, Jinlong Shi",
                "Abstract": "  To parse images into fine-grained semantic parts, the complex fine-grained\nelements will put it in trouble when using off-the-shelf semantic segmentation\nnetworks. In this paper, for image parsing task, we propose to parse images\nfrom coarse to fine with progressively refined semantic classes. It is achieved\nby stacking the segmentation layers in a segmentation network several times.\nThe former segmentation module parses images at a coarser-grained level, and\nthe result will be feed to the following one to provide effective contextual\nclues for the finer-grained parsing. To recover the details of small\nstructures, we add skip connections from shallow layers of the network to\nfine-grained parsing modules. As for the network training, we merge classes in\ngroundtruth to get coarse-to-fine label maps, and train the stacked network\nwith these hierarchical supervision end-to-end. Our coarse-to-fine stacked\nframework can be injected into many advanced neural networks to improve the\nparsing results. Extensive evaluations on several public datasets including\nface parsing and human parsing well demonstrate the superiority of our method.\n",
                "publicationDate": "2018-04-23T06:33:53Z",
                "Link": "http://arxiv.org/pdf/1804.08256v1",
                "arxiv_id": "1804.08256v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
                "Authors": "Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam",
                "Abstract": "  Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.\n",
                "publicationDate": "2024-01-22T18:52:51Z",
                "Link": "http://arxiv.org/pdf/2401.12210v1",
                "arxiv_id": "2401.12210v1"
            },
            {
                "Title": "Multitask Parsing Across Semantic Representations",
                "Authors": "Daniel Hershcovich, Omri Abend, Ari Rappoport",
                "Abstract": "  The ability to consolidate information of different types is at the core of\nintelligence, and has tremendous practical value in allowing learning for one\ntask to benefit from generalizations learned for others. In this paper we\ntackle the challenging task of improving semantic parsing performance, taking\nUCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD)\nparsing as auxiliary tasks. We experiment on three languages, using a uniform\ntransition-based system and learning architecture for all parsing tasks.\nDespite notable conceptual, formal and domain differences, we show that\nmultitask learning significantly improves UCCA parsing in both in-domain and\nout-of-domain settings.\n",
                "publicationDate": "2018-05-01T12:21:50Z",
                "Link": "http://arxiv.org/pdf/1805.00287v1",
                "arxiv_id": "1805.00287v1"
            },
            {
                "Title": "Efficient Normal-Form Parsing for Combinatory Categorial Grammar",
                "Authors": "Jason Eisner",
                "Abstract": "  Under categorial grammars that have powerful rules like composition, a simple\nn-word sentence can have exponentially many parses. Generating all parses is\ninefficient and obscures whatever true semantic ambiguities are in the input.\nThis paper addresses the problem for a fairly general form of Combinatory\nCategorial Grammar, by means of an efficient, correct, and easy to implement\nnormal-form parsing technique. The parser is proved to find exactly one parse\nin each semantic equivalence class of allowable parses; that is, spurious\nambiguity (as carefully defined) is shown to be both safely and completely\neliminated.\n",
                "publicationDate": "1996-06-02T01:55:57Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9605038v1",
                "arxiv_id": "9605038v1"
            },
            {
                "Title": "Hierarchical Neural Data Synthesis for Semantic Parsing",
                "Authors": "Wei Yang, Peng Xu, Yanshuai Cao",
                "Abstract": "  Semantic parsing datasets are expensive to collect. Moreover, even the\nquestions pertinent to a given domain, which are the input of a semantic\nparsing system, might not be readily available, especially in cross-domain\nsemantic parsing. This makes data augmentation even more challenging. Existing\nmethods to synthesize new data use hand-crafted or induced rules, requiring\nsubstantial engineering effort and linguistic expertise to achieve good\ncoverage and precision, which limits the scalability. In this work, we propose\na purely neural approach of data augmentation for semantic parsing that\ncompletely removes the need for grammar engineering while achieving higher\nsemantic parsing accuracy. Furthermore, our method can synthesize in the\nzero-shot setting, where only a new domain schema is available without any\ninput-output examples of the new domain. On the Spider cross-domain text-to-SQL\nsemantic parsing benchmark, we achieve the state-of-the-art performance on the\ndevelopment set (77.2% accuracy) using our zero-shot augmentation.\n",
                "publicationDate": "2021-12-04T01:33:08Z",
                "Link": "http://arxiv.org/pdf/2112.02212v1",
                "arxiv_id": "2112.02212v1"
            },
            {
                "Title": "Fast semantic parsing with well-typedness guarantees",
                "Authors": "Matthias Lindemann, Jonas Groschwitz, Alexander Koller",
                "Abstract": "  AM dependency parsing is a linguistically principled method for neural\nsemantic parsing with high accuracy across multiple graphbanks. It relies on a\ntype system that models semantic valency but makes existing parsers slow. We\ndescribe an A* parser and a transition-based parser for AM dependency parsing\nwhich guarantee well-typedness and improve parsing speed by up to 3 orders of\nmagnitude, while maintaining or improving accuracy.\n",
                "publicationDate": "2020-09-15T21:54:01Z",
                "Link": "http://arxiv.org/pdf/2009.07365v2",
                "arxiv_id": "2009.07365v2"
            },
            {
                "Title": "Memory-Based Semantic Parsing",
                "Authors": "Parag Jain, Mirella Lapata",
                "Abstract": "  We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.\n",
                "publicationDate": "2021-09-07T16:15:13Z",
                "Link": "http://arxiv.org/pdf/2110.07358v1",
                "arxiv_id": "2110.07358v1"
            },
            {
                "Title": "TreePiece: Faster Semantic Parsing via Tree Tokenization",
                "Authors": "Sid Wang, Akshat Shrivastava, Sasha Livshits",
                "Abstract": "  Autoregressive (AR) encoder-decoder neural networks have proved successful in\nmany NLP problems, including Semantic Parsing -- a task that translates natural\nlanguage to machine-readable parse trees. However, the sequential prediction\nprocess of AR models can be slow. To accelerate AR for semantic parsing, we\nintroduce a new technique called TreePiece that tokenizes a parse tree into\nsubtrees and generates one subtree per decoding step. On TopV2 benchmark,\nTreePiece shows 4.6 times faster decoding speed than standard AR, and\ncomparable speed but significantly higher accuracy compared to\nNon-Autoregressive (NAR).\n",
                "publicationDate": "2023-03-30T05:44:44Z",
                "Link": "http://arxiv.org/pdf/2303.17161v1",
                "arxiv_id": "2303.17161v1"
            },
            {
                "Title": "SemEval 2019 Shared Task: Cross-lingual Semantic Parsing with UCCA -\n  Call for Participation",
                "Authors": "Daniel Hershcovich, Leshem Choshen, Elior Sulem, Zohar Aizenbud, Ari Rappoport, Omri Abend",
                "Abstract": "  We announce a shared task on UCCA parsing in English, German and French, and\ncall for participants to submit their systems. UCCA is a cross-linguistically\napplicable framework for semantic representation, which builds on extensive\ntypological work and supports rapid annotation. UCCA poses a challenge for\nexisting parsing techniques, as it exhibits reentrancy (resulting in DAG\nstructures), discontinuous structures and non-terminal nodes corresponding to\ncomplex semantic units. Given the success of recent semantic parsing shared\ntasks (on SDP and AMR), we expect the task to have a significant contribution\nto the advancement of UCCA parsing in particular, and semantic parsing in\ngeneral. Furthermore, existing applications for semantic evaluation that are\nbased on UCCA will greatly benefit from better automatic methods for UCCA\nparsing. The competition website is\nhttps://competitions.codalab.org/competitions/19160\n",
                "publicationDate": "2018-05-31T09:11:16Z",
                "Link": "http://arxiv.org/pdf/1805.12386v4",
                "arxiv_id": "1805.12386v4"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Semantic Parsing for Task Oriented Dialog using Hierarchical\n  Representations",
                "Authors": "Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, Mike Lewis",
                "Abstract": "  Task oriented dialog systems typically first parse user utterances to\nsemantic frames comprised of intents and slots. Previous work on task oriented\nintent and slot-filling work has been restricted to one intent per query and\none slot label per token, and thus cannot model complex compositional requests.\nAlternative semantic parsing systems have represented queries as logical forms,\nbut these are challenging to annotate and parse. We propose a hierarchical\nannotation scheme for semantic parsing that allows the representation of\ncompositional queries, and can be efficiently and accurately parsed by standard\nconstituency parsing models. We release a dataset of 44k annotated queries\n(fb.me/semanticparsingdialog), and show that parsing models outperform\nsequence-to-sequence approaches on this dataset.\n",
                "publicationDate": "2018-10-18T08:22:49Z",
                "Link": "http://arxiv.org/pdf/1810.07942v1",
                "arxiv_id": "1810.07942v1"
            },
            {
                "Title": "Multiple-Human Parsing in the Wild",
                "Authors": "Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong Li, Terence Sim, Shuicheng Yan, Jiashi Feng",
                "Abstract": "  Human parsing is attracting increasing research attention. In this work, we\naim to push the frontier of human parsing by introducing the problem of\nmulti-human parsing in the wild. Existing works on human parsing mainly tackle\nsingle-person scenarios, which deviates from real-world applications where\nmultiple persons are present simultaneously with interaction and occlusion. To\naddress the multi-human parsing problem, we introduce a new multi-human parsing\n(MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP\ndataset contains multiple persons captured in real-world scenes with\npixel-level fine-grained semantic annotations in an instance-aware setting. The\nMH-Parser generates global parsing maps and person instance masks\nsimultaneously in a bottom-up fashion with the help of a new Graph-GAN model.\nWe envision that the MHP dataset will serve as a valuable data resource to\ndevelop new multi-human parsing models, and the MH-Parser offers a strong\nbaseline to drive future research for multi-human parsing in the wild.\n",
                "publicationDate": "2017-05-19T21:59:09Z",
                "Link": "http://arxiv.org/pdf/1705.07206v2",
                "arxiv_id": "1705.07206v2"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Semantic Parsing for Question Answering over Knowledge Graphs",
                "Authors": "Sijia Wei, Wenwen Zhang, Qisong Li, Jiang Zhao",
                "Abstract": "  In this paper, we introduce a novel method with graph-to-segment mapping for\nquestion answering over knowledge graphs, which helps understanding question\nutterances. This method centers on semantic parsing, a key approach for\ninterpreting these utterances. The challenges lie in comprehending implicit\nentities, relationships, and complex constraints like time, ordinality, and\naggregation within questions, contextualized by the knowledge graph. Our\nframework employs a combination of rule-based and neural-based techniques to\nparse and construct highly accurate and comprehensive semantic segment\nsequences. These sequences form semantic query graphs, effectively representing\nquestion utterances. We approach question semantic parsing as a sequence\ngeneration task, utilizing an encoder-decoder neural network to transform\nnatural language questions into semantic segments. Moreover, to enhance the\nparsing of implicit entities and relations, we incorporate a graph neural\nnetwork that leverages the context of the knowledge graph to better understand\nquestion representations. Our experimental evaluations on two datasets\ndemonstrate the effectiveness and superior performance of our model in semantic\nparsing for question answering.\n",
                "publicationDate": "2023-12-01T20:45:06Z",
                "Link": "http://arxiv.org/pdf/2401.06772v2",
                "arxiv_id": "2401.06772v2"
            },
            {
                "Title": "Model-based Interactive Semantic Parsing: A Unified Framework and A\n  Text-to-SQL Case Study",
                "Authors": "Ziyu Yao, Yu Su, Huan Sun, Wen-tau Yih",
                "Abstract": "  As a promising paradigm, interactive semantic parsing has shown to improve\nboth semantic parsing accuracy and user confidence in the results. In this\npaper, we propose a new, unified formulation of the interactive semantic\nparsing problem, where the goal is to design a model-based intelligent agent.\nThe agent maintains its own state as the current predicted semantic parse,\ndecides whether and where human intervention is needed, and generates a\nclarification question in natural language. A key part of the agent is a world\nmodel: it takes a percept (either an initial question or subsequent feedback\nfrom the user) and transitions to a new state. We then propose a simple yet\nremarkably effective instantiation of our framework, demonstrated on two\ntext-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base\nsemantic parsers. Compared to an existing interactive semantic parsing approach\nthat treats the base parser as a black box, our approach solicits less user\nfeedback but yields higher run-time accuracy.\n",
                "publicationDate": "2019-10-11T19:56:47Z",
                "Link": "http://arxiv.org/pdf/1910.05389v1",
                "arxiv_id": "1910.05389v1"
            },
            {
                "Title": "Graphonomy: Universal Human Parsing via Graph Transfer Learning",
                "Authors": "Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, Liang Lin",
                "Abstract": "  Prior highly-tuned human parsing models tend to fit towards each dataset in a\nspecific domain or with discrepant label granularity, and can hardly be adapted\nto other human parsing tasks without extensive re-training. In this paper, we\naim to learn a single universal human parsing model that can tackle all kinds\nof human parsing needs by unifying label annotations from different domains or\nat various levels of granularity. This poses many fundamental learning\nchallenges, e.g. discovering underlying semantic structures among different\nlabel granularity, performing proper transfer learning across different image\ndomains, and identifying and utilizing label redundancies across related tasks.\n  To address these challenges, we propose a new universal human parsing agent,\nnamed \"Graphonomy\", which incorporates hierarchical graph transfer learning\nupon the conventional parsing network to encode the underlying label semantic\nstructures and propagate relevant semantic information. In particular,\nGraphonomy first learns and propagates compact high-level graph representation\namong the labels within one dataset via Intra-Graph Reasoning, and then\ntransfers semantic information across multiple datasets via Inter-Graph\nTransfer. Various graph transfer dependencies (\\eg, similarity, linguistic\nknowledge) between different datasets are analyzed and encoded to enhance graph\ntransfer capability. By distilling universal semantic graph representation to\neach specific task, Graphonomy is able to predict all levels of parsing labels\nin one system without piling up the complexity. Experimental results show\nGraphonomy effectively achieves the state-of-the-art results on three human\nparsing benchmarks as well as advantageous universal human parsing performance.\n",
                "publicationDate": "2019-04-09T08:49:18Z",
                "Link": "http://arxiv.org/pdf/1904.04536v1",
                "arxiv_id": "1904.04536v1"
            },
            {
                "Title": "Using Elements Of Semantic Parsing In E-Learning Environments",
                "Authors": "Andrii Striuk",
                "Abstract": "  Possibilities for using semantic parsing to estimate the correspondence of\ntext materials to teaching aims, correspondence of test task to theoretical\nmaterials and other problems arising during the distance course designing and\neducational process itself in e-learning environments.\n",
                "publicationDate": "2018-07-01T11:26:43Z",
                "Link": "http://arxiv.org/pdf/1807.00316v1",
                "arxiv_id": "1807.00316v1"
            },
            {
                "Title": "DROP: Decouple Re-Identification and Human Parsing with Task-specific\n  Features for Occluded Person Re-identification",
                "Authors": "Shuguang Dou, Xiangyang Jiang, Yuanpeng Tu, Junyao Gao, Zefan Qu, Qingsong Zhao, Cairong Zhao",
                "Abstract": "  The paper introduces the Decouple Re-identificatiOn and human Parsing (DROP)\nmethod for occluded person re-identification (ReID). Unlike mainstream\napproaches using global features for simultaneous multi-task learning of ReID\nand human parsing, or relying on semantic information for attention guidance,\nDROP argues that the inferior performance of the former is due to distinct\ngranularity requirements for ReID and human parsing features. ReID focuses on\ninstance part-level differences between pedestrian parts, while human parsing\ncenters on semantic spatial context, reflecting the internal structure of the\nhuman body. To address this, DROP decouples features for ReID and human\nparsing, proposing detail-preserving upsampling to combine varying resolution\nfeature maps. Parsing-specific features for human parsing are decoupled, and\nhuman position information is exclusively added to the human parsing branch. In\nthe ReID branch, a part-aware compactness loss is introduced to enhance\ninstance-level part differences. Experimental results highlight the efficacy of\nDROP, especially achieving a Rank-1 accuracy of 76.8% on Occluded-Duke,\nsurpassing two mainstream methods. The codebase is accessible at\nhttps://github.com/shuguang-52/DROP.\n",
                "publicationDate": "2024-01-31T17:54:43Z",
                "Link": "http://arxiv.org/pdf/2401.18032v1",
                "arxiv_id": "2401.18032v1"
            },
            {
                "Title": "Sparse Fuzzy Attention for Structured Sentiment Analysis",
                "Authors": "Letian Peng, Zuchao Li, Hai Zhao",
                "Abstract": "  Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n",
                "publicationDate": "2021-09-14T14:37:56Z",
                "Link": "http://arxiv.org/pdf/2109.06719v3",
                "arxiv_id": "2109.06719v3"
            },
            {
                "Title": "Efficient probabilistic top-down and left-corner parsing",
                "Authors": "Brian Roark, Mark Johnson",
                "Abstract": "  This paper examines efficient predictive broad-coverage parsing without\ndynamic programming. In contrast to bottom-up methods, depth-first top-down\nparsing produces partial parses that are fully connected trees spanning the\nentire left context, from which any kind of non-local dependency or partial\nsemantic interpretation can in principle be read. We contrast two predictive\nparsing approaches, top-down and left-corner parsing, and find both to be\nviable. In addition, we find that enhancement with non-local information not\nonly improves parser accuracy, but also substantially improves the search\nefficiency.\n",
                "publicationDate": "2000-08-21T19:27:18Z",
                "Link": "http://arxiv.org/pdf/cs/0008017v1",
                "arxiv_id": "0008017v1"
            },
            {
                "Title": "Renovating Parsing R-CNN for Accurate Multiple Human Parsing",
                "Authors": "Lu Yang, Qing Song, Zhihui Wang, Mengjie Hu, Chun Liu, Xueshi Xin, Wenhe Jia, Songcen Xu",
                "Abstract": "  Multiple human parsing aims to segment various human parts and associate each\npart with the corresponding instance simultaneously. This is a very challenging\ntask due to the diverse human appearance, semantic ambiguity of different body\nparts, and complex background. Through analysis of multiple human parsing task,\nwe observe that human-centric global perception and accurate instance-level\nparsing scoring are crucial for obtaining high-quality results. But the most\nstate-of-the-art methods have not paid enough attention to these issues. To\nreverse this phenomenon, we present Renovating Parsing R-CNN (RP R-CNN), which\nintroduces a global semantic enhanced feature pyramid network and a parsing\nre-scoring network into the existing high-performance pipeline. The proposed RP\nR-CNN adopts global semantic representation to enhance multi-scale features for\ngenerating human parsing maps, and regresses a confidence score to represent\nits quality. Extensive experiments show that RP R-CNN performs favorably\nagainst state-of-the-art methods on CIHP and MHP-v2 datasets. Code and models\nare available at https://github.com/soeaver/RP-R-CNN.\n",
                "publicationDate": "2020-09-20T14:55:35Z",
                "Link": "http://arxiv.org/pdf/2009.09447v1",
                "arxiv_id": "2009.09447v1"
            },
            {
                "Title": "Is Supervised Syntactic Parsing Beneficial for Language Understanding?\n  An Empirical Investigation",
                "Authors": "Goran Glava\u0161, Ivan Vuli\u0107",
                "Abstract": "  Traditional NLP has long held (supervised) syntactic parsing necessary for\nsuccessful higher-level semantic language understanding (LU). The recent advent\nof end-to-end neural models, self-supervised via language modeling (LM), and\ntheir success on a wide range of LU tasks, however, questions this belief. In\nthis work, we empirically investigate the usefulness of supervised parsing for\nsemantic LU in the context of LM-pretrained transformer networks. Relying on\nthe established fine-tuning paradigm, we first couple a pretrained transformer\nwith a biaffine parsing head, aiming to infuse explicit syntactic knowledge\nfrom Universal Dependencies treebanks into the transformer. We then fine-tune\nthe model for LU tasks and measure the effect of the intermediate parsing\ntraining (IPT) on downstream LU task performance. Results from both monolingual\nEnglish and zero-shot language transfer experiments (with intermediate\ntarget-language parsing) show that explicit formalized syntax, injected into\ntransformers through IPT, has very limited and inconsistent effect on\ndownstream LU performance. Our results, coupled with our analysis of\ntransformers' representation spaces before and after intermediate parsing, make\na significant step towards providing answers to an essential question: how\n(un)availing is supervised parsing for high-level semantic natural language\nunderstanding in the era of large neural models?\n",
                "publicationDate": "2020-08-15T21:03:36Z",
                "Link": "http://arxiv.org/pdf/2008.06788v2",
                "arxiv_id": "2008.06788v2"
            },
            {
                "Title": "Don't Parse, Insert: Multilingual Semantic Parsing with Insertion Based\n  Decoding",
                "Authors": "Qile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls, Wael Hamza",
                "Abstract": "  Semantic parsing is one of the key components of natural language\nunderstanding systems. A successful parse transforms an input utterance to an\naction that is easily understood by the system. Many algorithms have been\nproposed to solve this problem, from conventional rulebased or statistical\nslot-filling systems to shiftreduce based neural parsers. For complex parsing\ntasks, the state-of-the-art method is based on autoregressive sequence to\nsequence models to generate the parse directly. This model is slow at inference\ntime, generating parses in O(n) decoding steps (n is the length of the target\nsequence). In addition, we demonstrate that this method performs poorly in\nzero-shot cross-lingual transfer learning settings. In this paper, we propose a\nnon-autoregressive parser which is based on the insertion transformer to\novercome these two issues. Our approach 1) speeds up decoding by 3x while\noutperforming the autoregressive model and 2) significantly improves\ncross-lingual transfer in the low-resource setting by 37% compared to\nautoregressive baseline. We test our approach on three well-known monolingual\ndatasets: ATIS, SNIPS and TOP. For cross lingual semantic parsing, we use the\nMultiATIS++ and the multilingual TOP datasets.\n",
                "publicationDate": "2020-10-08T01:18:42Z",
                "Link": "http://arxiv.org/pdf/2010.03714v1",
                "arxiv_id": "2010.03714v1"
            },
            {
                "Title": "ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs",
                "Authors": "Liang Chen, Peiyi Wang, Runxin Xu, Tianyu Liu, Zhifang Sui, Baobao Chang",
                "Abstract": "  As Abstract Meaning Representation (AMR) implicitly involves compound\nsemantic annotations, we hypothesize auxiliary tasks which are semantically or\nformally related can better enhance AMR parsing. We find that 1) Semantic role\nlabeling (SRL) and dependency parsing (DP), would bring more performance gain\nthan other tasks e.g. MT and summarization in the text-to-AMR transition even\nwith much less data. 2) To make a better fit for AMR, data from auxiliary tasks\nshould be properly \"AMRized\" to PseudoAMR before training. Knowledge from\nshallow level parsing tasks can be better transferred to AMR Parsing with\nstructure transform. 3) Intermediate-task learning is a better paradigm to\nintroduce auxiliary tasks to AMR parsing, compared to multitask learning. From\nan empirical perspective, we propose a principled method to involve auxiliary\ntasks to boost AMR parsing. Extensive experiments show that our method achieves\nnew state-of-the-art performance on different benchmarks especially in\ntopology-related scores.\n",
                "publicationDate": "2022-04-19T13:15:59Z",
                "Link": "http://arxiv.org/pdf/2204.08875v2",
                "arxiv_id": "2204.08875v2"
            },
            {
                "Title": "Parsing Combinatory Categorial Grammar with Answer Set Programming:\n  Preliminary Report",
                "Authors": "Yuliya Lierler, Peter Sch\u00fcller",
                "Abstract": "  Combinatory categorial grammar (CCG) is a grammar formalism used for natural\nlanguage parsing. CCG assigns structured lexical categories to words and uses a\nsmall set of combinatory rules to combine these categories to parse a sentence.\nIn this work we propose and implement a new approach to CCG parsing that relies\non a prominent knowledge representation formalism, answer set programming (ASP)\n- a declarative programming paradigm. We formulate the task of CCG parsing as a\nplanning problem and use an ASP computational tool to compute solutions that\ncorrespond to valid parses. Compared to other approaches, there is no need to\nimplement a specific parsing algorithm using such a declarative method. Our\napproach aims at producing all semantically distinct parse trees for a given\nsentence. From this goal, normalization and efficiency issues arise, and we\ndeal with them by combining and extending existing strategies. We have\nimplemented a CCG parsing tool kit - AspCcgTk - that uses ASP as its main\ncomputational means. The C&C supertagger can be used as a preprocessor within\nAspCcgTk, which allows us to achieve wide-coverage natural language parsing.\n",
                "publicationDate": "2011-08-29T14:27:04Z",
                "Link": "http://arxiv.org/pdf/1108.5567v1",
                "arxiv_id": "1108.5567v1"
            },
            {
                "Title": "SLING: A framework for frame semantic parsing",
                "Authors": "Michael Ringgaard, Rahul Gupta, Fernando C. N. Pereira",
                "Abstract": "  We describe SLING, a framework for parsing natural language into semantic\nframes. SLING supports general transition-based, neural-network parsing with\nbidirectional LSTM input encoding and a Transition Based Recurrent Unit (TBRU)\nfor output decoding. The parsing model is trained end-to-end using only the\ntext tokens as input. The transition system has been designed to output frame\ngraphs directly without any intervening symbolic representation. The SLING\nframework includes an efficient and scalable frame store implementation as well\nas a neural network JIT compiler for fast inference during parsing. SLING is\nimplemented in C++ and it is available for download on GitHub.\n",
                "publicationDate": "2017-10-19T08:13:19Z",
                "Link": "http://arxiv.org/pdf/1710.07032v1",
                "arxiv_id": "1710.07032v1"
            },
            {
                "Title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned\n  Corpora and Theorem Proving",
                "Authors": "Cezary Kaliszyk, Josef Urban, Ji\u0159\u00ed Vysko\u010dil",
                "Abstract": "  We study methods for automated parsing of informal mathematical expressions\ninto formal ones, a main prerequisite for deep computer understanding of\ninformal mathematical texts. We propose a context-based parsing approach that\ncombines efficient statistical learning of deep parse trees with their semantic\npruning by type checking and large-theory automated theorem proving. We show\nthat the methods very significantly improve on previous results in parsing\ntheorems from the Flyspeck corpus.\n",
                "publicationDate": "2016-11-29T16:20:24Z",
                "Link": "http://arxiv.org/pdf/1611.09703v1",
                "arxiv_id": "1611.09703v1"
            },
            {
                "Title": "Type-Driven Incremental Semantic Parsing with Polymorphism",
                "Authors": "Kai Zhao, Liang Huang",
                "Abstract": "  Semantic parsing has made significant progress, but most current semantic\nparsers are extremely slow (CKY-based) and rather primitive in representation.\nWe introduce three new techniques to tackle these problems. First, we design\nthe first linear-time incremental shift-reduce-style semantic parsing algorithm\nwhich is more efficient than conventional cubic-time bottom-up semantic\nparsers. Second, our parser, being type-driven instead of syntax-driven, uses\ntype-checking to decide the direction of reduction, which eliminates the need\nfor a syntactic grammar such as CCG. Third, to fully exploit the power of\ntype-driven semantic parsing beyond simple types (such as entities and truth\nvalues), we borrow from programming language theory the concepts of subtype\npolymorphism and parametric polymorphism to enrich the type system in order to\nbetter guide the parsing. Our system learns very accurate parses in GeoQuery,\nJobs and Atis domains.\n",
                "publicationDate": "2014-11-19T21:06:15Z",
                "Link": "http://arxiv.org/pdf/1411.5379v3",
                "arxiv_id": "1411.5379v3"
            },
            {
                "Title": "A Deep Architecture for Semantic Parsing",
                "Authors": "Edward Grefenstette, Phil Blunsom, Nando de Freitas, Karl Moritz Hermann",
                "Abstract": "  Many successful approaches to semantic parsing build on top of the syntactic\nanalysis of text, and make use of distributional representations or statistical\nmodels to match parses to ontology-specific queries. This paper presents a\nnovel deep learning architecture which provides a semantic parsing system\nthrough the union of two neural models of language semantics. It allows for the\ngeneration of ontology-specific queries from natural language statements and\nquestions without the need for parsing, which makes it especially suitable to\ngrammatically malformed or syntactically atypical text, such as tweets, as well\nas permitting the development of semantic parsers for resource-poor languages.\n",
                "publicationDate": "2014-04-29T10:10:13Z",
                "Link": "http://arxiv.org/pdf/1404.7296v1",
                "arxiv_id": "1404.7296v1"
            },
            {
                "Title": "3D Face Parsing via Surface Parameterization and 2D Semantic\n  Segmentation Network",
                "Authors": "Wenyuan Sun, Ping Zhou, Yangang Wang, Zongpu Yu, Jing Jin, Guangquan Zhou",
                "Abstract": "  Face parsing assigns pixel-wise semantic labels as the face representation\nfor computers, which is the fundamental part of many advanced face\ntechnologies. Compared with 2D face parsing, 3D face parsing shows more\npotential to achieve better performance and further application, but it is\nstill challenging due to 3D mesh data computation. Recent works introduced\ndifferent methods for 3D surface segmentation, while the performance is still\nlimited. In this paper, we propose a method based on the \"3D-2D-3D\" strategy to\naccomplish 3D face parsing. The topological disk-like 2D face image containing\nspatial and textural information is transformed from the sampled 3D face data\nthrough the face parameterization algorithm, and a specific 2D network called\nCPFNet is proposed to achieve the semantic segmentation of the 2D parameterized\nface data with multi-scale technologies and feature aggregation. The 2D\nsemantic result is then inversely re-mapped to 3D face data, which finally\nachieves the 3D face parsing. Experimental results show that both CPFNet and\nthe \"3D-2D-3D\" strategy accomplish high-quality 3D face parsing and outperform\nstate-of-the-art 2D networks as well as 3D methods in both qualitative and\nquantitative comparisons.\n",
                "publicationDate": "2022-06-18T15:21:24Z",
                "Link": "http://arxiv.org/pdf/2206.09221v1",
                "arxiv_id": "2206.09221v1"
            },
            {
                "Title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language\n  Models",
                "Authors": "Dheeraj Mekala, Jason Wolfe, Subhro Roy",
                "Abstract": "  We explore the use of large language models (LLMs) for zero-shot semantic\nparsing. Semantic parsing involves mapping natural language utterances to\ntask-specific meaning representations. Language models are generally trained on\nthe publicly available text and code and cannot be expected to directly\ngeneralize to domain-specific parsing tasks in a zero-shot setting. In this\nwork, we propose ZEROTOP, a zero-shot task-oriented parsing method that\ndecomposes a semantic parsing problem into a set of abstractive and extractive\nquestion-answering (QA) problems, enabling us to leverage the ability of LLMs\nto zero-shot answer reading comprehension questions. For each utterance, we\nprompt the LLM with questions corresponding to its top-level intent and a set\nof slots and use the LLM generations to construct the target meaning\nrepresentation. We observe that current LLMs fail to detect unanswerable\nquestions; and as a result, cannot handle questions corresponding to missing\nslots. To address this problem, we fine-tune a language model on public QA\ndatasets using synthetic negative samples. Experimental results show that our\nQA-based decomposition paired with the fine-tuned LLM can correctly parse ~16%\nof utterances in the MTOP dataset without requiring any annotated data.\n",
                "publicationDate": "2022-12-21T07:06:55Z",
                "Link": "http://arxiv.org/pdf/2212.10815v1",
                "arxiv_id": "2212.10815v1"
            },
            {
                "Title": "Shift-Reduce Task-Oriented Semantic Parsing with Stack-Transformers",
                "Authors": "Daniel Fern\u00e1ndez-Gonz\u00e1lez",
                "Abstract": "  Intelligent voice assistants, such as Apple Siri and Amazon Alexa, are widely\nused nowadays. These task-oriented dialog systems require a semantic parsing\nmodule in order to process user utterances and understand the action to be\nperformed. This semantic parsing component was initially implemented by\nrule-based or statistical slot-filling approaches for processing simple\nqueries; however, the appearance of more complex utterances demanded the\napplication of shift-reduce parsers or sequence-to-sequence models. While\nshift-reduce approaches initially demonstrated to be the best option, recent\nefforts on sequence-to-sequence systems pushed them to become the\nhighest-performing method for that task. In this article, we advance the\nresearch on shift-reduce semantic parsing for task-oriented dialog. In\nparticular, we implement novel shift-reduce parsers that rely on\nStack-Transformers. These allow to adequately model transition systems on the\ncutting-edge Transformer architecture, notably boosting shift-reduce parsing\nperformance. Additionally, we adapt alternative transition systems from\nconstituency parsing to task-oriented parsing, and empirically prove that the\nin-order algorithm substantially outperforms the commonly-used top-down\nstrategy. Finally, we extensively test our approach on multiple domains from\nthe Facebook TOP benchmark, improving over existing shift-reduce parsers and\nstate-of-the-art sequence-to-sequence models in both high-resource and\nlow-resource settings.\n",
                "publicationDate": "2022-10-21T14:19:47Z",
                "Link": "http://arxiv.org/pdf/2210.11984v1",
                "arxiv_id": "2210.11984v1"
            }
        ]
    },
    {
        "topic_name": "Information Extraction",
        "summary": "default",
        "papers": [
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            }
        ]
    },
    {
        "topic_name": "Named Entity Recognition (NER)",
        "summary": "default",
        "papers": [
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa",
                "Authors": "Rahul Mehta, Vasudeva Varma",
                "Abstract": "  Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n",
                "publicationDate": "2023-05-05T06:05:45Z",
                "Link": "http://arxiv.org/pdf/2305.03300v1",
                "arxiv_id": "2305.03300v1"
            },
            {
                "Title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition",
                "Authors": "Beiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-Hua Ling, Quan Liu",
                "Abstract": "  This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n",
                "publicationDate": "2022-03-07T09:05:37Z",
                "Link": "http://arxiv.org/pdf/2203.03216v2",
                "arxiv_id": "2203.03216v2"
            },
            {
                "Title": "Named Entity Sequence Classification",
                "Authors": "Mahdi Namazifar",
                "Abstract": "  Named Entity Recognition (NER) aims at locating and classifying named\nentities in text. In some use cases of NER, including cases where detected\nnamed entities are used in creating content recommendations, it is crucial to\nhave a reliable confidence level for the detected named entities. In this work\nwe study the problem of finding confidence levels for detected named entities.\nWe refer to this problem as Named Entity Sequence Classification (NESC). We\nframe NESC as a binary classification problem and we use NER as well as\nrecurrent neural networks to find the probability of candidate named entity is\na real named entity. We apply this approach to Tweet texts and we show how we\ncould find named entities with high confidence levels from Tweets.\n",
                "publicationDate": "2017-12-06T18:33:55Z",
                "Link": "http://arxiv.org/pdf/1712.02316v1",
                "arxiv_id": "1712.02316v1"
            },
            {
                "Title": "A Unified Generative Framework for Various NER Subtasks",
                "Authors": "Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, Xipeng Qiu",
                "Abstract": "  Named Entity Recognition (NER) is the task of identifying spans that\nrepresent entities in sentences. Whether the entity spans are nested or\ndiscontinuous, the NER task can be categorized into the flat NER, nested NER,\nand discontinuous NER subtasks. These subtasks have been mainly solved by the\ntoken-level sequence labelling or span-level classification. However, these\nsolutions can hardly tackle the three kinds of NER subtasks concurrently. To\nthat end, we propose to formulate the NER subtasks as an entity span sequence\ngeneration task, which can be solved by a unified sequence-to-sequence\n(Seq2Seq) framework. Based on our unified framework, we can leverage the\npre-trained Seq2Seq model to solve all three kinds of NER subtasks without the\nspecial design of the tagging schema or ways to enumerate spans. We exploit\nthree types of entity representations to linearize entities into a sequence.\nOur proposed framework is easy-to-implement and achieves state-of-the-art\n(SoTA) or near SoTA performance on eight English NER datasets, including two\nflat NER datasets, three nested NER datasets, and three discontinuous NER\ndatasets.\n",
                "publicationDate": "2021-06-02T15:19:23Z",
                "Link": "http://arxiv.org/pdf/2106.01223v1",
                "arxiv_id": "2106.01223v1"
            },
            {
                "Title": "Named Entity Recognition via Machine Reading Comprehension: A Multi-Task\n  Learning Approach",
                "Authors": "Yibo Wang, Wenting Zhao, Yao Wan, Zhongfen Deng, Philip S. Yu",
                "Abstract": "  Named Entity Recognition (NER) aims to extract and classify entity mentions\nin the text into pre-defined types (e.g., organization or person name).\nRecently, many works have been proposed to shape the NER as a machine reading\ncomprehension problem (also termed MRC-based NER), in which entity recognition\nis achieved by answering the formulated questions related to pre-defined entity\ntypes through MRC, based on the contexts. However, these works ignore the label\ndependencies among entity types, which are critical for precisely recognizing\nnamed entities. In this paper, we propose to incorporate the label dependencies\namong entity types into a multi-task learning framework for better MRC-based\nNER. We decompose MRC-based NER into multiple tasks and use a self-attention\nmodule to capture label dependencies. Comprehensive experiments on both nested\nNER and flat NER datasets are conducted to validate the effectiveness of the\nproposed Multi-NER. Experimental results show that Multi-NER can achieve better\nperformance on all datasets.\n",
                "publicationDate": "2023-09-20T03:15:05Z",
                "Link": "http://arxiv.org/pdf/2309.11027v1",
                "arxiv_id": "2309.11027v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Recognizing Nested Entities from Flat Supervision: A New NER Subtask,\n  Feasibility and Challenges",
                "Authors": "Enwei Zhu, Yiyang Liu, Ming Jin, Jinpeng Li",
                "Abstract": "  Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.\n",
                "publicationDate": "2022-11-01T06:41:42Z",
                "Link": "http://arxiv.org/pdf/2211.00301v1",
                "arxiv_id": "2211.00301v1"
            },
            {
                "Title": "Gaussian Prior Reinforcement Learning for Nested Named Entity\n  Recognition",
                "Authors": "Yawen Yang, Xuming Hu, Fukun Ma, Shu'ang Li, Aiwei Liu, Lijie Wen, Philip S. Yu",
                "Abstract": "  Named Entity Recognition (NER) is a well and widely studied task in natural\nlanguage processing. Recently, the nested NER has attracted more attention\nsince its practicality and difficulty. Existing works for nested NER ignore the\nrecognition order and boundary position relation of nested entities. To address\nthese issues, we propose a novel seq2seq model named GPRL, which formulates the\nnested NER task as an entity triplet sequence generation process. GPRL adopts\nthe reinforcement learning method to generate entity triplets decoupling the\nentity order in gold labels and expects to learn a reasonable recognition order\nof entities via trial and error. Based on statistics of boundary distance for\nnested entities, GPRL designs a Gaussian prior to represent the boundary\ndistance distribution between nested entities and adjust the output probability\ndistribution of nested boundary tokens. Experiments on three nested NER\ndatasets demonstrate that GPRL outperforms previous nested NER models.\n",
                "publicationDate": "2023-05-12T05:55:34Z",
                "Link": "http://arxiv.org/pdf/2305.07266v1",
                "arxiv_id": "2305.07266v1"
            },
            {
                "Title": "Automated Testing and Improvement of Named Entity Recognition Systems",
                "Authors": "Boxi Yu, Yiyan Hu, Qiuyang Mang, Wenhan Hu, Pinjia He",
                "Abstract": "  Named entity recognition (NER) systems have seen rapid progress in recent\nyears due to the development of deep neural networks. These systems are widely\nused in various natural language processing applications, such as information\nextraction, question answering, and sentiment analysis. However, the complexity\nand intractability of deep neural networks can make NER systems unreliable in\ncertain circumstances, resulting in incorrect predictions. For example, NER\nsystems may misidentify female names as chemicals or fail to recognize the\nnames of minority groups, leading to user dissatisfaction. To tackle this\nproblem, we introduce TIN, a novel, widely applicable approach for\nautomatically testing and repairing various NER systems. The key idea for\nautomated testing is that the NER predictions of the same named entities under\nsimilar contexts should be identical. The core idea for automated repairing is\nthat similar named entities should have the same NER prediction under the same\ncontext. We use TIN to test two SOTA NER models and two commercial NER APIs,\ni.e., Azure NER and AWS NER. We manually verify 784 of the suspicious issues\nreported by TIN and find that 702 are erroneous issues, leading to high\nprecision (85.0%-93.4%) across four categories of NER errors: omission,\nover-labeling, incorrect category, and range error. For automated repairing,\nTIN achieves a high error reduction rate (26.8%-50.6%) over the four systems\nunder test, which successfully repairs 1,056 out of the 1,877 reported NER\nerrors.\n",
                "publicationDate": "2023-08-14T03:17:24Z",
                "Link": "http://arxiv.org/pdf/2308.07937v1",
                "arxiv_id": "2308.07937v1"
            },
            {
                "Title": "AISHELL-NER: Named Entity Recognition from Chinese Speech",
                "Authors": "Boli Chen, Guangwei Xu, Xiaobin Wang, Pengjun Xie, Meishan Zhang, Fei Huang",
                "Abstract": "  Named Entity Recognition (NER) from speech is among Spoken Language\nUnderstanding (SLU) tasks, aiming to extract semantic information from the\nspeech signal. NER from speech is usually made through a two-step pipeline that\nconsists of (1) processing the audio using an Automatic Speech Recognition\n(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works\nhave shown the capability of the End-to-End (E2E) approach for NER from English\nand French speech, which is essentially entity-aware ASR. However, due to the\nmany homophones and polyphones that exist in Chinese, NER from Chinese speech\nis effectively a more challenging task. In this paper, we introduce a new\ndataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are\nconducted to explore the performance of several state-of-the-art methods. The\nresults demonstrate that the performance could be improved by combining\nentity-aware ASR and pretrained NER tagger, which can be easily applied to the\nmodern SLU pipeline. The dataset is publicly available at\ngithub.com/Alibaba-NLP/AISHELL-NER.\n",
                "publicationDate": "2022-02-17T09:18:48Z",
                "Link": "http://arxiv.org/pdf/2202.08533v1",
                "arxiv_id": "2202.08533v1"
            },
            {
                "Title": "A Boundary Offset Prediction Network for Named Entity Recognition",
                "Authors": "Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin",
                "Abstract": "  Named entity recognition (NER) is a fundamental task in natural language\nprocessing that aims to identify and classify named entities in text. However,\nspan-based methods for NER typically assign entity types to text spans,\nresulting in an imbalanced sample space and neglecting the connections between\nnon-entity and entity spans. To address these issues, we propose a novel\napproach for NER, named the Boundary Offset Prediction Network (BOPN), which\npredicts the boundary offsets between candidate spans and their nearest entity\nspans. By leveraging the guiding semantics of boundary offsets, BOPN\nestablishes connections between non-entity and entity spans, enabling\nnon-entity spans to function as additional positive samples for entity\ndetection. Furthermore, our method integrates entity type and span\nrepresentations to generate type-aware boundary offsets instead of using entity\ntypes as detection targets. We conduct experiments on eight widely-used NER\ndatasets, and the results demonstrate that our proposed BOPN outperforms\nprevious state-of-the-art methods.\n",
                "publicationDate": "2023-10-23T05:04:07Z",
                "Link": "http://arxiv.org/pdf/2310.18349v1",
                "arxiv_id": "2310.18349v1"
            },
            {
                "Title": "SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition\n  (MultiCoNER 2)",
                "Authors": "Besnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi",
                "Abstract": "  We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual\nNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task\nfocused on methods to identify complex fine-grained named entities (like\nWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and\nmultilingual scenarios, as well as noisy settings. The task used the MultiCoNER\nV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and\nUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It\nattracted 842 submissions from 47 teams, and 34 teams submitted system papers.\nResults showed that complex entity types such as media titles and product names\nwere the most challenging. Methods fusing external knowledge into transformer\nmodels achieved the best performance, and the largest gains were on the\nCreative Work and Group classes, which are still challenging even with external\nknowledge. Some fine-grained classes proved to be more challenging than others,\nsuch as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data\nhas a significant impact on model performance, with an average drop of 10% on\nthe noisy subset. The task highlights the need for future research on improving\nNER robustness on noisy data containing complex entities.\n",
                "publicationDate": "2023-05-11T05:56:08Z",
                "Link": "http://arxiv.org/pdf/2305.06586v2",
                "arxiv_id": "2305.06586v2"
            },
            {
                "Title": "CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by\n  leveraging multilingual data",
                "Authors": "Suman Dowlagar, Radhika Mamidi",
                "Abstract": "  Identifying named entities is, in general, a practical and challenging task\nin the field of Natural Language Processing. Named Entity Recognition on the\ncode-mixed text is further challenging due to the linguistic complexity\nresulting from the nature of the mixing. This paper addresses the submission of\nteam CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER\ntask aimed to identify named entities on the code-mixed dataset. Our work\nconsists of Named Entity Recognition (NER) on the code-mixed dataset by\nleveraging the multilingual data. We achieved a weighted average F1 score of\n0.7044, i.e., 6% greater than the baseline.\n",
                "publicationDate": "2022-06-15T06:33:13Z",
                "Link": "http://arxiv.org/pdf/2206.07318v1",
                "arxiv_id": "2206.07318v1"
            },
            {
                "Title": "Computer Science Named Entity Recognition in the Open Research Knowledge\n  Graph",
                "Authors": "Jennifer D'Souza, S\u00f6ren Auer",
                "Abstract": "  Domain-specific named entity recognition (NER) on Computer Science (CS)\nscholarly articles is an information extraction task that is arguably more\nchallenging for the various annotation aims that can beset the task and has\nbeen less studied than NER in the general domain. Given that significant\nprogress has been made on NER, we believe that scholarly domain-specific NER\nwill receive increasing attention in the years to come. Currently, progress on\nCS NER -- the focus of this work -- is hampered in part by its recency and the\nlack of a standardized annotation aim for scientific entities/terms. This work\nproposes a standardized task by defining a set of seven contribution-centric\nscholarly entities for CS NER viz., research problem, solution, resource,\nlanguage, tool, method, and dataset. Following which, its main contributions\nare: combines existing CS NER resources that maintain their annotation focus on\nthe set or subset of contribution-centric scholarly entities we consider;\nfurther, noting the need for big data to train neural NER models, this work\nadditionally supplies thousands of contribution-centric entity annotations from\narticle titles and abstracts, thus releasing a cumulative large novel resource\nfor CS NER; and, finally, trains a sequence labeling CS NER model inspired\nafter state-of-the-art neural architectures from the general domain NER task.\nThroughout the work, several practical considerations are made which can be\nuseful to information technology designers of the digital libraries.\n",
                "publicationDate": "2022-03-28T08:44:43Z",
                "Link": "http://arxiv.org/pdf/2203.14579v2",
                "arxiv_id": "2203.14579v2"
            },
            {
                "Title": "A Corpus for Named Entity Recognition in Chinese Novels with\n  Multi-genres",
                "Authors": "Hanjie Zhao, Jinge Xie, Yuchen Yan, Yuxiang Jia, Yawen Ye, Hongying Zan",
                "Abstract": "  Entities like person, location, organization are important for literary text\nanalysis. The lack of annotated data hinders the progress of named entity\nrecognition (NER) in literary domain. To promote the research of literary NER,\nwe build the largest multi-genre literary NER corpus containing 263,135\nentities in 105,851 sentences from 260 online Chinese novels spanning 13\ndifferent genres. Based on the corpus, we investigate characteristics of\nentities from different genres. We propose several baseline NER models and\nconduct cross-genre and cross-domain experiments. Experimental results show\nthat genre difference significantly impact NER performance though not as much\nas domain difference like literary domain and news domain. Compared with NER in\nnews domain, literary NER still needs much improvement and the\nOut-of-Vocabulary (OOV) problem is more challenging due to the high variety of\nentities in literary works.\n",
                "publicationDate": "2023-11-27T03:08:41Z",
                "Link": "http://arxiv.org/pdf/2311.15509v1",
                "arxiv_id": "2311.15509v1"
            },
            {
                "Title": "Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named\n  Entity Recognition",
                "Authors": "Onkar Litake, Maithili Sabane, Parth Patil, Aparna Ranade, Raviraj Joshi",
                "Abstract": "  Named entity recognition (NER) is the process of recognising and classifying\nimportant information (entities) in text. Proper nouns, such as a person's\nname, an organization's name, or a location's name, are examples of entities.\nThe NER is one of the important modules in applications like human resources,\ncustomer support, search engines, content classification, and academia. In this\nwork, we consider NER for low-resource Indian languages like Hindi and Marathi.\nThe transformer-based models have been widely used for NER tasks. We consider\ndifferent variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark\nthem on publicly available Hindi and Marathi NER datasets. We provide an\nexhaustive comparison of different monolingual and multilingual\ntransformer-based models and establish simple baselines currently missing in\nthe literature. We show that the monolingual MahaRoBERTa model performs the\nbest for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for\nHindi NER. We also perform cross-language evaluation and present mixed\nobservations.\n",
                "publicationDate": "2022-03-24T07:50:41Z",
                "Link": "http://arxiv.org/pdf/2203.12907v1",
                "arxiv_id": "2203.12907v1"
            },
            {
                "Title": "Domain-Transferable Method for Named Entity Recognition Task",
                "Authors": "Vladislav Mikhailov, Tatiana Shavrina",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in the fields of natural\nlanguage processing and information extraction. NER has been widely used as a\nstandalone tool or an essential component in a variety of applications such as\nquestion answering, dialogue assistants and knowledge graphs development.\nHowever, training reliable NER models requires a large amount of labelled data\nwhich is expensive to obtain, particularly in specialized domains. This paper\ndescribes a method to learn a domain-specific NER model for an arbitrary set of\nnamed entities when domain-specific supervision is not available. We assume\nthat the supervision can be obtained with no human effort, and neural models\ncan learn from each other. The code, data and models are publicly available.\n",
                "publicationDate": "2020-11-24T15:45:52Z",
                "Link": "http://arxiv.org/pdf/2011.12170v1",
                "arxiv_id": "2011.12170v1"
            },
            {
                "Title": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire",
                "Authors": "Nicky Ringland, Xiang Dai, Ben Hachey, Sarvnaz Karimi, Cecile Paris, James R. Curran",
                "Abstract": "  Named entity recognition (NER) is widely used in natural language processing\napplications and downstream tasks. However, most NER tools target flat\nannotation from popular datasets, eschewing the semantic information available\nin nested entity mentions. We describe NNE---a fine-grained, nested named\nentity dataset over the full Wall Street Journal portion of the Penn Treebank\n(PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to\n6 layers of nesting. We hope the public release of this large dataset for\nEnglish newswire will encourage development of new techniques for nested NER.\n",
                "publicationDate": "2019-06-04T11:46:37Z",
                "Link": "http://arxiv.org/pdf/1906.01359v1",
                "arxiv_id": "1906.01359v1"
            },
            {
                "Title": "Uncertainty Estimation on Sequential Labeling via Uncertainty\n  Transmission",
                "Authors": "Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen",
                "Abstract": "  Sequential labeling is a task predicting labels for each token in a sequence,\nsuch as Named Entity Recognition (NER). NER tasks aim to extract entities and\npredict their labels given a text, which is important in information\nextraction. Although previous works have shown great progress in improving NER\nperformance, uncertainty estimation on NER (UE-NER) is still underexplored but\nessential. This work focuses on UE-NER, which aims to estimate uncertainty\nscores for the NER predictions. Previous uncertainty estimation models often\noverlook two unique characteristics of NER: the connection between entities\n(i.e., one entity embedding is learned based on the other ones) and wrong span\ncases in the entity extraction subtask. Therefore, we propose a Sequential\nLabeling Posterior Network (SLPN) to estimate uncertainty scores for the\nextracted entities, considering uncertainty transmitted from other tokens.\nMoreover, we have defined an evaluation strategy to address the specificity of\nwrong-span cases. Our SLPN has achieved significant improvements on two\ndatasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant\ndataset.\n",
                "publicationDate": "2023-11-15T06:36:29Z",
                "Link": "http://arxiv.org/pdf/2311.08726v1",
                "arxiv_id": "2311.08726v1"
            },
            {
                "Title": "A Survey on Recent Advances in Named Entity Recognition from Deep\n  Learning models",
                "Authors": "Vikas Yadav, Steven Bethard",
                "Abstract": "  Named Entity Recognition (NER) is a key component in NLP systems for question\nanswering, information retrieval, relation extraction, etc. NER systems have\nbeen studied and developed widely for decades, but accurate systems using deep\nneural networks (NN) have only been introduced in the last few years. We\npresent a comprehensive survey of deep neural network architectures for NER,\nand contrast them with previous approaches to NER based on feature engineering\nand other supervised or semi-supervised learning algorithms. Our results\nhighlight the improvements achieved by neural networks, and show how\nincorporating some of the lessons learned from past work on feature-based NER\nsystems can yield further improvements.\n",
                "publicationDate": "2019-10-25T00:45:48Z",
                "Link": "http://arxiv.org/pdf/1910.11470v1",
                "arxiv_id": "1910.11470v1"
            },
            {
                "Title": "Semi-supervised Bootstrapping approach for Named Entity Recognition",
                "Authors": "S. Thenmalar, J. Balaji, T. V. Geetha",
                "Abstract": "  The aim of Named Entity Recognition (NER) is to identify references of named\nentities in unstructured documents, and to classify them into pre-defined\nsemantic categories. NER often aids from added background knowledge in the form\nof gazetteers. However using such a collection does not deal with name variants\nand cannot resolve ambiguities associated in identifying the entities in\ncontext and associating them with predefined categories. We present a\nsemi-supervised NER approach that starts with identifying named entities with a\nsmall set of training data. Using the identified named entities, the word and\nthe context features are used to define the pattern. This pattern of each named\nentity category is used as a seed pattern to identify the named entities in the\ntest set. Pattern scoring and tuple value score enables the generation of the\nnew patterns to identify the named entity categories. We have evaluated the\nproposed system for English language with the dataset of tagged (IEER) and\nuntagged (CoNLL 2003) named entity corpus and for Tamil language with the\ndocuments from the FIRE corpus and yield an average f-measure of 75% for both\nthe languages.\n",
                "publicationDate": "2015-11-21T04:11:44Z",
                "Link": "http://arxiv.org/pdf/1511.06833v1",
                "arxiv_id": "1511.06833v1"
            },
            {
                "Title": "NER-MQMRC: Formulating Named Entity Recognition as Multi Question\n  Machine Reading Comprehension",
                "Authors": "Anubhav Shrimal, Avi Jain, Kartik Mehta, Promod Yenigalla",
                "Abstract": "  NER has been traditionally formulated as a sequence labeling task. However,\nthere has been recent trend in posing NER as a machine reading comprehension\ntask (Wang et al., 2020; Mengge et al., 2020), where entity name (or other\ninformation) is considered as a question, text as the context and entity value\nin text as answer snippet. These works consider MRC based on a single question\n(entity) at a time. We propose posing NER as a multi-question MRC task, where\nmultiple questions (one question per entity) are considered at the same time\nfor a single text. We propose a novel BERT-based multi-question MRC (NER-MQMRC)\narchitecture for this formulation. NER-MQMRC architecture considers all\nentities as input to BERT for learning token embeddings with self-attention and\nleverages BERT-based entity representation for further improving these token\nembeddings for NER task. Evaluation on three NER datasets show that our\nproposed architecture leads to average 2.5 times faster training and 2.3 times\nfaster inference as compared to NER-SQMRC framework based models by considering\nall entities together in a single pass. Further, we show that our model\nperformance does not degrade compared to single-question based MRC (NER-SQMRC)\n(Devlin et al., 2019) leading to F1 gain of +0.41%, +0.32% and +0.27% for\nAE-Pub, Ecommerce5PT and Twitter datasets respectively. We propose this\narchitecture primarily to solve large scale e-commerce attribute (or entity)\nextraction from unstructured text of a magnitude of 50k+ attributes to be\nextracted on a scalable production environment with high performance and\noptimised training and inference runtimes.\n",
                "publicationDate": "2022-05-12T06:54:03Z",
                "Link": "http://arxiv.org/pdf/2205.05904v1",
                "arxiv_id": "2205.05904v1"
            },
            {
                "Title": "Optimizing Bi-Encoder for Named Entity Recognition via Contrastive\n  Learning",
                "Authors": "Sheng Zhang, Hao Cheng, Jianfeng Gao, Hoifung Poon",
                "Abstract": "  We present a bi-encoder framework for named entity recognition (NER), which\napplies contrastive learning to map candidate text spans and entity types into\nthe same vector representation space. Prior work predominantly approaches NER\nas sequence labeling or span classification. We instead frame NER as a\nrepresentation learning problem that maximizes the similarity between the\nvector representations of an entity mention and its type. This makes it easy to\nhandle nested and flat NER alike, and can better leverage noisy\nself-supervision signals. A major challenge to this bi-encoder formulation for\nNER lies in separating non-entity spans from entity mentions. Instead of\nexplicitly labeling all non-entity spans as the same class $\\texttt{Outside}$\n($\\texttt{O}$) as in most prior methods, we introduce a novel dynamic\nthresholding loss. Experiments show that our method performs well in both\nsupervised and distantly supervised settings, for nested and flat NER alike,\nestablishing new state of the art across standard datasets in the general\ndomain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine\n(e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at\ngithub.com/microsoft/binder.\n",
                "publicationDate": "2022-08-30T23:19:04Z",
                "Link": "http://arxiv.org/pdf/2208.14565v2",
                "arxiv_id": "2208.14565v2"
            },
            {
                "Title": "Neural Modeling for Named Entities and Morphology (NEMO^2)",
                "Authors": "Dan Bareket, Reut Tsarfaty",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated\nas classification over a sequence of tokens. Morphologically-Rich Languages\n(MRLs) pose a challenge to this basic formulation, as the boundaries of Named\nEntities do not necessarily coincide with token boundaries, rather, they\nrespect morphological boundaries. To address NER in MRLs we then need to answer\ntwo fundamental questions, namely, what are the basic units to be labeled, and\nhow can these units be detected and classified in realistic settings, i.e.,\nwhere no gold morphology is available. We empirically investigate these\nquestions on a novel NER benchmark, with parallel tokenlevel and morpheme-level\nNER annotations, which we develop for Modern Hebrew, a morphologically\nrich-and-ambiguous language. Our results show that explicitly modeling\nmorphological boundaries leads to improved NER performance, and that a novel\nhybrid architecture, in which NER precedes and prunes morphological\ndecomposition, greatly outperforms the standard pipeline, where morphological\ndecomposition strictly precedes NER, setting a new performance bar for both\nHebrew NER and Hebrew morphological decomposition tasks.\n",
                "publicationDate": "2020-07-30T17:43:14Z",
                "Link": "http://arxiv.org/pdf/2007.15620v2",
                "arxiv_id": "2007.15620v2"
            },
            {
                "Title": "Large-Scale Label Interpretation Learning for Few-Shot Named Entity\n  Recognition",
                "Authors": "Jonas Golde, Felix Hamborg, Alan Akbik",
                "Abstract": "  Few-shot named entity recognition (NER) detects named entities within text\nusing only a few annotated examples. One promising line of research is to\nleverage natural language descriptions of each entity type: the common label\nPER might, for example, be verbalized as ''person entity.'' In an initial label\ninterpretation learning phase, the model learns to interpret such verbalized\ndescriptions of entity types. In a subsequent few-shot tagset extension phase,\nthis model is then given a description of a previously unseen entity type (such\nas ''music album'') and optionally a few training examples to perform few-shot\nNER for this type. In this paper, we systematically explore the impact of a\nstrong semantic prior to interpret verbalizations of new entity types by\nmassively scaling up the number and granularity of entity types used for label\ninterpretation learning. To this end, we leverage an entity linking benchmark\nto create a dataset with orders of magnitude of more distinct entity types and\ndescriptions as currently used datasets. We find that this increased signal\nyields strong results in zero- and few-shot NER in in-domain, cross-domain, and\neven cross-lingual settings. Our findings indicate significant potential for\nimproving few-shot NER through heuristical data-based optimization.\n",
                "publicationDate": "2024-03-21T08:22:44Z",
                "Link": "http://arxiv.org/pdf/2403.14222v1",
                "arxiv_id": "2403.14222v1"
            },
            {
                "Title": "Boundary Smoothing for Named Entity Recognition",
                "Authors": "Enwei Zhu, Jinpeng Li",
                "Abstract": "  Neural named entity recognition (NER) models may easily encounter the\nover-confidence issue, which degrades the performance and calibration. Inspired\nby label smoothing and driven by the ambiguity of boundary annotation in NER\nengineering, we propose boundary smoothing as a regularization technique for\nspan-based neural NER models. It re-assigns entity probabilities from annotated\nspans to the surrounding ones. Built on a simple but strong baseline, our model\nachieves results better than or competitive with previous state-of-the-art\nsystems on eight well-known NER benchmarks. Further empirical analysis suggests\nthat boundary smoothing effectively mitigates over-confidence, improves model\ncalibration, and brings flatter neural minima and more smoothed loss\nlandscapes.\n",
                "publicationDate": "2022-04-26T02:04:09Z",
                "Link": "http://arxiv.org/pdf/2204.12031v1",
                "arxiv_id": "2204.12031v1"
            },
            {
                "Title": "Neural Entity Reasoner for Global Consistency in NER",
                "Authors": "Xiaoxiao Yin, Daqi Zheng, Zhengdong Lu, Ruifang Liu",
                "Abstract": "  We propose Neural Entity Reasoner (NE-Reasoner), a framework to introduce\nglobal consistency of recognized entities into Neural Reasoner over Named\nEntity Recognition (NER) task. Given an input sentence, the NE-Reasoner layer\ncan infer over multiple entities to increase the global consistency of output\nlabels, which then be transfered into entities for the input of next layer.\nNE-Reasoner inherits and develops some features from Neural Reasoner 1) a\nsymbolic memory, allowing it to exchange entities between layers. 2) the\nspecific interaction-pooling mechanism, allowing it to connect each local word\nto multiple global entities, and 3) the deep architecture, allowing it to\nbootstrap the recognized entity set from coarse to fine. Like human beings,\nNE-Reasoner is able to accommodate ambiguous words and Name Entities that\nrarely or never met before. Despite the symbolic information the model\nintroduced, NE-Reasoner can still be trained effectively in an end-to-end\nmanner via parameter sharing strategy. NE-Reasoner can outperform conventional\nNER models in most cases on both English and Chinese NER datasets. For example,\nit achieves state-of-art on CoNLL-2003 English NER dataset.\n",
                "publicationDate": "2018-09-30T09:28:57Z",
                "Link": "http://arxiv.org/pdf/1810.00347v1",
                "arxiv_id": "1810.00347v1"
            },
            {
                "Title": "Named Entity Recognition in the Legal Domain using a Pointer Generator\n  Network",
                "Authors": "Stavroula Skylaki, Ali Oskooei, Omar Bari, Nadja Herger, Zac Kriegman",
                "Abstract": "  Named Entity Recognition (NER) is the task of identifying and classifying\nnamed entities in unstructured text. In the legal domain, named entities of\ninterest may include the case parties, judges, names of courts, case numbers,\nreferences to laws etc. We study the problem of legal NER with noisy text\nextracted from PDF files of filed court cases from US courts. The \"gold\nstandard\" training data for NER systems provide annotation for each token of\nthe text with the corresponding entity or non-entity label. We work with only\npartially complete training data, which differ from the gold standard NER data\nin that the exact location of the entities in the text is unknown and the\nentities may contain typos and/or OCR mistakes. To overcome the challenges of\nour noisy training data, e.g. text extraction errors and/or typos and unknown\nlabel indices, we formulate the NER task as a text-to-text sequence generation\ntask and train a pointer generator network to generate the entities in the\ndocument rather than label them. We show that the pointer generator can be\neffective for NER in the absence of gold standard data and outperforms the\ncommon NER neural network architectures in long legal documents.\n",
                "publicationDate": "2020-12-17T21:10:34Z",
                "Link": "http://arxiv.org/pdf/2012.09936v1",
                "arxiv_id": "2012.09936v1"
            },
            {
                "Title": "S2F-NER: Exploring Sequence-to-Forest Generation for Complex Entity\n  Recognition",
                "Authors": "Yongxiu Xu, Heyan Huang, Yue Hu",
                "Abstract": "  Named Entity Recognition (NER) remains challenging due to the complex\nentities, like nested, overlapping, and discontinuous entities. Existing\napproaches, such as sequence-to-sequence (Seq2Seq) generation and span-based\nclassification, have shown impressive performance on various NER subtasks, but\nthey are difficult to scale to datasets with longer input text because of\neither exposure bias issue or inefficient computation. In this paper, we\npropose a novel Sequence-to-Forest generation paradigm, S2F-NER, which can\ndirectly extract entities in sentence via a Forest decoder that decode multiple\nentities in parallel rather than sequentially. Specifically, our model generate\neach path of each tree in forest autoregressively, where the maximum depth of\neach tree is three (which is the shortest feasible length for complex NER and\nis far smaller than the decoding length of Seq2Seq). Based on this novel\nparadigm, our model can elegantly mitigates the exposure bias problem and keep\nthe simplicity of Seq2Seq. Experimental results show that our model\nsignificantly outperforms the baselines on three discontinuous NER datasets and\non two nested NER datasets, especially for discontinuous entity recognition.\n",
                "publicationDate": "2023-10-29T09:09:10Z",
                "Link": "http://arxiv.org/pdf/2310.18944v1",
                "arxiv_id": "2310.18944v1"
            },
            {
                "Title": "Attack Named Entity Recognition by Entity Boundary Interference",
                "Authors": "Yifei Yang, Hongqiu Wu, Hai Zhao",
                "Abstract": "  Named Entity Recognition (NER) is a cornerstone NLP task while its robustness\nhas been given little attention. This paper rethinks the principles of NER\nattacks derived from sentence classification, as they can easily violate the\nlabel consistency between the original and adversarial NER examples. This is\ndue to the fine-grained nature of NER, as even minor word changes in the\nsentence can result in the emergence or mutation of any entities, resulting in\ninvalid adversarial examples. To this end, we propose a novel one-word\nmodification NER attack based on a key insight, NER models are always\nvulnerable to the boundary position of an entity to make their decision. We\nthus strategically insert a new boundary into the sentence and trigger the\nEntity Boundary Interference that the victim model makes the wrong prediction\neither on this boundary word or on other words in the sentence. We call this\nattack Virtual Boundary Attack (ViBA), which is shown to be remarkably\neffective when attacking both English and Chinese models with a 70%-90% attack\nsuccess rate on state-of-the-art language models (e.g. RoBERTa, DeBERTa) and\nalso significantly faster than previous methods.\n",
                "publicationDate": "2023-05-09T08:21:11Z",
                "Link": "http://arxiv.org/pdf/2305.05253v1",
                "arxiv_id": "2305.05253v1"
            },
            {
                "Title": "DMNER: Biomedical Entity Recognition by Detection and Matching",
                "Authors": "Junyi Bian, Rongze Jiang, Weiqi Zhai, Tianyang Huang, Hong Zhou, Shanfeng Zhu",
                "Abstract": "  Biomedical named entity recognition (BNER) serves as the foundation for\nnumerous biomedical text mining tasks. Unlike general NER, BNER require a\ncomprehensive grasp of the domain, and incorporating external knowledge beyond\ntraining data poses a significant challenge. In this study, we propose a novel\nBNER framework called DMNER. By leveraging existing entity representation\nmodels SAPBERT, we tackle BNER as a two-step process: entity boundary detection\nand biomedical entity matching. DMNER exhibits applicability across multiple\nNER scenarios: 1) In supervised NER, we observe that DMNER effectively\nrectifies the output of baseline NER models, thereby further enhancing\nperformance. 2) In distantly supervised NER, combining MRC and AutoNER as span\nboundary detectors enables DMNER to achieve satisfactory results. 3) For\ntraining NER by merging multiple datasets, we adopt a framework similar to\nDS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the\ntraining. Through extensive experiments conducted on 10 benchmark datasets, we\ndemonstrate the versatility and effectiveness of DMNER.\n",
                "publicationDate": "2023-06-27T18:32:07Z",
                "Link": "http://arxiv.org/pdf/2306.15736v2",
                "arxiv_id": "2306.15736v2"
            },
            {
                "Title": "E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition",
                "Authors": "Zhen Zhang, Mengting Hu, Shiwan Zhao, Minlie Huang, Haotian Wang, Lemao Liu, Zhirui Zhang, Zhe Liu, Bingzhe Wu",
                "Abstract": "  Most named entity recognition (NER) systems focus on improving model\nperformance, ignoring the need to quantify model uncertainty, which is critical\nto the reliability of NER systems in open environments. Evidential deep\nlearning (EDL) has recently been proposed as a promising solution to explicitly\nmodel predictive uncertainty for classification tasks. However, directly\napplying EDL to NER applications faces two challenges, i.e., the problems of\nsparse entities and OOV/OOD entities in NER tasks. To address these challenges,\nwe propose a trustworthy NER framework named E-NER by introducing two\nuncertainty-guided loss terms to the conventional EDL, along with a series of\nuncertainty-guided training strategies. Experiments show that E-NER can be\napplied to multiple NER paradigms to obtain accurate uncertainty estimation.\nFurthermore, compared to state-of-the-art baselines, the proposed method\nachieves a better OOV/OOD detection performance and better generalization\nability on OOV entities.\n",
                "publicationDate": "2023-05-29T02:36:16Z",
                "Link": "http://arxiv.org/pdf/2305.17854v1",
                "arxiv_id": "2305.17854v1"
            },
            {
                "Title": "Multi-Task Learning with Contextualized Word Representations for\n  Extented Named Entity Recognition",
                "Authors": "Thai-Hoang Pham, Khai Mai, Nguyen Minh Trung, Nguyen Tuan Duc, Danushka Bolegala, Ryohei Sasano, Satoshi Sekine",
                "Abstract": "  Fine-Grained Named Entity Recognition (FG-NER) is critical for many NLP\napplications. While classical named entity recognition (NER) has attracted a\nsubstantial amount of research, FG-NER is still an open research domain. The\ncurrent state-of-the-art (SOTA) model for FG-NER relies heavily on manual\nefforts for building a dictionary and designing hand-crafted features. The\nend-to-end framework which achieved the SOTA result for NER did not get the\ncompetitive result compared to SOTA model for FG-NER. In this paper, we\ninvestigate how effective multi-task learning approaches are in an end-to-end\nframework for FG-NER in different aspects. Our experiments show that using\nmulti-task learning approaches with contextualized word representation can help\nan end-to-end neural network model achieve SOTA results without using any\nadditional manual effort for creating data and designing features.\n",
                "publicationDate": "2019-02-26T18:53:22Z",
                "Link": "http://arxiv.org/pdf/1902.10118v1",
                "arxiv_id": "1902.10118v1"
            },
            {
                "Title": "Multicultural Name Recognition For Previously Unseen Names",
                "Authors": "Alexandra Loessberg-Zahl",
                "Abstract": "  State of the art Named Entity Recognition (NER) models have achieved an\nimpressive ability to extract common phrases from text that belong to labels\nsuch as location, organization, time, and person. However, typical NER systems\nthat rely on having seen a specific entity in their training data in order to\nlabel an entity perform poorly on rare or unseen entities ta in order to label\nan entity perform poorly on rare or unseen entities (Derczynski et al., 2017).\nThis paper attempts to improve recognition of person names, a diverse category\nthat can grow any time someone is born or changes their name. In order for\ndownstream tasks to not exhibit bias based on cultural background, a model\nshould perform well on names from a variety of backgrounds. In this paper I\nexperiment with the training data and input structure of an English Bi-LSTM\nname recognition model. I look at names from 103 countries to compare how well\nthe model performs on names from different cultures, specifically in the\ncontext of a downstream task where extracted names will be matched to\ninformation on file. I find that a model with combined character and word input\noutperforms word-only models and may improve on accuracy compared to classical\nNER models that are not geared toward identifying unseen entity values.\n",
                "publicationDate": "2024-01-23T17:58:38Z",
                "Link": "http://arxiv.org/pdf/2401.12941v1",
                "arxiv_id": "2401.12941v1"
            },
            {
                "Title": "Robust Few-Shot Named Entity Recognition with Boundary Discrimination\n  and Correlation Purification",
                "Authors": "Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, Zhendong Niu",
                "Abstract": "  Few-shot named entity recognition (NER) aims to recognize novel named\nentities in low-resource domains utilizing existing knowledge. However, the\npresent few-shot NER models assume that the labeled data are all clean without\nnoise or outliers, and there are few works focusing on the robustness of the\ncross-domain transfer learning ability to textual adversarial attacks in\nFew-shot NER. In this work, we comprehensively explore and assess the\nrobustness of few-shot NER models under textual adversarial attack scenario,\nand found the vulnerability of existing few-shot NER models. Furthermore, we\npropose a robust two-stage few-shot NER method with Boundary Discrimination and\nCorrelation Purification (BDCP). Specifically, in the span detection stage, the\nentity boundary discriminative module is introduced to provide a highly\ndistinguishing boundary representation space to detect entity spans. In the\nentity typing stage, the correlations between entities and contexts are\npurified by minimizing the interference information and facilitating\ncorrelation generalization to alleviate the perturbations caused by textual\nadversarial attacks. In addition, we construct adversarial examples for\nfew-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive\nevaluations on those two groups of few-shot NER datasets containing adversarial\nexamples demonstrate the robustness and superiority of the proposed method.\n",
                "publicationDate": "2023-12-13T08:17:00Z",
                "Link": "http://arxiv.org/pdf/2312.07961v1",
                "arxiv_id": "2312.07961v1"
            },
            {
                "Title": "Multi-task Transformer with Relation-attention and Type-attention for\n  Named Entity Recognition",
                "Authors": "Ying Mo, Hongyin Tang, Jiahao Liu, Qifan Wang, Zenglin Xu, Jingang Wang, Wei Wu, Zhoujun Li",
                "Abstract": "  Named entity recognition (NER) is an important research problem in natural\nlanguage processing. There are three types of NER tasks, including flat, nested\nand discontinuous entity recognition. Most previous sequential labeling models\nare task-specific, while recent years have witnessed the rising of generative\nmodels due to the advantage of unifying all NER tasks into the seq2seq model\nframework. Although achieving promising performance, our pilot studies\ndemonstrate that existing generative models are ineffective at detecting entity\nboundaries and estimating entity types. This paper proposes a multi-task\nTransformer, which incorporates an entity boundary detection task into the\nnamed entity recognition task. More concretely, we achieve entity boundary\ndetection by classifying the relations between tokens within the sentence. To\nimprove the accuracy of entity-type mapping during decoding, we adopt an\nexternal knowledge base to calculate the prior entity-type distributions and\nthen incorporate the information into the model via the self and\ncross-attention mechanisms. We perform experiments on an extensive set of NER\nbenchmarks, including two flat, three nested, and three discontinuous NER\ndatasets. Experimental results show that our approach considerably improves the\ngenerative NER model's performance.\n",
                "publicationDate": "2023-03-20T05:11:22Z",
                "Link": "http://arxiv.org/pdf/2303.10870v1",
                "arxiv_id": "2303.10870v1"
            },
            {
                "Title": "MANER: Mask Augmented Named Entity Recognition for Extreme Low-Resource\n  Languages",
                "Authors": "Shashank Sonkar, Zichao Wang, Richard G. Baraniuk",
                "Abstract": "  This paper investigates the problem of Named Entity Recognition (NER) for\nextreme low-resource languages with only a few hundred tagged data samples. NER\nis a fundamental task in Natural Language Processing (NLP). A critical driver\naccelerating NER systems' progress is the existence of large-scale language\ncorpora that enable NER systems to achieve outstanding performance in languages\nsuch as English and French with abundant training data. However, NER for\nlow-resource languages remains relatively unexplored. In this paper, we\nintroduce Mask Augmented Named Entity Recognition (MANER), a new methodology\nthat leverages the distributional hypothesis of pre-trained masked language\nmodels (MLMs) for NER. The <mask> token in pre-trained MLMs encodes valuable\nsemantic contextual information. MANER re-purposes the <mask> token for NER\nprediction. Specifically, we prepend the <mask> token to every word in a\nsentence for which we would like to predict the named entity tag. During\ntraining, we jointly fine-tune the MLM and a new NER prediction head attached\nto each <mask> token. We demonstrate that MANER is well-suited for NER in\nlow-resource languages; our experiments show that for 100 languages with as few\nas 100 training examples, it improves on state-of-the-art methods by up to 48%\nand by 12% on average on F1 score. We also perform detailed analyses and\nablation studies to understand the scenarios that are best-suited to MANER.\n",
                "publicationDate": "2022-12-19T18:49:50Z",
                "Link": "http://arxiv.org/pdf/2212.09723v1",
                "arxiv_id": "2212.09723v1"
            },
            {
                "Title": "Exploiting Lists of Names for Named Entity Identification of Financial\n  Institutions from Unstructured Documents",
                "Authors": "Zheng Xu, Douglas Burdick, Louiqa Raschid",
                "Abstract": "  There is a wealth of information about financial systems that is embedded in\ndocument collections. In this paper, we focus on a specialized text extraction\ntask for this domain. The objective is to extract mentions of names of\nfinancial institutions, or FI names, from financial prospectus documents, and\nto identify the corresponding real world entities, e.g., by matching against a\ncorpus of such entities. The tasks are Named Entity Recognition (NER) and\nEntity Resolution (ER); both are well studied in the literature. Our\ncontribution is to develop a rule-based approach that will exploit lists of FI\nnames for both tasks; our solution is labeled Dict-based NER and Rank-based ER.\nSince the FI names are typically represented by a root, and a suffix that\nmodifies the root, we use these lists of FI names to create specialized root\nand suffix dictionaries. To evaluate the effectiveness of our specialized\nsolution for extracting FI names, we compare Dict-based NER with a general\npurpose rule-based NER solution, ORG NER. Our evaluation highlights the\nbenefits and limitations of specialized versus general purpose approaches, and\npresents additional suggestions for tuning and customization for FI name\nextraction. To our knowledge, our proposed solutions, Dict-based NER and\nRank-based ER, and the root and suffix dictionaries, are the first attempt to\nexploit specialized knowledge, i.e., lists of FI names, for rule-based NER and\nER.\n",
                "publicationDate": "2016-02-14T07:31:28Z",
                "Link": "http://arxiv.org/pdf/1602.04427v2",
                "arxiv_id": "1602.04427v2"
            },
            {
                "Title": "A Unified MRC Framework for Named Entity Recognition",
                "Authors": "Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li",
                "Abstract": "  The task of named entity recognition (NER) is normally divided into nested\nNER and flat NER depending on whether named entities are nested or not. Models\nare usually separately developed for the two tasks, since sequence labeling\nmodels, the most widely used backbone for flat NER, are only able to assign a\nsingle label to a particular token, which is unsuitable for nested NER where a\ntoken may be assigned several labels.\n  In this paper, we propose a unified framework that is capable of handling\nboth flat and nested NER tasks. Instead of treating the task of NER as a\nsequence labeling problem, we propose to formulate it as a machine reading\ncomprehension (MRC) task. For example, extracting entities with the\n\\textsc{per} label is formalized as extracting answer spans to the question\n\"{\\it which person is mentioned in the text?}\". This formulation naturally\ntackles the entity overlapping issue in nested NER: the extraction of two\noverlapping entities for different categories requires answering two\nindependent questions. Additionally, since the query encodes informative prior\nknowledge, this strategy facilitates the process of entity extraction, leading\nto better performances for not only nested NER, but flat NER.\n  We conduct experiments on both {\\em nested} and {\\em flat} NER datasets.\nExperimental results demonstrate the effectiveness of the proposed formulation.\nWe are able to achieve vast amount of performance boost over current SOTA\nmodels on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively\non ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets,\ni.e.,+0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English\nOntoNotes 5.0, Chinese MSRA, Chinese OntoNotes 4.0.\n",
                "publicationDate": "2019-10-25T01:06:07Z",
                "Link": "http://arxiv.org/pdf/1910.11476v7",
                "arxiv_id": "1910.11476v7"
            },
            {
                "Title": "Combining neural and knowledge-based approaches to Named Entity\n  Recognition in Polish",
                "Authors": "S\u0142awomir Dadas",
                "Abstract": "  Named entity recognition (NER) is one of the tasks in natural language\nprocessing that can greatly benefit from the use of external knowledge sources.\nWe propose a named entity recognition framework composed of knowledge-based\nfeature extractors and a deep learning model including contextual word\nembeddings, long short-term memory (LSTM) layers and conditional random fields\n(CRF) inference layer. We use an entity linking module to integrate our system\nwith Wikipedia. The combination of effective neural architecture and external\nresources allows us to obtain state-of-the-art results on recognition of Polish\nproper names. We evaluate our model on data from PolEval 2018 NER challenge on\nwhich it outperforms other methods, reducing the error rate by 22.4% compared\nto the winning solution. Our work shows that combining neural NER model and\nentity linking model with a knowledge base is more effective in recognizing\nnamed entities than using NER model alone.\n",
                "publicationDate": "2018-11-26T14:52:06Z",
                "Link": "http://arxiv.org/pdf/1811.10418v1",
                "arxiv_id": "1811.10418v1"
            },
            {
                "Title": "Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity\n  Recognition",
                "Authors": "Bin Ji, Shasha Li, Jie Yu, Jun Ma, Huijun Liu",
                "Abstract": "  For Named Entity Recognition (NER), sequence labeling-based and span-based\nparadigms are quite different. Previous research has demonstrated that the two\nparadigms have clear complementary advantages, but few models have attempted to\nleverage these advantages in a single NER model as far as we know. In our\nprevious work, we proposed a paradigm known as Bundling Learning (BL) to\naddress the above problem. The BL paradigm bundles the two NER paradigms,\nenabling NER models to jointly tune their parameters by weighted summing each\nparadigm's training loss. However, three critical issues remain unresolved:\nWhen does BL work? Why does BL work? Can BL enhance the existing\nstate-of-the-art (SOTA) NER models? To address the first two issues, we\nimplement three NER models, involving a sequence labeling-based model--SeqNER,\na span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER\ntogether. We draw two conclusions regarding the two issues based on the\nexperimental results on eleven NER datasets from five domains. We then apply BL\nto five existing SOTA NER models to investigate the third issue, consisting of\nthree sequence labeling-based models and two span-based models. Experimental\nresults indicate that BL consistently enhances their performance, suggesting\nthat it is possible to construct a new SOTA NER system by incorporating BL into\nthe current SOTA system. Moreover, we find that BL reduces both entity boundary\nand type prediction errors. In addition, we compare two commonly used labeling\ntagging methods as well as three types of span semantic representations.\n",
                "publicationDate": "2022-07-07T13:52:06Z",
                "Link": "http://arxiv.org/pdf/2207.03300v2",
                "arxiv_id": "2207.03300v2"
            },
            {
                "Title": "Less than One-shot: Named Entity Recognition via Extremely Weak\n  Supervision",
                "Authors": "Letian Peng, Zihan Wang, Jingbo Shang",
                "Abstract": "  We study the named entity recognition (NER) problem under the extremely weak\nsupervision (XWS) setting, where only one example entity per type is given in a\ncontext-free way. While one can see that XWS is lighter than one-shot in terms\nof the amount of supervision, we propose a novel method X-NER that can\noutperform the state-of-the-art one-shot NER methods. We first mine entity\nspans that are similar to the example entities from an unlabelled training\ncorpus. Instead of utilizing entity span representations from language models,\nwe find it more effective to compare the context distributions before and after\nthe span is replaced by the entity example. We then leverage the top-ranked\nspans as pseudo-labels to train an NER tagger. Extensive experiments and\nanalyses on 4 NER datasets show the superior end-to-end NER performance of\nX-NER, outperforming the state-of-the-art few-shot methods with 1-shot\nsupervision and ChatGPT annotations significantly. Finally, our X-NER possesses\nseveral notable properties, such as inheriting the cross-lingual abilities of\nthe underlying language models.\n",
                "publicationDate": "2023-11-06T04:20:42Z",
                "Link": "http://arxiv.org/pdf/2311.02861v1",
                "arxiv_id": "2311.02861v1"
            },
            {
                "Title": "Linguistically Informed Relation Extraction and Neural Architectures for\n  Nested Named Entity Recognition in BioNLP-OST 2019",
                "Authors": "Usama Yaseen, Pankaj Gupta, Hinrich Sch\u00fctze",
                "Abstract": "  Named Entity Recognition (NER) and Relation Extraction (RE) are essential\ntools in distilling knowledge from biomedical literature. This paper presents\nour findings from participating in BioNLP Shared Tasks 2019. We addressed Named\nEntity Recognition including nested entities extraction, Entity Normalization\nand Relation Extraction. Our proposed approach of Named Entities can be\ngeneralized to different languages and we have shown it's effectiveness for\nEnglish and Spanish text. We investigated linguistic features, hybrid loss\nincluding ranking and Conditional Random Fields (CRF), multi-task objective and\ntoken-level ensembling strategy to improve NER. We employed dictionary based\nfuzzy and semantic search to perform Entity Normalization. Finally, our RE\nsystem employed Support Vector Machine (SVM) with linguistic features.\n  Our NER submission (team:MIC-CIS) ranked first in BB-2019 norm+NER task with\nstandard error rate (SER) of 0.7159 and showed competitive performance on\nPharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the\nSeeDev-binary Relation Extraction Task with F1-score of 0.3738.\n",
                "publicationDate": "2019-10-08T13:33:48Z",
                "Link": "http://arxiv.org/pdf/1910.03385v1",
                "arxiv_id": "1910.03385v1"
            },
            {
                "Title": "MProto: Multi-Prototype Network with Denoised Optimal Transport for\n  Distantly Supervised Named Entity Recognition",
                "Authors": "Shuhui Wu, Yongliang Shen, Zeqi Tan, Wenqi Ren, Jietian Guo, Shiliang Pu, Weiming Lu",
                "Abstract": "  Distantly supervised named entity recognition (DS-NER) aims to locate entity\nmentions and classify their types with only knowledge bases or gazetteers and\nunlabeled corpus. However, distant annotations are noisy and degrade the\nperformance of NER models. In this paper, we propose a noise-robust prototype\nnetwork named MProto for the DS-NER task. Different from previous\nprototype-based NER methods, MProto represents each entity type with multiple\nprototypes to characterize the intra-class variance among entity\nrepresentations. To optimize the classifier, each token should be assigned an\nappropriate ground-truth prototype and we consider such token-prototype\nassignment as an optimal transport (OT) problem. Furthermore, to mitigate the\nnoise from incomplete labeling, we propose a novel denoised optimal transport\n(DOT) algorithm. Specifically, we utilize the assignment result between Other\nclass tokens and all prototypes to distinguish unlabeled entity tokens from\ntrue negatives. Experiments on several DS-NER benchmarks demonstrate that our\nMProto achieves state-of-the-art performance. The source code is now available\non Github.\n",
                "publicationDate": "2023-10-12T13:02:34Z",
                "Link": "http://arxiv.org/pdf/2310.08298v1",
                "arxiv_id": "2310.08298v1"
            },
            {
                "Title": "WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task",
                "Authors": "Mustafa Jarrar, Muhammad Abdul-Mageed, Mohammed Khalilia, Bashar Talafha, AbdelRahim Elmadany, Nagham Hamad, Alaa' Omar",
                "Abstract": "  We present WojoodNER-2023, the first Arabic Named Entity Recognition (NER)\nShared Task. The primary focus of WojoodNER-2023 is on Arabic NER, offering\nnovel NER datasets (i.e., Wojood) and the definition of subtasks designed to\nfacilitate meaningful comparisons between different NER approaches.\nWojoodNER-2023 encompassed two Subtasks: FlatNER and NestedNER. A total of 45\nunique teams registered for this shared task, with 11 of them actively\nparticipating in the test phase. Specifically, 11 teams participated in\nFlatNER, while $8$ teams tackled NestedNER. The winning teams achieved F1\nscores of 91.96 and 93.73 in FlatNER and NestedNER, respectively.\n",
                "publicationDate": "2023-10-24T19:50:07Z",
                "Link": "http://arxiv.org/pdf/2310.16153v1",
                "arxiv_id": "2310.16153v1"
            },
            {
                "Title": "Comparative Analysis of Named Entity Recognition in the Dungeons and\n  Dragons Domain",
                "Authors": "Gayashan Weerasundara, Nisansa de Silva",
                "Abstract": "  Many NLP tasks, although well-resolved for general English, face challenges\nin specific domains like fantasy literature. This is evident in Named Entity\nRecognition (NER), which detects and categorizes entities in text. We analyzed\n10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess\ndomain-specific performance. Using open-source Large Language Models, we\nannotated named entities in these books and evaluated each model's precision.\nOur findings indicate that, without modifications, Flair, Trankit, and Spacy\noutperform others in identifying named entities in the D&D context.\n",
                "publicationDate": "2023-09-29T12:09:36Z",
                "Link": "http://arxiv.org/pdf/2309.17171v1",
                "arxiv_id": "2309.17171v1"
            },
            {
                "Title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity\n  Recognition",
                "Authors": "Hang Yan, Yu Sun, Xiaonan Li, Xipeng Qiu",
                "Abstract": "  Named entity recognition (NER) is the task to detect and classify the entity\nspans in the text. When entity spans overlap between each other, this problem\nis named as nested NER. Span-based methods have been widely used to tackle the\nnested NER. Most of these methods will get a score $n \\times n$ matrix, where\n$n$ means the length of sentence, and each entry corresponds to a span.\nHowever, previous work ignores spatial relations in the score matrix. In this\npaper, we propose using Convolutional Neural Network (CNN) to model these\nspatial relations in the score matrix. Despite being simple, experiments in\nthree commonly used nested NER datasets show that our model surpasses several\nrecently proposed methods with the same pre-trained encoders. Further analysis\nshows that using CNN can help the model find more nested entities. Besides, we\nfound that different papers used different sentence tokenizations for the three\nnested NER datasets, which will influence the comparison. Thus, we release a\npre-processing script to facilitate future comparison.\n",
                "publicationDate": "2022-08-09T04:33:46Z",
                "Link": "http://arxiv.org/pdf/2208.04534v3",
                "arxiv_id": "2208.04534v3"
            },
            {
                "Title": "Czech Text Processing with Contextual Embeddings: POS Tagging,\n  Lemmatization, Parsing and NER",
                "Authors": "Milan Straka, Jana Strakov\u00e1, Jan Haji\u010d",
                "Abstract": "  Contextualized embeddings, which capture appropriate word meaning depending\non context, have recently been proposed. We evaluate two meth ods for\nprecomputing such embeddings, BERT and Flair, on four Czech text processing\ntasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and\nnamed entity recognition (NER). The first three tasks, POS tagging,\nlemmatization and dependency parsing, are evaluated on two corpora: the Prague\nDependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity\nrecognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We\nreport state-of-the-art results for the above mentioned tasks and corpora.\n",
                "publicationDate": "2019-09-08T21:00:05Z",
                "Link": "http://arxiv.org/pdf/1909.03544v2",
                "arxiv_id": "1909.03544v2"
            }
        ]
    },
    {
        "topic_name": "Text Alignment",
        "summary": "default",
        "papers": [
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            }
        ]
    },
    {
        "topic_name": "Document Classification",
        "summary": "default",
        "papers": [
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Ensemble of Anchor-Free Models for Robust Bangla Document Layout\n  Segmentation",
                "Authors": "U Mong Sain Chak, Md. Asib Rahman",
                "Abstract": "  In this research paper, we introduce a novel approach designed for the\npurpose of segmenting the layout of Bangla documents. Our methodology involves\nthe utilization of a sophisticated ensemble of YOLOv8 models, which were\ntrained for the DL Sprint 2.0 - BUET CSE Fest 2023 Competition focused on\nBangla document layout segmentation. Our primary emphasis lies in enhancing\nvarious aspects of the task, including techniques such as image augmentation,\nmodel architecture, and the incorporation of model ensembles. We deliberately\nreduce the quality of a subset of document images to enhance the resilience of\nmodel training, thereby resulting in an improvement in our cross-validation\nscore. By employing Bayesian optimization, we determine the optimal confidence\nand Intersection over Union (IoU) thresholds for our model ensemble. Through\nour approach, we successfully demonstrate the effectiveness of anchor-free\nmodels in achieving robust layout segmentation in Bangla documents.\n",
                "publicationDate": "2023-08-28T08:24:25Z",
                "Link": "http://arxiv.org/pdf/2308.14397v2",
                "arxiv_id": "2308.14397v2"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            }
        ]
    },
    {
        "topic_name": "Cross-lingual Information Retrieval",
        "summary": "default",
        "papers": [
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Ranking the locations and predicting future crime occurrence by\n  retrieving news from different Bangla online newspapers",
                "Authors": "Jumman Hossain, Rajib Chandra Das, Md. Ruhul Amin, Md. Saiful Islam",
                "Abstract": "  There have thousands of crimes are happening daily all around. But people\nkeep statistics only few of them, therefore crime rates are increasing day by\nday. The reason behind can be less concern or less statistics of previous\ncrimes. It is much more important to observe the previous crime statistics for\ngeneral people to make their outing decision and police for catching the\ncriminals are taking steps to restrain the crimes and tourists to make their\ntravelling decision. National institute of justice releases crime survey data\nfor the country, but does not offer crime statistics up to Union or Thana\nlevel. Considering all of these cases we have come up with an approach which\ncan give an approximation to people about the safety of a specific location\nwith crime ranking of different areas locating the crimes on a map including a\nfuture crime occurrence prediction mechanism. Our approach relies on different\nonline Bangla newspapers for crawling the crime data, stemming and keyword\nextraction, location finding algorithm, cosine similarity, naive Bayes\nclassifier, and a custom crime prediction model\n",
                "publicationDate": "2023-05-18T04:19:26Z",
                "Link": "http://arxiv.org/pdf/2305.10698v1",
                "arxiv_id": "2305.10698v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection",
                "Authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor Ryen, Arman Hossain, Faisal Bin Ashraf",
                "Abstract": "  Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.\n",
                "publicationDate": "2021-02-04T08:35:18Z",
                "Link": "http://arxiv.org/pdf/2102.02478v1",
                "arxiv_id": "2102.02478v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters",
                "Authors": "Qun Liu, Edward Collier, Supratik Mukhopadhyay",
                "Abstract": "  Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets.\n",
                "publicationDate": "2019-08-11T08:01:58Z",
                "Link": "http://arxiv.org/pdf/1908.08987v1",
                "arxiv_id": "1908.08987v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding",
                "Authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Avirup Bhattacharyya, Umapada Pal",
                "Abstract": "  Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach.\n",
                "publicationDate": "2017-08-18T07:47:05Z",
                "Link": "http://arxiv.org/pdf/1708.05529v6",
                "arxiv_id": "1708.05529v6"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Neural Cross-Lingual Entity Linking",
                "Authors": "Avirup Sil, Gourab Kundu, Radu Florian, Wael Hamza",
                "Abstract": "  A major challenge in Entity Linking (EL) is making effective use of\ncontextual information to disambiguate mentions to Wikipedia that might refer\nto different entities in different contexts. The problem exacerbates with\ncross-lingual EL which involves linking mentions written in non-English\ndocuments to entries in the English Wikipedia: to compare textual clues across\nlanguages we need to compute similarity between textual fragments across\nlanguages. In this paper, we propose a neural EL model that trains fine-grained\nsimilarities and dissimilarities between the query and candidate document from\nmultiple perspectives, combined with convolution and tensor networks. Further,\nwe show that this English-trained system can be applied, in zero-shot learning,\nto other languages by making surprisingly effective use of multi-lingual\nembeddings. The proposed system has strong empirical evidence yielding\nstate-of-the-art results in English as well as cross-lingual: Spanish and\nChinese TAC 2015 datasets.\n",
                "publicationDate": "2017-12-05T18:43:57Z",
                "Link": "http://arxiv.org/pdf/1712.01813v1",
                "arxiv_id": "1712.01813v1"
            },
            {
                "Title": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora",
                "Authors": "Mitodru Niyogi, Kripabandhu Ghosh, Arnab Bhattacharya",
                "Abstract": "  Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting.\n",
                "publicationDate": "2018-04-12T12:46:08Z",
                "Link": "http://arxiv.org/pdf/1804.04475v1",
                "arxiv_id": "1804.04475v1"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
                "Authors": "MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee",
                "Abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
                "publicationDate": "2024-02-21T23:43:04Z",
                "Link": "http://arxiv.org/pdf/2402.14179v1",
                "arxiv_id": "2402.14179v1"
            },
            {
                "Title": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval",
                "Authors": "Ranju Mandal, Partha Pratim Roy, Umapada Pal, Michael Blumenstein",
                "Abstract": "  An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches.\n",
                "publicationDate": "2018-07-18T04:29:20Z",
                "Link": "http://arxiv.org/pdf/1807.06772v1",
                "arxiv_id": "1807.06772v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            }
        ]
    },
    {
        "topic_name": "Emotion Analysis",
        "summary": "default",
        "papers": [
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "publicationDate": "2023-12-21T16:52:41Z",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "A Case Study on the Independence of Speech Emotion Recognition in Bangla\n  and English Languages using Language-Independent Prosodic Features",
                "Authors": "Fardin Saad, Hasan Mahmud, Mohammad Ridwan Kabir, Md. Alamin Shaheen, Paresha Farastu, Md. Kamrul Hasan",
                "Abstract": "  A language agnostic approach to recognizing emotions from speech remains an\nincomplete and challenging task. In this paper, we performed a step-by-step\ncomparative analysis of Speech Emotion Recognition (SER) using Bangla and\nEnglish languages to assess whether distinguishing emotions from speech is\nindependent of language. Six emotions were categorized for this study, such as\n- happy, angry, neutral, sad, disgust, and fear. We employed three Emotional\nSpeech Sets (ESS), of which the first two were developed by native Bengali\nspeakers in Bangla and English languages separately. The third was a subset of\nthe Toronto Emotional Speech Set (TESS), which was developed by native English\nspeakers from Canada. We carefully selected language-independent prosodic\nfeatures, adopted a Support Vector Machine (SVM) model, and conducted three\nexperiments to carry out our proposition. In the first experiment, we measured\nthe performance of the three speech sets individually, followed by the second\nexperiment, where different ESS pairs were integrated to analyze the impact on\nSER. Finally, we measured the recognition rate by training and testing the\nmodel with different speech sets in the third experiment. Although this study\nreveals that SER in Bangla and English languages is mostly\nlanguage-independent, some disparities were observed while recognizing\nemotional states like disgust and fear in these two languages. Moreover, our\ninvestigations revealed that non-native speakers convey emotions through\nspeech, much like expressing themselves in their native tongue.\n",
                "publicationDate": "2021-11-21T09:28:49Z",
                "Link": "http://arxiv.org/pdf/2111.10776v3",
                "arxiv_id": "2111.10776v3"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "publicationDate": "2019-07-18T01:00:42Z",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Anubhuti -- An annotated dataset for emotional analysis of Bengali short\n  stories",
                "Authors": "Aditya Pal, Bhaskar Karn",
                "Abstract": "  Thousands of short stories and articles are being written in many different\nlanguages all around the world today. Bengali, or Bangla, is the second highest\nspoken language in India after Hindi and is the national language of the\ncountry of Bangladesh. This work reports in detail the creation of Anubhuti --\nthe first and largest text corpus for analyzing emotions expressed by writers\nof Bengali short stories. We explain the data collection methods, the manual\nannotation process and the resulting high inter-annotator agreement of the\ndataset due to the linguistic expertise of the annotators and the clear\nmethodology of labelling followed. We also address some of the challenges faced\nin the collection of raw data and annotation process of a low resource language\nlike Bengali. We have verified the performance of our dataset with baseline\nMachine Learning as well as a Deep Learning model for emotion classification\nand have found that these standard models have a high accuracy and relevant\nfeature selection on Anubhuti. In addition, we also explain how this dataset\ncan be of interest to linguists and data analysts to study the flow of emotions\nas expressed by writers of Bengali literature.\n",
                "publicationDate": "2020-10-06T22:33:58Z",
                "Link": "http://arxiv.org/pdf/2010.03065v1",
                "arxiv_id": "2010.03065v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "BanglaSarc: A Dataset for Sarcasm Detection",
                "Authors": "Tasnim Sakib Apon, Ramisa Anan, Elizabeth Antora Modhu, Arjun Suter, Ifrit Jamal Sneha, MD. Golam Rabiul Alam",
                "Abstract": "  Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.\n",
                "publicationDate": "2022-09-27T15:28:21Z",
                "Link": "http://arxiv.org/pdf/2209.13461v1",
                "arxiv_id": "2209.13461v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "A Survey of Robotics and Emotion: Classifications and Models of\n  Emotional Interaction",
                "Authors": "Richard Savery, Gil Weinberg",
                "Abstract": "  As emotion plays a growing role in robotic research it is crucial to develop\nmethods to analyze and compare among the wide range of approaches. To this end\nwe present a survey of 1427 IEEE and ACM publications that include robotics and\nemotion. This includes broad categorizations of trends in emotion input\nanalysis, robot emotional expression, studies of emotional interaction and\nmodels for internal processing. We then focus on 232 papers that present\ninternal processing of emotion, such as using a human's emotion for better\ninteraction or turning environmental stimuli into an emotional drive for\nrobotic path planning. We conducted constant comparison analysis of the 232\npapers and arrived at three broad categorization metrics; emotional\nintelligence, emotional model and implementation, each including two or three\nsubcategories. The subcategories address the algorithm used, emotional mapping,\nhistory, the emotional model, emotional categories, the role of emotion, the\npurpose of emotion and the platform. Our results show a diverse field of study,\nlargely divided by the role of emotion in the system, either for improved\ninteraction, or improved robotic performance. We also present multiple future\nopportunities for research and describe intrinsic challenges common in all\npublications.\n",
                "publicationDate": "2020-07-29T13:55:37Z",
                "Link": "http://arxiv.org/pdf/2007.14838v1",
                "arxiv_id": "2007.14838v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach",
                "Authors": "Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, Iqbal H. Sarker",
                "Abstract": "  Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.\n",
                "publicationDate": "2021-04-17T18:28:39Z",
                "Link": "http://arxiv.org/pdf/2104.08613v1",
                "arxiv_id": "2104.08613v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Where are We in Event-centric Emotion Analysis? Bridging Emotion Role\n  Labeling and Appraisal-based Approaches",
                "Authors": "Roman Klinger",
                "Abstract": "  The term emotion analysis in text subsumes various natural language\nprocessing tasks which have in common the goal to enable computers to\nunderstand emotions. Most popular is emotion classification in which one or\nmultiple emotions are assigned to a predefined textual unit. While such setting\nis appropriate for identifying the reader's or author's emotion, emotion role\nlabeling adds the perspective of mentioned entities and extracts text spans\nthat correspond to the emotion cause. The underlying emotion theories agree on\none important point; that an emotion is caused by some internal or external\nevent and comprises several subcomponents, including the subjective feeling and\na cognitive evaluation. We therefore argue that emotions and events are related\nin two ways. (1) Emotions are events; and this perspective is the fundament in\nnatural language processing for emotion role labeling. (2) Emotions are caused\nby events; a perspective that is made explicit with research how to incorporate\npsychological appraisal theories in NLP models to interpret events. These two\nresearch directions, role labeling and (event-focused) emotion classification,\nhave by and large been tackled separately. In this paper, we contextualize both\nperspectives and discuss open research questions.\n",
                "publicationDate": "2023-09-05T09:56:29Z",
                "Link": "http://arxiv.org/pdf/2309.02092v3",
                "arxiv_id": "2309.02092v3"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Emotion Correlation Mining Through Deep Learning Models on Natural\n  Language Text",
                "Authors": "Xinzhi Wang, Luyao Kou, Vijayan Sugumaran, Xiangfeng Luo, Hui Zhang",
                "Abstract": "  Emotion analysis has been attracting researchers' attention. Most previous\nworks in the artificial intelligence field focus on recognizing emotion rather\nthan mining the reason why emotions are not or wrongly recognized. Correlation\namong emotions contributes to the failure of emotion recognition. In this\npaper, we try to fill the gap between emotion recognition and emotion\ncorrelation mining through natural language text from web news. Correlation\namong emotions, expressed as the confusion and evolution of emotion, is\nprimarily caused by human emotion cognitive bias. To mine emotion correlation\nfrom emotion recognition through text, three kinds of features and two deep\nneural network models are presented. The emotion confusion law is extracted\nthrough orthogonal basis. The emotion evolution law is evaluated from three\nperspectives, one-step shift, limited-step shifts, and shortest path transfer.\nThe method is validated using three datasets-the titles, the bodies, and the\ncomments of news articles, covering both objective and subjective texts in\nvarying lengths (long and short). The experimental results show that, in\nsubjective comments, emotions are easily mistaken as anger. Comments tend to\narouse emotion circulations of love-anger and sadness-anger. In objective news,\nit is easy to recognize text emotion as love and cause fear-joy circulation.\nThat means, journalists may try to attract attention using fear and joy words\nbut arouse the emotion love instead; After news release, netizens generate\nemotional comments to express their intense emotions, i.e., anger, sadness, and\nlove. These findings could provide insights for applications regarding\naffective interaction such as network public sentiment, social media\ncommunication, and human-computer interaction.\n",
                "publicationDate": "2020-07-28T08:59:16Z",
                "Link": "http://arxiv.org/pdf/2007.14071v1",
                "arxiv_id": "2007.14071v1"
            },
            {
                "Title": "Emotion Prediction Oriented method with Multiple Supervisions for\n  Emotion-Cause Pair Extraction",
                "Authors": "Guimin Hu, Yi Zhao, Guangming Lu",
                "Abstract": "  Emotion-cause pair extraction (ECPE) task aims to extract all the pairs of\nemotions and their causes from an unannotated emotion text. The previous works\nusually extract the emotion-cause pairs from two perspectives of emotion and\ncause. However, emotion extraction is more crucial to the ECPE task than cause\nextraction. Motivated by this analysis, we propose an end-to-end emotion-cause\nextraction approach oriented toward emotion prediction (EPO-ECPE), aiming to\nfully exploit the potential of emotion prediction to enhance emotion-cause pair\nextraction. Considering the strong dependence between emotion prediction and\nemotion-cause pair extraction, we propose a synchronization mechanism to share\ntheir improvement in the training process. That is, the improvement of emotion\nprediction can facilitate the emotion-cause pair extraction, and then the\nresults of emotion-cause pair extraction can also be used to improve the\naccuracy of emotion prediction simultaneously. For the emotion-cause pair\nextraction, we divide it into genuine pair supervision and fake pair\nsupervision, where the genuine pair supervision learns from the pairs with more\npossibility to be emotion-cause pairs. In contrast, fake pair supervision\nlearns from other pairs. In this way, the emotion-cause pairs can be extracted\ndirectly from the genuine pair, thereby reducing the difficulty of extraction.\nExperimental results show that our approach outperforms the 13 compared systems\nand achieves new state-of-the-art performance.\n",
                "publicationDate": "2023-02-24T02:45:49Z",
                "Link": "http://arxiv.org/pdf/2302.12417v1",
                "arxiv_id": "2302.12417v1"
            },
            {
                "Title": "MsEmoTTS: Multi-scale emotion transfer, prediction, and control for\n  emotional speech synthesis",
                "Authors": "Yi Lei, Shan Yang, Xinsheng Wang, Lei Xie",
                "Abstract": "  Expressive synthetic speech is essential for many human-computer interaction\nand audio broadcast scenarios, and thus synthesizing expressive speech has\nattracted much attention in recent years. Previous methods performed the\nexpressive speech synthesis either with explicit labels or with a fixed-length\nstyle embedding extracted from reference audio, both of which can only learn an\naverage style and thus ignores the multi-scale nature of speech prosody. In\nthis paper, we propose MsEmoTTS, a multi-scale emotional speech synthesis\nframework, to model the emotion from different levels. Specifically, the\nproposed method is a typical attention-based sequence-to-sequence model and\nwith proposed three modules, including global-level emotion presenting module\n(GM), utterance-level emotion presenting module (UM), and local-level emotion\npresenting module (LM), to model the global emotion category, utterance-level\nemotion variation, and syllable-level emotion strength, respectively. In\naddition to modeling the emotion from different levels, the proposed method\nalso allows us to synthesize emotional speech in different ways, i.e.,\ntransferring the emotion from reference audio, predicting the emotion from\ninput text, and controlling the emotion strength manually. Extensive\nexperiments conducted on a Chinese emotional speech corpus demonstrate that the\nproposed method outperforms the compared reference audio-based and text-based\nemotional speech synthesis methods on the emotion transfer speech synthesis and\ntext-based emotion prediction speech synthesis respectively. Besides, the\nexperiments also show that the proposed method can control the emotion\nexpressions flexibly. Detailed analysis shows the effectiveness of each module\nand the good design of the proposed method.\n",
                "publicationDate": "2022-01-17T15:13:18Z",
                "Link": "http://arxiv.org/pdf/2201.06460v1",
                "arxiv_id": "2201.06460v1"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation",
                "Authors": "Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin",
                "Abstract": "  This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.\n",
                "publicationDate": "2023-06-09T12:07:10Z",
                "Link": "http://arxiv.org/pdf/2306.06147v1",
                "arxiv_id": "2306.06147v1"
            },
            {
                "Title": "Emotion Action Detection and Emotion Inference: the Task and Dataset",
                "Authors": "Pengyuan Liu, Chengyu Du, Shuofeng Zhao, Chenghao Zhu",
                "Abstract": "  Many Natural Language Processing works on emotion analysis only focus on\nsimple emotion classification without exploring the potentials of putting\nemotion into \"event context\", and ignore the analysis of emotion-related\nevents. One main reason is the lack of this kind of corpus. Here we present\nCause-Emotion-Action Corpus, which manually annotates not only emotion, but\nalso cause events and action events. We propose two new tasks based on the\ndata-set: emotion causality and emotion inference. The first task is to extract\na triple (cause, emotion, action). The second task is to infer the probable\nemotion. We are currently releasing the data-set with 10,603 samples and 15,892\nevents, basic statistic analysis and baseline on both emotion causality and\nemotion inference tasks. Baseline performance demonstrates that there is much\nroom for both tasks to be improved.\n",
                "publicationDate": "2019-03-16T09:46:29Z",
                "Link": "http://arxiv.org/pdf/1903.06901v1",
                "arxiv_id": "1903.06901v1"
            },
            {
                "Title": "MES-P: an Emotional Tonal Speech Dataset in Mandarin Chinese with Distal\n  and Proximal Labels",
                "Authors": "Zhongzhe Xiao, Ying Chen, Weibei Dou, Zhi Tao, Liming Chen",
                "Abstract": "  Emotion shapes all aspects of our interpersonal and intellectual experiences.\nIts automatic analysis has there-fore many applications, e.g., human-machine\ninterface. In this paper, we propose an emotional tonal speech dataset, namely\nMandarin Chinese Emotional Speech Dataset - Portrayed (MES-P), with both distal\nand proximal labels. In contrast with state of the art emotional speech\ndatasets which are only focused on perceived emotions, the proposed MES-P\ndataset includes not only perceived emotions with their proximal labels but\nalso intended emotions with distal labels, thereby making it possible to study\nhuman emotional intelligence, i.e. people emotion expression ability and their\nskill of understanding emotions, thus explicitly accounting for perception\ndifferences between intended and perceived emotions in speech signals and\nenabling studies of emotional misunderstandings which often occur in real life.\nFurthermore, the proposed MES-P dataset also captures a main feature of tonal\nlanguages, i.e., tonal variations, and provides recorded emotional speech\nsamples whose tonal variations match the tonal distribution in real life\nMandarin Chinese. Besides, the proposed MES-P dataset features emotion\nintensity variations as well, and includes both moderate and intense versions\nof recordings for joy, anger, and sadness in addition to neutral speech.\nRatings of the collected speech samples are made in valence-arousal space\nthrough continuous coordinate locations, resulting in an emotional distribution\npattern in 2D VA space. The consistency between the speakers' emotional\nintentions and the listeners' perceptions is also studied using Cohen's Kappa\ncoefficients. Finally, we also carry out extensive experiments using a baseline\non MES-P for automatic emotion recognition and compare the results with human\nemotion intelligence.\n",
                "publicationDate": "2018-08-30T03:02:46Z",
                "Link": "http://arxiv.org/pdf/1808.10095v2",
                "arxiv_id": "1808.10095v2"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "publicationDate": "2020-04-19T07:42:22Z",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Building a Dialogue Corpus Annotated with Expressed and Experienced\n  Emotions",
                "Authors": "Tatsuya Ide, Daisuke Kawahara",
                "Abstract": "  In communication, a human would recognize the emotion of an interlocutor and\nrespond with an appropriate emotion, such as empathy and comfort. Toward\ndeveloping a dialogue system with such a human-like ability, we propose a\nmethod to build a dialogue corpus annotated with two kinds of emotions. We\ncollect dialogues from Twitter and annotate each utterance with the emotion\nthat a speaker put into the utterance (expressed emotion) and the emotion that\na listener felt after listening to the utterance (experienced emotion). We\nbuilt a dialogue corpus in Japanese using this method, and its statistical\nanalysis revealed the differences between expressed and experienced emotions.\nWe conducted experiments on recognition of the two kinds of emotions. The\nexperimental results indicated the difficulty in recognizing experienced\nemotions and the effectiveness of multi-task learning of the two kinds of\nemotions. We hope that the constructed corpus will facilitate the study on\nemotion recognition in a dialogue and emotion-aware dialogue response\ngeneration.\n",
                "publicationDate": "2022-05-24T07:40:11Z",
                "Link": "http://arxiv.org/pdf/2205.11867v1",
                "arxiv_id": "2205.11867v1"
            },
            {
                "Title": "Analysis of Microstate Organization During Emotional Events",
                "Authors": "Sudhakar Mishra, Narayanan Srinivasan, Uma Shanker Tiwary",
                "Abstract": "  Understanding the dynamics of emotional experience is an old problem.\nHowever, a clear understanding of the mechanism of emotional experience is\nstill far away. In the presented work, we tried to address this problem using a\nwell-established method called microstate analysis using multichannel\nelectroencephalography (EEG). We recorded the brain activity of spontaneous\nemotional experiences while participants were watching multimedia emotional\nstimuli. The time duration where the participants spontaneously felt an\nemotion, we termed it an emotional event. Microstate segmentation is performed\nfor all emotional events to calculate the set of microstates (MS). Followed by\na comparison of calculated statistical parameters and transition probabilities\nfor the emotional and non-emotional conditions. We found a set of MS (four MS)\nfor emotional and non-emotional conditions that differ from each other. We\nobserved that MS1 has a higher value of occurrence, duration and coverage for\nemotional conditions. In addition, the transition to MS1 for the emotional\ncondition was higher. On the other hand, for non-emotion (or neutral)\ncondition, transition to MS3 was higher. A set of MS related to neutral\ncondition was source localized to brain regions involved in higher-level\nsensory feature processing. On the other hand, for emotional conditions, MS1 \\&\nMS2 are localized to sensory feature processing regions, and MS3 \\& MS4 are\nadditionally localized to regions related to socio-emotional processing. Our\nresults hint toward the constructionist mechanism, which has an asymmetric\ncontribution from bottom-up and top-down processing during an emotional\nexperience.\n",
                "publicationDate": "2022-10-16T17:17:02Z",
                "Link": "http://arxiv.org/pdf/2210.08588v1",
                "arxiv_id": "2210.08588v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "publicationDate": "2024-03-31T09:52:25Z",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1"
            },
            {
                "Title": "Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment\n  Analysis",
                "Authors": "Jiazhen Wang",
                "Abstract": "  A multi-modal emotion recognition method was established by combining\ntwo-channel convolutional neural network with ring network. This method can\nextract emotional information effectively and improve learning efficiency. The\nwords were vectorized with GloVe, and the word vector was input into the\nconvolutional neural network. Combining attention mechanism and maximum pool\nconverter BiSRU channel, the local deep emotion and pre-post sequential emotion\nsemantics are obtained. Finally, multiple features are fused and input as the\npolarity of emotion, so as to achieve the emotion analysis of the target.\nExperiments show that the emotion analysis method based on feature fusion can\neffectively improve the recognition accuracy of emotion data set and reduce the\nlearning time. The model has a certain generalization.\n",
                "publicationDate": "2023-11-19T05:49:39Z",
                "Link": "http://arxiv.org/pdf/2311.11237v1",
                "arxiv_id": "2311.11237v1"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            }
        ]
    },
    {
        "topic_name": "Discourse Analysis",
        "summary": "default",
        "papers": [
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Persian Rhetorical Structure Theory",
                "Authors": "Sara Shahmohammadi, Hadi Veisi, Ali Darzi",
                "Abstract": "  Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.\n",
                "publicationDate": "2021-06-25T18:15:47Z",
                "Link": "http://arxiv.org/pdf/2106.13833v1",
                "arxiv_id": "2106.13833v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "BdSLW60: A Word-Level Bangla Sign Language Dataset",
                "Authors": "Husne Ara Rubaiyeat, Hasan Mahmud, Ahsan Habib, Md. Kamrul Hasan",
                "Abstract": "  Sign language discourse is an essential mode of daily communication for the\ndeaf and hard-of-hearing people. However, research on Bangla Sign Language\n(BdSL) faces notable limitations, primarily due to the lack of datasets.\nRecognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of\nchallenges, including the need for well-annotated datasets, capturing the\ndynamic nature of sign gestures from facial or hand landmarks, developing\nsuitable machine learning or deep learning-based models with substantial video\nsamples, and so on. In this paper, we address these challenges by creating a\ncomprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and\nnatural setting, allowing positional and temporal variations and allowing sign\nusers to change hand dominance freely. The dataset encompasses 60 Bangla sign\nwords, with a significant scale of 9307 video trials provided by 18 signers\nunder the supervision of a sign language professional. The dataset was\nrigorously annotated and cross-checked by 60 annotators. We also introduced a\nunique approach of a relative quantization-based key frame encoding technique\nfor landmark based sign gesture recognition. We report the benchmarking of our\nBdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up\nto 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The\ndataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and\nthe code base is accessible from https://github.com/hasanssl/BdSLW60_Code.\n",
                "publicationDate": "2024-02-13T18:02:58Z",
                "Link": "http://arxiv.org/pdf/2402.08635v1",
                "arxiv_id": "2402.08635v1"
            },
            {
                "Title": "Large Discourse Treebanks from Scalable Distant Supervision",
                "Authors": "Patrick Huber, Giuseppe Carenini",
                "Abstract": "  Discourse parsing is an essential upstream task in Natural Language\nProcessing with strong implications for many real-world applications. Despite\nits widely recognized role, most recent discourse parsers (and consequently\ndownstream tasks) still rely on small-scale human-annotated discourse\ntreebanks, trying to infer general-purpose discourse structures from very\nlimited data in a few narrow domains. To overcome this dire situation and allow\ndiscourse parsers to be trained on larger, more diverse and domain-independent\ndatasets, we propose a framework to generate \"silver-standard\" discourse trees\nfrom distant supervision on the auxiliary task of sentiment analysis.\n",
                "publicationDate": "2022-10-18T03:33:43Z",
                "Link": "http://arxiv.org/pdf/2212.06038v1",
                "arxiv_id": "2212.06038v1"
            },
            {
                "Title": "Towards Domain-Independent Supervised Discourse Parsing Through Gradient\n  Boosting",
                "Authors": "Patrick Huber, Giuseppe Carenini",
                "Abstract": "  Discourse analysis and discourse parsing have shown great impact on many\nimportant problems in the field of Natural Language Processing (NLP). Given the\ndirect impact of discourse annotations on model performance and\ninterpretability, robustly extracting discourse structures from arbitrary\ndocuments is a key task to further improve computational models in NLP. To this\nend, we present a new, supervised paradigm directly tackling the domain\nadaptation issue in discourse parsing. Specifically, we introduce the first\nfully supervised discourse parser designed to alleviate the domain dependency\nthrough a staged model of weak classifiers by introducing the gradient boosting\nframework.\n",
                "publicationDate": "2022-10-18T03:44:27Z",
                "Link": "http://arxiv.org/pdf/2210.09565v1",
                "arxiv_id": "2210.09565v1"
            },
            {
                "Title": "Distributed Marker Representation for Ambiguous Discourse Markers and\n  Entangled Relations",
                "Authors": "Dongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang, Zheng Zhang",
                "Abstract": "  Discourse analysis is an important task because it models intrinsic semantic\nstructures between sentences in a document. Discourse markers are natural\nrepresentations of discourse in our daily language. One challenge is that the\nmarkers as well as pre-defined and human-labeled discourse relations can be\nambiguous when describing the semantics between sentences. We believe that a\nbetter approach is to use a contextual-dependent distribution over the markers\nto express discourse information. In this work, we propose to learn a\nDistributed Marker Representation (DMR) by utilizing the (potentially)\nunlimited discourse marker data with a latent discourse sense, thereby bridging\nmarkers with sentence pairs. Such representations can be learned automatically\nfrom data without supervision, and in turn provide insights into the data\nitself. Experiments show the SOTA performance of our DMR on the implicit\ndiscourse relation recognition task and strong interpretability. Our method\nalso offers a valuable tool to understand complex ambiguity and entanglement\namong discourse markers and manually defined discourse relations.\n",
                "publicationDate": "2023-06-19T00:49:51Z",
                "Link": "http://arxiv.org/pdf/2306.10658v1",
                "arxiv_id": "2306.10658v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "The ComMA Dataset V0.2: Annotating Aggression and Bias in Multilingual\n  Social Media Discourse",
                "Authors": "Ritesh Kumar, Enakshi Nandi, Laishram Niranjana Devi, Shyam Ratan, Siddharth Singh, Akash Bhagat, Yogesh Dawer",
                "Abstract": "  In this paper, we discuss the development of a multilingual dataset annotated\nwith a hierarchical, fine-grained tagset marking different types of aggression\nand the \"context\" in which they occur. The context, here, is defined by the\nconversational thread in which a specific comment occurs and also the \"type\" of\ndiscursive role that the comment is performing with respect to the previous\ncomment. The initial dataset, being discussed here (and made available as part\nof the ComMA@ICON shared task), consists of a total 15,000 annotated comments\nin four languages - Meitei, Bangla, Hindi, and Indian English - collected from\nvarious social media platforms such as YouTube, Facebook, Twitter and Telegram.\nAs is usual on social media websites, a large number of these comments are\nmultilingual, mostly code-mixed with English. The paper gives a detailed\ndescription of the tagset being used for annotation and also the process of\ndeveloping a multi-label, fine-grained tagset that can be used for marking\ncomments with aggression and bias of various kinds including gender bias,\nreligious intolerance (called communal bias in the tagset), class/caste bias\nand ethnic/racial bias. We also define and discuss the tags that have been used\nfor marking different the discursive role being performed through the comments,\nsuch as attack, defend, etc. We also present a statistical analysis of the\ndataset as well as results of our baseline experiments with developing an\nautomatic aggression identification system using the dataset developed.\n",
                "publicationDate": "2021-11-19T19:03:22Z",
                "Link": "http://arxiv.org/pdf/2111.10390v1",
                "arxiv_id": "2111.10390v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Prompt-based Connective Prediction Method for Fine-grained Implicit\n  Discourse Relation Recognition",
                "Authors": "Hao Zhou, Man Lan, Yuanbin Wu, Yuefeng Chen, Meirong Ma",
                "Abstract": "  Due to the absence of connectives, implicit discourse relation recognition\n(IDRR) is still a challenging and crucial task in discourse analysis. Most of\nthe current work adopted multi-task learning to aid IDRR through explicit\ndiscourse relation recognition (EDRR) or utilized dependencies between\ndiscourse relation labels to constrain model predictions. But these methods\nstill performed poorly on fine-grained IDRR and even utterly misidentified on\nmost of the few-shot discourse relation classes. To address these problems, we\npropose a novel Prompt-based Connective Prediction (PCP) method for IDRR. Our\nmethod instructs large-scale pre-trained models to use knowledge relevant to\ndiscourse relation and utilizes the strong correlation between connectives and\ndiscourse relation to help the model recognize implicit discourse relations.\nExperimental results show that our method surpasses the current\nstate-of-the-art model and achieves significant improvements on those\nfine-grained few-shot discourse relation. Moreover, our approach is able to be\ntransferred to EDRR and obtain acceptable results. Our code is released in\nhttps://github.com/zh-i9/PCP-for-IDRR.\n",
                "publicationDate": "2022-10-13T13:47:13Z",
                "Link": "http://arxiv.org/pdf/2210.07032v2",
                "arxiv_id": "2210.07032v2"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "Incorporating Annotator Uncertainty into Representations of Discourse\n  Relations",
                "Authors": "S. Magal\u00ed L\u00f3pez Cortez, Cassandra L. Jacobs",
                "Abstract": "  Annotation of discourse relations is a known difficult task, especially for\nnon-expert annotators. In this paper, we investigate novice annotators'\nuncertainty on the annotation of discourse relations on spoken conversational\ndata. We find that dialogue context (single turn, pair of turns within speaker,\nand pair of turns across speakers) is a significant predictor of confidence\nscores. We compute distributed representations of discourse relations from\nco-occurrence statistics that incorporate information about confidence scores\nand dialogue context. We perform a hierarchical clustering analysis using these\nrepresentations and show that weighting discourse relation representations with\ninformation about confidence and dialogue context coherently models our\nannotators' uncertainty about discourse relation labels.\n",
                "publicationDate": "2023-08-14T14:39:02Z",
                "Link": "http://arxiv.org/pdf/2308.07179v1",
                "arxiv_id": "2308.07179v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation",
                "Authors": "Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin",
                "Abstract": "  This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.\n",
                "publicationDate": "2023-06-09T12:07:10Z",
                "Link": "http://arxiv.org/pdf/2306.06147v1",
                "arxiv_id": "2306.06147v1"
            },
            {
                "Title": "Unifying Discourse Resources with Dependency Framework",
                "Authors": "Yi Cheng, Sujian Li, Yueyuan Li",
                "Abstract": "  For text-level discourse analysis, there are various discourse schemes but\nrelatively few labeled data, because discourse research is still immature and\nit is labor-intensive to annotate the inner logic of a text. In this paper, we\nattempt to unify multiple Chinese discourse corpora under different annotation\nschemes with discourse dependency framework by designing semi-automatic methods\nto convert them into dependency structures. We also implement several benchmark\ndependency parsers and research on how they can leverage the unified data to\nimprove performance.\n",
                "publicationDate": "2021-01-01T05:23:29Z",
                "Link": "http://arxiv.org/pdf/2101.00167v3",
                "arxiv_id": "2101.00167v3"
            },
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "publicationDate": "2023-12-21T16:52:41Z",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "publicationDate": "2020-04-19T07:42:22Z",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1"
            },
            {
                "Title": "Chinese Discourse Segmentation Using Bilingual Discourse Commonality",
                "Authors": "Jingfeng Yang, Sujian Li",
                "Abstract": "  Discourse segmentation aims to segment Elementary Discourse Units (EDUs) and\nis a fundamental task in discourse analysis. For Chinese, previous researches\nidentify EDUs just through discriminating the functions of punctuations. In\nthis paper, we argue that Chinese EDUs may not end at the punctuation positions\nand should follow the definition of EDU in RST-DT. With this definition, we\nconduct Chinese discourse segmentation with the help of English labeled\ndata.Using discourse commonality between English and Chinese, we design an\nadversarial neural network framework to extract common language-independent\nfeatures and language-specific features which are useful for discourse\nsegmentation, when there is no or only a small scale of Chinese labeled data\navailable. Experiments on discourse segmentation demonstrate that our models\ncan leverage common features from bilingual data, and learn efficient\nChinese-specific features from a small amount of Chinese labeled data,\noutperforming the baseline models.\n",
                "publicationDate": "2018-08-30T00:57:09Z",
                "Link": "http://arxiv.org/pdf/1809.01497v1",
                "arxiv_id": "1809.01497v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "DiscSense: Automated Semantic Analysis of Discourse Markers",
                "Authors": "Damien Sileo, Tim Van de Cruys, Camille Pradel, Philippe Muller",
                "Abstract": "  Discourse markers ({\\it by contrast}, {\\it happily}, etc.) are words or\nphrases that are used to signal semantic and/or pragmatic relationships between\nclauses or sentences. Recent work has fruitfully explored the prediction of\ndiscourse markers between sentence pairs in order to learn accurate sentence\nrepresentations, that are useful in various classification tasks. In this work,\nwe take another perspective: using a model trained to predict discourse markers\nbetween sentence pairs, we predict plausible markers between sentence pairs\nwith a known semantic relation (provided by existing classification datasets).\nThese predictions allow us to study the link between discourse markers and the\nsemantic relations annotated in classification datasets. Handcrafted mappings\nhave been proposed between markers and discourse relations on a limited set of\nmarkers and a limited set of categories, but there exist hundreds of discourse\nmarkers expressing a wide variety of relations, and there is no consensus on\nthe taxonomy of relations between competing discourse theories (which are\nlargely built in a top-down fashion). By using an automatic rediction method\nover existing semantically annotated datasets, we provide a bottom-up\ncharacterization of discourse markers in English. The resulting dataset, named\nDiscSense, is publicly available.\n",
                "publicationDate": "2020-06-02T13:39:53Z",
                "Link": "http://arxiv.org/pdf/2006.01603v1",
                "arxiv_id": "2006.01603v1"
            },
            {
                "Title": "Cross-Genre Argument Mining: Can Language Models Automatically Fill in\n  Missing Discourse Markers?",
                "Authors": "Gil Rocha, Henrique Lopes Cardoso, Jonas Belouadi, Steffen Eger",
                "Abstract": "  Available corpora for Argument Mining differ along several axes, and one of\nthe key differences is the presence (or absence) of discourse markers to signal\nargumentative content. Exploring effective ways to use discourse markers has\nreceived wide attention in various discourse parsing tasks, from which it is\nwell-known that discourse markers are strong indicators of discourse relations.\nTo improve the robustness of Argument Mining systems across different genres,\nwe propose to automatically augment a given text with discourse markers such\nthat all relations are explicitly signaled. Our analysis unveils that popular\nlanguage models taken out-of-the-box fail on this task; however, when\nfine-tuned on a new heterogeneous dataset that we construct (including\nsynthetic and real examples), they perform considerably better. We demonstrate\nthe impact of our approach on an Argument Mining downstream task, evaluated on\ndifferent corpora, showing that language models can be trained to automatically\nfill in discourse markers across different corpora, improving the performance\nof a downstream model in some, but not all, cases. Our proposed approach can\nfurther be employed as an assistive tool for better discourse understanding.\n",
                "publicationDate": "2023-06-07T10:19:50Z",
                "Link": "http://arxiv.org/pdf/2306.04314v1",
                "arxiv_id": "2306.04314v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "publicationDate": "2024-03-31T09:52:25Z",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "Better Document-level Sentiment Analysis from RST Discourse Parsing",
                "Authors": "Parminder Bhatia, Yangfeng Ji, Jacob Eisenstein",
                "Abstract": "  Discourse structure is the hidden link between surface features and\ndocument-level properties, such as sentiment polarity. We show that the\ndiscourse analyses produced by Rhetorical Structure Theory (RST) parsers can\nimprove document-level sentiment analysis, via composition of local information\nup the discourse tree. First, we show that reweighting discourse units\naccording to their position in a dependency representation of the rhetorical\nstructure can yield substantial improvements on lexicon-based sentiment\nanalysis. Next, we present a recursive neural network over the RST structure,\nwhich offers significant improvements over classification-based methods.\n",
                "publicationDate": "2015-09-04T20:28:12Z",
                "Link": "http://arxiv.org/pdf/1509.01599v2",
                "arxiv_id": "1509.01599v2"
            },
            {
                "Title": "Towards Using Machine Translation Techniques to Induce Multilingual\n  Lexica of Discourse Markers",
                "Authors": "Ant\u00f3nio Lopes, David Martins de Matos, Vera Cabarr\u00e3o, Ricardo Ribeiro, Helena Moniz, Isabel Trancoso, Ana Isabel Mata",
                "Abstract": "  Discourse markers are universal linguistic events subject to language\nvariation. Although an extensive literature has already reported language\nspecific traits of these events, little has been said on their cross-language\nbehavior and on building an inventory of multilingual lexica of discourse\nmarkers. This work describes new methods and approaches for the description,\nclassification, and annotation of discourse markers in the specific domain of\nthe Europarl corpus. The study of discourse markers in the context of\ntranslation is crucial due to the idiomatic nature of these structures.\nMultilingual lexica together with the functional analysis of such structures\nare useful tools for the hard task of translating discourse markers into\npossible equivalents from one language to another. Using Daniel Marcu's\nvalidated discourse markers for English, extracted from the Brown Corpus, our\npurpose is to build multilingual lexica of discourse markers for other\nlanguages, based on machine translation techniques. The major assumption in\nthis study is that the usage of a discourse marker is independent of the\nlanguage, i.e., the rhetorical function of a discourse marker in a sentence in\none language is equivalent to the rhetorical function of the same discourse\nmarker in another language.\n",
                "publicationDate": "2015-03-31T17:56:07Z",
                "Link": "http://arxiv.org/pdf/1503.09144v1",
                "arxiv_id": "1503.09144v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            }
        ]
    },
    {
        "topic_name": "Question Answering",
        "summary": "default",
        "papers": [
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP",
                "Authors": "Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque",
                "Abstract": "  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.\n",
                "publicationDate": "2023-09-22T20:29:34Z",
                "Link": "http://arxiv.org/pdf/2309.13173v2",
                "arxiv_id": "2309.13173v2"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Enhancing Answer Selection in Community Question Answering with\n  Pre-trained and Large Language Models",
                "Authors": "Xinghang Hu",
                "Abstract": "  Community Question Answering (CQA) becomes increasingly prevalent in recent\nyears. However, there are a large number of answers, which is difficult for\nusers to select the relevant answers. Therefore, answer selection is a very\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\ncross attention networks (QAN) with pre-trained models for answer selection and\nutilize large language model (LLM) to perform answer selection with knowledge\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\npre-training for question subjects, question bodies and answers, respectively,\nthen the cross attention mechanism selects the most relevant answer for\ndifferent questions. Experiments show that the QAN model achieves\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\nMoreover, we use the LLM to generate external knowledge from questions and\ncorrect answers to achieve knowledge augmentation for the answer selection task\nby LLM, while optimizing the prompt of LLM in different aspects. The results\nshow that the introduction of external knowledge can improve the correct answer\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\ncan also select the correct answer on more questions by optimized prompt.\n",
                "publicationDate": "2023-11-29T10:24:50Z",
                "Link": "http://arxiv.org/pdf/2311.17502v1",
                "arxiv_id": "2311.17502v1"
            },
            {
                "Title": "Learning to answer questions",
                "Authors": "Ana Cristina Mendes, Lu\u00edsa Coheur, S\u00e9rgio Curto",
                "Abstract": "  We present an open-domain Question-Answering system that learns to answer\nquestions based on successful past interactions. We follow a pattern-based\napproach to Answer-Extraction, where (lexico-syntactic) patterns that relate a\nquestion to its answer are automatically learned and used to answer future\nquestions. Results show that our approach contributes to the system's best\nperformance when it is conjugated with typical Answer-Extraction strategies.\nMoreover, it allows the system to learn with the answered questions and to\nrectify wrong or unsolved past questions.\n",
                "publicationDate": "2013-09-04T18:10:22Z",
                "Link": "http://arxiv.org/pdf/1309.1125v1",
                "arxiv_id": "1309.1125v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "QAMPARI: An Open-domain Question Answering Benchmark for Questions with\n  Many Answers from Multiple Paragraphs",
                "Authors": "Samuel Joseph Amouyal, Tomer Wolfson, Ohad Rubin, Ori Yoran, Jonathan Herzig, Jonathan Berant",
                "Abstract": "  Existing benchmarks for open-domain question answering (ODQA) typically focus\non questions whose answers can be extracted from a single paragraph. By\ncontrast, many natural questions, such as \"What players were drafted by the\nBrooklyn Nets?\" have a list of answers. Answering such questions requires\nretrieving and reading from many passages, in a large corpus. We introduce\nQAMPARI, an ODQA benchmark, where question answers are lists of entities,\nspread across many paragraphs. We created QAMPARI by (a) generating questions\nwith multiple answers from Wikipedia's knowledge graph and tables, (b)\nautomatically pairing answers with supporting evidence in Wikipedia paragraphs,\nand (c) manually paraphrasing questions and validating each answer. We train\nODQA models from the retrieve-and-read family and find that QAMPARI is\nchallenging in terms of both passage retrieval and answer generation, reaching\nan F1 score of 32.8 at best. Our results highlight the need for developing ODQA\nmodels that handle a broad range of question types, including single and\nmulti-answer questions.\n",
                "publicationDate": "2022-05-25T11:21:30Z",
                "Link": "http://arxiv.org/pdf/2205.12665v4",
                "arxiv_id": "2205.12665v4"
            },
            {
                "Title": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051",
                "Authors": "Nasif Muslim, Md. Tanvir Adnan, Mohammad Zahidul Kabir, Md. Humayun Kabir, Sheikh Mominul Islam",
                "Abstract": "  In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance.\n",
                "publicationDate": "2012-08-05T09:22:06Z",
                "Link": "http://arxiv.org/pdf/1208.0995v1",
                "arxiv_id": "1208.0995v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Generating Answer Candidates for Quizzes and Answer-Aware Question\n  Generators",
                "Authors": "Kristiyan Vachev, Momchil Hardalov, Georgi Karadzhov, Georgi Georgiev, Ivan Koychev, Preslav Nakov",
                "Abstract": "  In education, open-ended quiz questions have become an important tool for\nassessing the knowledge of students. Yet, manually preparing such questions is\na tedious task, and thus automatic question generation has been proposed as a\npossible alternative. So far, the vast majority of research has focused on\ngenerating the question text, relying on question answering datasets with\nreadily picked answers, and the problem of how to come up with answer\ncandidates in the first place has been largely ignored. Here, we aim to bridge\nthis gap. In particular, we propose a model that can generate a specified\nnumber of answer candidates for a given passage of text, which can then be used\nby instructors to write questions manually or can be passed as an input to\nautomatic answer-aware question generators. Our experiments show that our\nproposed answer candidate generation model outperforms several baselines.\n",
                "publicationDate": "2021-08-29T19:33:51Z",
                "Link": "http://arxiv.org/pdf/2108.12898v1",
                "arxiv_id": "2108.12898v1"
            },
            {
                "Title": "The combination of context information to enhance simple question\n  answering",
                "Authors": "Zhaohui Chao, Lin Li",
                "Abstract": "  With the rapid development of knowledge base,question answering based on\nknowledge base has been a hot research issue. In this paper, we focus on\nanswering singlerelation factoid questions based on knowledge base. We build a\nquestion answering system and study the effect of context information on fact\nselection, such as entity's notable type,outdegree. Experimental results show\nthat context information can improve the result of simple question answering.\n",
                "publicationDate": "2018-10-09T14:02:56Z",
                "Link": "http://arxiv.org/pdf/1810.04000v1",
                "arxiv_id": "1810.04000v1"
            }
        ]
    },
    {
        "topic_name": "Information Retrieval",
        "summary": "default",
        "papers": [
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Ranking the locations and predicting future crime occurrence by\n  retrieving news from different Bangla online newspapers",
                "Authors": "Jumman Hossain, Rajib Chandra Das, Md. Ruhul Amin, Md. Saiful Islam",
                "Abstract": "  There have thousands of crimes are happening daily all around. But people\nkeep statistics only few of them, therefore crime rates are increasing day by\nday. The reason behind can be less concern or less statistics of previous\ncrimes. It is much more important to observe the previous crime statistics for\ngeneral people to make their outing decision and police for catching the\ncriminals are taking steps to restrain the crimes and tourists to make their\ntravelling decision. National institute of justice releases crime survey data\nfor the country, but does not offer crime statistics up to Union or Thana\nlevel. Considering all of these cases we have come up with an approach which\ncan give an approximation to people about the safety of a specific location\nwith crime ranking of different areas locating the crimes on a map including a\nfuture crime occurrence prediction mechanism. Our approach relies on different\nonline Bangla newspapers for crawling the crime data, stemming and keyword\nextraction, location finding algorithm, cosine similarity, naive Bayes\nclassifier, and a custom crime prediction model\n",
                "publicationDate": "2023-05-18T04:19:26Z",
                "Link": "http://arxiv.org/pdf/2305.10698v1",
                "arxiv_id": "2305.10698v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection",
                "Authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor Ryen, Arman Hossain, Faisal Bin Ashraf",
                "Abstract": "  Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.\n",
                "publicationDate": "2021-02-04T08:35:18Z",
                "Link": "http://arxiv.org/pdf/2102.02478v1",
                "arxiv_id": "2102.02478v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters",
                "Authors": "Qun Liu, Edward Collier, Supratik Mukhopadhyay",
                "Abstract": "  Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets.\n",
                "publicationDate": "2019-08-11T08:01:58Z",
                "Link": "http://arxiv.org/pdf/1908.08987v1",
                "arxiv_id": "1908.08987v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding",
                "Authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Avirup Bhattacharyya, Umapada Pal",
                "Abstract": "  Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach.\n",
                "publicationDate": "2017-08-18T07:47:05Z",
                "Link": "http://arxiv.org/pdf/1708.05529v6",
                "arxiv_id": "1708.05529v6"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora",
                "Authors": "Mitodru Niyogi, Kripabandhu Ghosh, Arnab Bhattacharya",
                "Abstract": "  Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting.\n",
                "publicationDate": "2018-04-12T12:46:08Z",
                "Link": "http://arxiv.org/pdf/1804.04475v1",
                "arxiv_id": "1804.04475v1"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
                "Authors": "MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee",
                "Abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
                "publicationDate": "2024-02-21T23:43:04Z",
                "Link": "http://arxiv.org/pdf/2402.14179v1",
                "arxiv_id": "2402.14179v1"
            },
            {
                "Title": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval",
                "Authors": "Ranju Mandal, Partha Pratim Roy, Umapada Pal, Michael Blumenstein",
                "Abstract": "  An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches.\n",
                "publicationDate": "2018-07-18T04:29:20Z",
                "Link": "http://arxiv.org/pdf/1807.06772v1",
                "arxiv_id": "1807.06772v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            }
        ]
    },
    {
        "topic_name": "Dependency Parsing",
        "summary": "default",
        "papers": [
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "A Survey of Syntactic-Semantic Parsing Based on Constituent and\n  Dependency Structures",
                "Authors": "Meishan Zhang",
                "Abstract": "  Syntactic and semantic parsing has been investigated for decades, which is\none primary topic in the natural language processing community. This article\naims for a brief survey on this topic. The parsing community includes many\ntasks, which are difficult to be covered fully. Here we focus on two of the\nmost popular formalizations of parsing: constituent parsing and dependency\nparsing. Constituent parsing is majorly targeted to syntactic analysis, and\ndependency parsing can handle both syntactic and semantic analysis. This\narticle briefly reviews the representative models of constituent parsing and\ndependency parsing, and also dependency graph parsing with rich semantics.\nBesides, we also review the closely-related topics such as cross-domain,\ncross-lingual and joint parsing models, parser application as well as corpus\ndevelopment of parsing in the article.\n",
                "publicationDate": "2020-06-19T10:21:17Z",
                "Link": "http://arxiv.org/pdf/2006.11056v1",
                "arxiv_id": "2006.11056v1"
            },
            {
                "Title": "A Survey of Unsupervised Dependency Parsing",
                "Authors": "Wenjuan Han, Yong Jiang, Hwee Tou Ng, Kewei Tu",
                "Abstract": "  Syntactic dependency parsing is an important task in natural language\nprocessing. Unsupervised dependency parsing aims to learn a dependency parser\nfrom sentences that have no annotation of their correct parse trees. Despite\nits difficulty, unsupervised parsing is an interesting research direction\nbecause of its capability of utilizing almost unlimited unannotated text data.\nIt also serves as the basis for other research in low-resource parsing. In this\npaper, we survey existing approaches to unsupervised dependency parsing,\nidentify two major classes of approaches, and discuss recent trends. We hope\nthat our survey can provide insights for researchers and facilitate future\nresearch on this topic.\n",
                "publicationDate": "2020-10-04T10:51:22Z",
                "Link": "http://arxiv.org/pdf/2010.01535v1",
                "arxiv_id": "2010.01535v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Context Dependent Semantic Parsing: A Survey",
                "Authors": "Zhuang Li, Lizhen Qu, Gholamreza Haffari",
                "Abstract": "  Semantic parsing is the task of translating natural language utterances into\nmachine-readable meaning representations. Currently, most semantic parsing\nmethods are not able to utilize contextual information (e.g. dialogue and\ncomments history), which has a great potential to boost semantic parsing\nperformance. To address this issue, context dependent semantic parsing has\nrecently drawn a lot of attention. In this survey, we investigate progress on\nthe methods for the context dependent semantic parsing, together with the\ncurrent datasets and tasks. We then point out open problems and challenges for\nfuture research in this area. The collected resources for this topic are\navailable\nat:https://github.com/zhuang-li/Contextual-Semantic-Parsing-Paper-List.\n",
                "publicationDate": "2020-11-02T07:51:05Z",
                "Link": "http://arxiv.org/pdf/2011.00797v1",
                "arxiv_id": "2011.00797v1"
            },
            {
                "Title": "Semi-Supervised Methods for Out-of-Domain Dependency Parsing",
                "Authors": "Juntao Yu",
                "Abstract": "  Dependency parsing is one of the important natural language processing tasks\nthat assigns syntactic trees to texts. Due to the wider availability of\ndependency corpora and improved parsing and machine learning techniques,\nparsing accuracies of supervised learning-based systems have been significantly\nimproved. However, due to the nature of supervised learning, those parsing\nsystems highly rely on the manually annotated training corpora. They work\nreasonably good on the in-domain data but the performance drops significantly\nwhen tested on out-of-domain texts. To bridge the performance gap between\nin-domain and out-of-domain, this thesis investigates three semi-supervised\ntechniques for out-of-domain dependency parsing, namely co-training,\nself-training and dependency language models. Our approaches use easily\nobtainable unlabelled data to improve out-of-domain parsing accuracies without\nthe need of expensive corpora annotation. The evaluations on several English\ndomains and multi-lingual data show quite good improvements on parsing\naccuracy. Overall this work conducted a survey of semi-supervised methods for\nout-of-domain dependency parsing, where I extended and compared a number of\nimportant semi-supervised methods in a unified framework. The comparison\nbetween those techniques shows that self-training works equally well as\nco-training on out-of-domain parsing, while dependency language models can\nimprove both in- and out-of-domain accuracies.\n",
                "publicationDate": "2018-10-04T08:41:50Z",
                "Link": "http://arxiv.org/pdf/1810.02100v1",
                "arxiv_id": "1810.02100v1"
            },
            {
                "Title": "End-to-End Chinese Parsing Exploiting Lexicons",
                "Authors": "Yuan Zhang, Zhiyang Teng, Yue Zhang",
                "Abstract": "  Chinese parsing has traditionally been solved by three pipeline systems\nincluding word-segmentation, part-of-speech tagging and dependency parsing\nmodules. In this paper, we propose an end-to-end Chinese parsing model based on\ncharacter inputs which jointly learns to output word segmentation,\npart-of-speech tags and dependency structures. In particular, our parsing model\nrelies on word-char graph attention networks, which can enrich the character\ninputs with external word knowledge. Experiments on three Chinese parsing\nbenchmark datasets show the effectiveness of our models, achieving the\nstate-of-the-art results on end-to-end Chinese parsing.\n",
                "publicationDate": "2020-12-08T12:24:36Z",
                "Link": "http://arxiv.org/pdf/2012.04395v1",
                "arxiv_id": "2012.04395v1"
            },
            {
                "Title": "Do All Fragments Count?",
                "Authors": "Rens Bod",
                "Abstract": "  We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank. We isolate a number of dependency relations which\nprevious models neglect but which contribute to higher parse accuracy.\n",
                "publicationDate": "2000-11-24T23:51:21Z",
                "Link": "http://arxiv.org/pdf/cs/0011040v1",
                "arxiv_id": "0011040v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Error Analysis for Vietnamese Dependency Parsing",
                "Authors": "Kiet Van Nguyen, Ngan Luu-Thuy Nguyen",
                "Abstract": "  Dependency parsing is needed in different applications of natural language\nprocessing. In this paper, we present a thorough error analysis for dependency\nparsing for the Vietnamese language, using two state-of-the-art parsers:\nMSTParser and MaltParser. The error analysis results provide us insights in\norder to improve the performance of dependency parsing for the Vietnamese\nlanguage.\n",
                "publicationDate": "2019-11-09T16:00:26Z",
                "Link": "http://arxiv.org/pdf/1911.03724v1",
                "arxiv_id": "1911.03724v1"
            },
            {
                "Title": "Precision-biased Parsing and High-Quality Parse Selection",
                "Authors": "Yoav Goldberg, Michael Elhadad",
                "Abstract": "  We introduce precision-biased parsing: a parsing task which favors precision\nover recall by allowing the parser to abstain from decisions deemed uncertain.\nWe focus on dependency-parsing and present an ensemble method which is capable\nof assigning parents to 84% of the text tokens while being over 96% accurate on\nthese tokens. We use the precision-biased parsing task to solve the related\nhigh-quality parse-selection task: finding a subset of high-quality (accurate)\ntrees in a large collection of parsed text. We present a method for choosing\nover a third of the input trees while keeping unlabeled dependency parsing\naccuracy of 97% on these trees. We also present a method which is not based on\nan ensemble but rather on directly predicting the risk associated with\nindividual parser decisions. In addition to its efficiency, this method\ndemonstrates that a parsing system can provide reasonable estimates of\nconfidence in its predictions without relying on ensembles or aggregate corpus\ncounts.\n",
                "publicationDate": "2012-05-20T06:36:19Z",
                "Link": "http://arxiv.org/pdf/1205.4387v1",
                "arxiv_id": "1205.4387v1"
            },
            {
                "Title": "Sparse Fuzzy Attention for Structured Sentiment Analysis",
                "Authors": "Letian Peng, Zuchao Li, Hai Zhao",
                "Abstract": "  Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n",
                "publicationDate": "2021-09-14T14:37:56Z",
                "Link": "http://arxiv.org/pdf/2109.06719v3",
                "arxiv_id": "2109.06719v3"
            },
            {
                "Title": "Zero-shot Chinese Discourse Dependency Parsing via Cross-lingual Mapping",
                "Authors": "Yi Cheng, Sujian Li",
                "Abstract": "  Due to the absence of labeled data, discourse parsing still remains\nchallenging in some languages. In this paper, we present a simple and efficient\nmethod to conduct zero-shot Chinese text-level dependency parsing by leveraging\nEnglish discourse labeled data and parsing techniques. We first construct the\nChinese-English mapping from the level of sentence and elementary discourse\nunit (EDU), and then exploit the parsing results of the corresponding English\ntranslations to obtain the discourse trees for the Chinese text. This method\ncan automatically conduct Chinese discourse parsing, with no need of a large\nscale of Chinese labeled data.\n",
                "publicationDate": "2019-11-27T08:17:45Z",
                "Link": "http://arxiv.org/pdf/1911.12014v1",
                "arxiv_id": "1911.12014v1"
            },
            {
                "Title": "Span-Based Constituency Parsing with a Structure-Label System and\n  Provably Optimal Dynamic Oracles",
                "Authors": "James Cross, Liang Huang",
                "Abstract": "  Parsing accuracy using efficient greedy transition systems has improved\ndramatically in recent years thanks to neural networks. Despite striking\nresults in dependency parsing, however, neural models have not surpassed\nstate-of-the-art approaches in constituency parsing. To remedy this, we\nintroduce a new shift-reduce system whose stack contains merely sentence spans,\nrepresented by a bare minimum of LSTM features. We also design the first\nprovably optimal dynamic oracle for constituency parsing, which runs in\namortized O(1) time, compared to O(n^3) oracles for standard dependency\nparsing. Training with this oracle, we achieve the best F1 scores on both\nEnglish and French of any parser that does not use reranking or external data.\n",
                "publicationDate": "2016-12-20T01:23:00Z",
                "Link": "http://arxiv.org/pdf/1612.06475v1",
                "arxiv_id": "1612.06475v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "A Graph-based Model for Joint Chinese Word Segmentation and Dependency\n  Parsing",
                "Authors": "Hang Yan, Xipeng Qiu, Xuanjing Huang",
                "Abstract": "  Chinese word segmentation and dependency parsing are two fundamental tasks\nfor Chinese natural language processing. The dependency parsing is defined on\nword-level. Therefore word segmentation is the precondition of dependency\nparsing, which makes dependency parsing suffer from error propagation and\nunable to directly make use of the character-level pre-trained language model\n(such as BERT). In this paper, we propose a graph-based model to integrate\nChinese word segmentation and dependency parsing. Different from previous\ntransition-based joint models, our proposed model is more concise, which\nresults in fewer efforts of feature engineering. Our graph-based joint model\nachieves better performance than previous joint models and state-of-the-art\nresults in both Chinese word segmentation and dependency parsing. Besides, when\nBERT is combined, our model can substantially reduce the performance gap of\ndependency parsing between joint models and gold-segmented word-based models.\nOur code is publicly available at https://github.com/fastnlp/JointCwsParser.\n",
                "publicationDate": "2019-04-09T14:25:17Z",
                "Link": "http://arxiv.org/pdf/1904.04697v2",
                "arxiv_id": "1904.04697v2"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Multitask Parsing Across Semantic Representations",
                "Authors": "Daniel Hershcovich, Omri Abend, Ari Rappoport",
                "Abstract": "  The ability to consolidate information of different types is at the core of\nintelligence, and has tremendous practical value in allowing learning for one\ntask to benefit from generalizations learned for others. In this paper we\ntackle the challenging task of improving semantic parsing performance, taking\nUCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD)\nparsing as auxiliary tasks. We experiment on three languages, using a uniform\ntransition-based system and learning architecture for all parsing tasks.\nDespite notable conceptual, formal and domain differences, we show that\nmultitask learning significantly improves UCCA parsing in both in-domain and\nout-of-domain settings.\n",
                "publicationDate": "2018-05-01T12:21:50Z",
                "Link": "http://arxiv.org/pdf/1805.00287v1",
                "arxiv_id": "1805.00287v1"
            },
            {
                "Title": "Concurrent Parsing of Constituency and Dependency",
                "Authors": "Junru Zhou, Shuailiang Zhang, Hai Zhao",
                "Abstract": "  Constituent and dependency representation for syntactic structure share a lot\nof linguistic and computational characteristics, this paper thus makes the\nfirst attempt by introducing a new model that is capable of parsing constituent\nand dependency at the same time, so that lets either of the parsers enhance\neach other. Especially, we evaluate the effect of different shared network\ncomponents and empirically verify that dependency parsing may be much more\nbeneficial from constituent parsing structure.\n  The proposed parser achieves new state-of-the-art performance for both\nparsing tasks, constituent and dependency on PTB and CTB benchmarks.\n",
                "publicationDate": "2019-08-18T05:10:59Z",
                "Link": "http://arxiv.org/pdf/1908.06379v2",
                "arxiv_id": "1908.06379v2"
            },
            {
                "Title": "A Unifying Theory of Transition-based and Sequence Labeling Parsing",
                "Authors": "Carlos G\u00f3mez-Rodr\u00edguez, Michalina Strzyz, David Vilares",
                "Abstract": "  We define a mapping from transition-based parsing algorithms that read\nsentences from left to right to sequence labeling encodings of syntactic trees.\nThis not only establishes a theoretical relation between transition-based\nparsing and sequence-labeling parsing, but also provides a method to obtain new\nencodings for fast and simple sequence labeling parsing from the many existing\ntransition-based parsers for different formalisms. Applying it to dependency\nparsing, we implement sequence labeling versions of four algorithms, showing\nthat they are learnable and obtain comparable performance to existing\nencodings.\n",
                "publicationDate": "2020-11-01T18:25:15Z",
                "Link": "http://arxiv.org/pdf/2011.00584v1",
                "arxiv_id": "2011.00584v1"
            },
            {
                "Title": "Efficient probabilistic top-down and left-corner parsing",
                "Authors": "Brian Roark, Mark Johnson",
                "Abstract": "  This paper examines efficient predictive broad-coverage parsing without\ndynamic programming. In contrast to bottom-up methods, depth-first top-down\nparsing produces partial parses that are fully connected trees spanning the\nentire left context, from which any kind of non-local dependency or partial\nsemantic interpretation can in principle be read. We contrast two predictive\nparsing approaches, top-down and left-corner parsing, and find both to be\nviable. In addition, we find that enhancement with non-local information not\nonly improves parser accuracy, but also substantially improves the search\nefficiency.\n",
                "publicationDate": "2000-08-21T19:27:18Z",
                "Link": "http://arxiv.org/pdf/cs/0008017v1",
                "arxiv_id": "0008017v1"
            },
            {
                "Title": "Encoder-Decoder Shift-Reduce Syntactic Parsing",
                "Authors": "Jiangming Liu, Yue Zhang",
                "Abstract": "  Starting from NMT, encoder-decoder neu- ral networks have been used for many\nNLP problems. Graph-based models and transition-based models borrowing the en-\ncoder components achieve state-of-the-art performance on dependency parsing and\nconstituent parsing, respectively. How- ever, there has not been work\nempirically studying the encoder-decoder neural net- works for transition-based\nparsing. We apply a simple encoder-decoder to this end, achieving comparable\nresults to the parser of Dyer et al. (2015) on standard de- pendency parsing,\nand outperforming the parser of Vinyals et al. (2015) on con- stituent parsing.\n",
                "publicationDate": "2017-06-24T04:08:11Z",
                "Link": "http://arxiv.org/pdf/1706.07905v1",
                "arxiv_id": "1706.07905v1"
            },
            {
                "Title": "Lock-Free Parallel Perceptron for Graph-based Dependency Parsing",
                "Authors": "Xu Sun, Shuming Ma",
                "Abstract": "  Dependency parsing is an important NLP task. A popular approach for\ndependency parsing is structured perceptron. Still, graph-based dependency\nparsing has the time complexity of $O(n^3)$, and it suffers from slow training.\nTo deal with this problem, we propose a parallel algorithm called parallel\nperceptron. The parallel algorithm can make full use of a multi-core computer\nwhich saves a lot of training time. Based on experiments we observe that\ndependency parsing with parallel perceptron can achieve 8-fold faster training\nspeed than traditional structured perceptron methods when using 10 threads, and\nwith no loss at all in accuracy.\n",
                "publicationDate": "2017-03-02T13:49:23Z",
                "Link": "http://arxiv.org/pdf/1703.00782v1",
                "arxiv_id": "1703.00782v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "What is the minimal set of fragments that achieves maximal parse\n  accuracy?",
                "Authors": "Rens Bod",
                "Abstract": "  We aim at finding the minimal set of fragments which achieves maximal parse\naccuracy in Data Oriented Parsing. Experiments with the Penn Wall Street\nJournal treebank show that counts of almost arbitrary fragments within parse\ntrees are important, leading to improved parse accuracy over previous models\ntested on this treebank (a precision of 90.8% and a recall of 90.6%). We\nisolate some dependency relations which previous models neglect but which\ncontribute to higher parse accuracy.\n",
                "publicationDate": "2001-10-24T11:01:08Z",
                "Link": "http://arxiv.org/pdf/cs/0110050v1",
                "arxiv_id": "0110050v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Parsing All: Syntax and Semantics, Dependencies and Spans",
                "Authors": "Junru Zhou, Zuchao Li, Hai Zhao",
                "Abstract": "  Both syntactic and semantic structures are key linguistic contextual clues,\nin which parsing the latter has been well shown beneficial from parsing the\nformer. However, few works ever made an attempt to let semantic parsing help\nsyntactic parsing. As linguistic representation formalisms, both syntax and\nsemantics may be represented in either span (constituent/phrase) or dependency,\non both of which joint learning was also seldom explored. In this paper, we\npropose a novel joint model of syntactic and semantic parsing on both span and\ndependency representations, which incorporates syntactic information\neffectively in the encoder of neural network and benefits from two\nrepresentation formalisms in a uniform way. The experiments show that semantics\nand syntax can benefit each other by optimizing joint objectives. Our single\nmodel achieves new state-of-the-art or competitive results on both span and\ndependency semantic parsing on Propbank benchmarks and both dependency and\nconstituent syntactic parsing on Penn Treebank.\n",
                "publicationDate": "2019-08-30T03:49:19Z",
                "Link": "http://arxiv.org/pdf/1908.11522v3",
                "arxiv_id": "1908.11522v3"
            },
            {
                "Title": "Global Greedy Dependency Parsing",
                "Authors": "Zuchao Li, Hai Zhao, Kevin Parnow",
                "Abstract": "  Most syntactic dependency parsing models may fall into one of two categories:\ntransition- and graph-based models. The former models enjoy high inference\nefficiency with linear time complexity, but they rely on the stacking or\nre-ranking of partially-built parse trees to build a complete parse tree and\nare stuck with slower training for the necessity of dynamic oracle training.\nThe latter, graph-based models, may boast better performance but are\nunfortunately marred by polynomial time inference. In this paper, we propose a\nnovel parsing order objective, resulting in a novel dependency parsing model\ncapable of both global (in sentence scope) feature extraction as in graph\nmodels and linear time inference as in transitional models. The proposed global\ngreedy parser only uses two arc-building actions, left and right arcs, for\nprojective parsing. When equipped with two extra non-projective arc-building\nactions, the proposed parser may also smoothly support non-projective parsing.\nUsing multiple benchmark treebanks, including the Penn Treebank (PTB), the\nCoNLL-X treebanks, and the Universal Dependency Treebanks, we evaluate our\nparser and demonstrate that the proposed novel parser achieves good performance\nwith faster training and decoding.\n",
                "publicationDate": "2019-11-20T02:57:53Z",
                "Link": "http://arxiv.org/pdf/1911.08673v3",
                "arxiv_id": "1911.08673v3"
            },
            {
                "Title": "Memory-Based Semantic Parsing",
                "Authors": "Parag Jain, Mirella Lapata",
                "Abstract": "  We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.\n",
                "publicationDate": "2021-09-07T16:15:13Z",
                "Link": "http://arxiv.org/pdf/2110.07358v1",
                "arxiv_id": "2110.07358v1"
            },
            {
                "Title": "On the Challenges of Fully Incremental Neural Dependency Parsing",
                "Authors": "Ana Ezquerro, Carlos G\u00f3mez-Rodr\u00edguez, David Vilares",
                "Abstract": "  Since the popularization of BiLSTMs and Transformer-based bidirectional\nencoders, state-of-the-art syntactic parsers have lacked incrementality,\nrequiring access to the whole sentence and deviating from human language\nprocessing. This paper explores whether fully incremental dependency parsing\nwith modern architectures can be competitive. We build parsers combining\nstrictly left-to-right neural encoders with fully incremental sequence-labeling\nand transition-based decoders. The results show that fully incremental parsing\nwith modern architectures considerably lags behind bidirectional parsing,\nnoting the challenges of psycholinguistically plausible parsing.\n",
                "publicationDate": "2023-09-28T08:44:08Z",
                "Link": "http://arxiv.org/pdf/2309.16254v1",
                "arxiv_id": "2309.16254v1"
            },
            {
                "Title": "Fast semantic parsing with well-typedness guarantees",
                "Authors": "Matthias Lindemann, Jonas Groschwitz, Alexander Koller",
                "Abstract": "  AM dependency parsing is a linguistically principled method for neural\nsemantic parsing with high accuracy across multiple graphbanks. It relies on a\ntype system that models semantic valency but makes existing parsers slow. We\ndescribe an A* parser and a transition-based parser for AM dependency parsing\nwhich guarantee well-typedness and improve parsing speed by up to 3 orders of\nmagnitude, while maintaining or improving accuracy.\n",
                "publicationDate": "2020-09-15T21:54:01Z",
                "Link": "http://arxiv.org/pdf/2009.07365v2",
                "arxiv_id": "2009.07365v2"
            }
        ]
    },
    {
        "topic_name": "Topic Modeling",
        "summary": "default",
        "papers": [
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Vacaspati: A Diverse Corpus of Bangla Literature",
                "Authors": "Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya",
                "Abstract": "  Bangla (or Bengali) is the fifth most spoken language globally; yet, the\nstate-of-the-art NLP in Bangla is lagging for even simple tasks such as\nlemmatization, POS tagging, etc. This is partly due to lack of a varied quality\ncorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla\nliterature. The literary works are collected from various websites; only those\nworks that are publicly available without copyright violations or restrictions\nare collected. We believe that published literature captures the features of a\nlanguage much better than newspapers, blogs or social media posts which tend to\nfollow only a certain literary pattern and, therefore, miss out on language\nvariety. Our corpus Vacaspati is varied from multiple aspects, including type\nof composition, topic, author, time, space, etc. It contains more than 11\nmillion sentences and 115 million words. We also built a word embedding model,\nVac-FT, using FastText from Vacaspati as well as trained an Electra model,\nVac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only\na fraction of resources compared to other state-of-the-art transformer models\nand yet performs either better or similar on various downstream tasks. On\nmultiple downstream tasks, Vac-FT outperforms other FastText-based models. We\nalso demonstrate the efficacy of Vacaspati as a corpus by showing that similar\nmodels built from other corpora are not as effective. The models are available\nat https://bangla.iitk.ac.in/.\n",
                "publicationDate": "2023-07-11T07:32:12Z",
                "Link": "http://arxiv.org/pdf/2307.05083v1",
                "arxiv_id": "2307.05083v1"
            },
            {
                "Title": "Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition",
                "Authors": "Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  One of the major challenges for developing automatic speech recognition (ASR)\nfor low-resource languages is the limited access to labeled data with\ndomain-specific variations. In this study, we propose a pseudo-labeling\napproach to develop a large-scale domain-agnostic ASR dataset. With the\nproposed methodology, we developed a 20k+ hours labeled Bangla speech dataset\ncovering diverse topics, speaking styles, dialects, noisy environments, and\nconversational scenarios. We then exploited the developed corpus to design a\nconformer-based ASR system. We benchmarked the trained ASR with publicly\navailable datasets and compared it with other available models. To investigate\nthe efficacy, we designed and developed a human-annotated domain-agnostic test\nset composed of news, telephony, and conversational data among others. Our\nresults demonstrate the efficacy of the model trained on psuedo-label data for\nthe designed test-set along with publicly-available Bangla datasets. The\nexperimental resources will be publicly\navailable.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)\n",
                "publicationDate": "2023-11-06T15:37:14Z",
                "Link": "http://arxiv.org/pdf/2311.03196v1",
                "arxiv_id": "2311.03196v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "DPCSpell: A Transformer-based Detector-Purificator-Corrector Framework\n  for Spelling Error Correction of Bangla and Resource Scarce Indic Languages",
                "Authors": "Mehedi Hasan Bijoy, Nahid Hossain, Salekul Islam, Swakkhar Shatabda",
                "Abstract": "  Spelling error correction is the task of identifying and rectifying\nmisspelled words in texts. It is a potential and active research topic in\nNatural Language Processing because of numerous applications in human language\nunderstanding. The phonetically or visually similar yet semantically distinct\ncharacters make it an arduous task in any language. Earlier efforts on spelling\nerror correction in Bangla and resource-scarce Indic languages focused on\nrule-based, statistical, and machine learning-based methods which we found\nrather inefficient. In particular, machine learning-based approaches, which\nexhibit superior performance to rule-based and statistical methods, are\nineffective as they correct each character regardless of its appropriateness.\nIn this work, we propose a novel detector-purificator-corrector framework based\non denoising transformers by addressing previous issues. Moreover, we present a\nmethod for large-scale corpus creation from scratch which in turn resolves the\nresource limitation problem of any left-to-right scripted language. The\nempirical outcomes demonstrate the effectiveness of our approach that\noutperforms previous state-of-the-art methods by a significant margin for\nBangla spelling error correction. The models and corpus are publicly available\nat https://tinyurl.com/DPCSpell.\n",
                "publicationDate": "2022-11-07T17:59:05Z",
                "Link": "http://arxiv.org/pdf/2211.03730v1",
                "arxiv_id": "2211.03730v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Ensemble of Anchor-Free Models for Robust Bangla Document Layout\n  Segmentation",
                "Authors": "U Mong Sain Chak, Md. Asib Rahman",
                "Abstract": "  In this research paper, we introduce a novel approach designed for the\npurpose of segmenting the layout of Bangla documents. Our methodology involves\nthe utilization of a sophisticated ensemble of YOLOv8 models, which were\ntrained for the DL Sprint 2.0 - BUET CSE Fest 2023 Competition focused on\nBangla document layout segmentation. Our primary emphasis lies in enhancing\nvarious aspects of the task, including techniques such as image augmentation,\nmodel architecture, and the incorporation of model ensembles. We deliberately\nreduce the quality of a subset of document images to enhance the resilience of\nmodel training, thereby resulting in an improvement in our cross-validation\nscore. By employing Bayesian optimization, we determine the optimal confidence\nand Intersection over Union (IoU) thresholds for our model ensemble. Through\nour approach, we successfully demonstrate the effectiveness of anchor-free\nmodels in achieving robust layout segmentation in Bangla documents.\n",
                "publicationDate": "2023-08-28T08:24:25Z",
                "Link": "http://arxiv.org/pdf/2308.14397v2",
                "arxiv_id": "2308.14397v2"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models",
                "Authors": "Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman",
                "Abstract": "  Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.\n",
                "publicationDate": "2016-02-25T05:35:16Z",
                "Link": "http://arxiv.org/pdf/1602.07803v1",
                "arxiv_id": "1602.07803v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "publicationDate": "2024-03-31T09:52:25Z",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            }
        ]
    },
    {
        "topic_name": "Automatic Text Simplification",
        "summary": "default",
        "papers": [
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "Automatic Lexical Simplification for Turkish",
                "Authors": "Ahmet Yavuz Uluslu",
                "Abstract": "  In this paper, we present the first automatic lexical simplification system\nfor the Turkish language. Recent text simplification efforts rely on manually\ncrafted simplified corpora and comprehensive NLP tools that can analyse the\ntarget text both in word and sentence levels. Turkish is a morphologically rich\nagglutinative language that requires unique considerations such as the proper\nhandling of inflectional cases. Being a low-resource language in terms of\navailable resources and industrial-strength tools, it makes the text\nsimplification task harder to approach. We present a new text simplification\npipeline based on pretrained representation model BERT together with\nmorphological features to generate grammatically correct and semantically\nappropriate word-level simplifications.\n",
                "publicationDate": "2022-01-15T15:58:44Z",
                "Link": "http://arxiv.org/pdf/2201.05878v3",
                "arxiv_id": "2201.05878v3"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Semantic Structural Evaluation for Text Simplification",
                "Authors": "Elior Sulem, Omri Abend, Ari Rappoport",
                "Abstract": "  Current measures for evaluating text simplification systems focus on\nevaluating lexical text aspects, neglecting its structural aspects. In this\npaper we propose the first measure to address structural aspects of text\nsimplification, called SAMSA. It leverages recent advances in semantic parsing\nto assess simplification quality by decomposing the input based on its semantic\nstructure and comparing it to the output. SAMSA provides a reference-less\nautomatic evaluation procedure, avoiding the problems that reference-based\nmethods face due to the vast space of valid simplifications for a given\nsentence. Our human evaluation experiments show both SAMSA's substantial\ncorrelation with human judgments, as well as the deficiency of existing\nreference-based measures in evaluating structural simplification.\n",
                "publicationDate": "2018-10-11T13:54:50Z",
                "Link": "http://arxiv.org/pdf/1810.05022v1",
                "arxiv_id": "1810.05022v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "ARTIST: ARTificial Intelligence for Simplified Text",
                "Authors": "Lorenzo Corti, Jie Yang",
                "Abstract": "  Complex text is a major barrier for many citizens when accessing public\ninformation and knowledge. While often done manually, Text Simplification is a\nkey Natural Language Processing task that aims for reducing the linguistic\ncomplexity of a text while preserving the original meaning. Recent advances in\nGenerative Artificial Intelligence (AI) have enabled automatic text\nsimplification both on the lexical and syntactical levels. However, as\napplications often focus on English, little is understood about the\neffectiveness of Generative AI techniques on low-resource languages such as\nDutch. For this reason, we carry out empirical studies to understand the\nbenefits and limitations of applying generative technologies for text\nsimplification and provide the following outcomes: 1) the design and\nimplementation for a configurable text simplification pipeline that\norchestrates state-of-the-art generative text simplification models, domain and\nreader adaptation, and visualisation modules; 2) insights and lessons learned,\nshowing the strengths of automatic text simplification while exposing the\nchallenges in handling cultural and commonsense knowledge. These outcomes\nrepresent a first step in the exploration of Dutch text simplification and shed\nlight on future endeavours both for research and practice.\n",
                "publicationDate": "2023-08-25T16:06:06Z",
                "Link": "http://arxiv.org/pdf/2308.13458v1",
                "arxiv_id": "2308.13458v1"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "publicationDate": "2024-04-17T21:09:13Z",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Simple and Effective Text Simplification Using Semantic and Neural\n  Methods",
                "Authors": "Elior Sulem, Omri Abend, Ari Rappoport",
                "Abstract": "  Sentence splitting is a major simplification operator. Here we present a\nsimple and efficient splitting algorithm based on an automatic semantic parser.\nAfter splitting, the text is amenable for further fine-tuned simplification\noperations. In particular, we show that neural Machine Translation can be\neffectively used in this situation. Previous application of Machine Translation\nfor simplification suffers from a considerable disadvantage in that they are\nover-conservative, often failing to modify the source in any way. Splitting\nbased on semantic parsing, as proposed here, alleviates this issue. Extensive\nautomatic and human evaluation shows that the proposed method compares\nfavorably to the state-of-the-art in combined lexical and structural\nsimplification.\n",
                "publicationDate": "2018-10-11T16:14:24Z",
                "Link": "http://arxiv.org/pdf/1810.05104v1",
                "arxiv_id": "1810.05104v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "LENS: A Learnable Evaluation Metric for Text Simplification",
                "Authors": "Mounica Maddela, Yao Dou, David Heineman, Wei Xu",
                "Abstract": "  Training learnable metrics using modern language models has recently emerged\nas a promising method for the automatic evaluation of machine translation.\nHowever, existing human evaluation datasets for text simplification have\nlimited annotations that are based on unitary or outdated models, making them\nunsuitable for this approach. To address these issues, we introduce the\nSimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on\n2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging\nsimplification benchmark consisting of over 1K human ratings of 360\nsimplifications including GPT-3.5 generated text. Training on SimpEval, we\npresent LENS, a Learnable Evaluation Metric for Text Simplification. Extensive\nempirical results show that LENS correlates much better with human judgment\nthan existing metrics, paving the way for future progress in the evaluation of\ntext simplification. We also introduce Rank and Rate, a human evaluation\nframework that rates simplifications from several models in a list-wise manner\nusing an interactive interface, which ensures both consistency and accuracy in\nthe evaluation process and is used to create the SimpEval datasets.\n",
                "publicationDate": "2022-12-19T18:56:52Z",
                "Link": "http://arxiv.org/pdf/2212.09739v4",
                "arxiv_id": "2212.09739v4"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Exploring Automatic Text Simplification of German Narrative Documents",
                "Authors": "Thorben Schomacker, Tillmann D\u00f6nicke, Marina Tropmann-Frick",
                "Abstract": "  In this paper, we apply transformer-based Natural Language Generation (NLG)\ntechniques to the problem of text simplification. Currently, there are only a\nfew German datasets available for text simplification, even fewer with larger\nand aligned documents, and not a single one with narrative texts. In this\npaper, we explore to which degree modern NLG techniques can be applied to\nGerman narrative text simplifications. We use Longformer attention and a\npre-trained mBART model. Our findings indicate that the existing approaches for\nGerman are not able to solve the task properly. We conclude on a few directions\nfor future research to address this problem.\n",
                "publicationDate": "2023-12-15T16:10:44Z",
                "Link": "http://arxiv.org/pdf/2312.09907v1",
                "arxiv_id": "2312.09907v1"
            },
            {
                "Title": "Med-EASi: Finely Annotated Dataset and Models for Controllable\n  Simplification of Medical Texts",
                "Authors": "Chandrayee Basu, Rosni Vasu, Michihiro Yasunaga, Qian Yang",
                "Abstract": "  Automatic medical text simplification can assist providers with\npatient-friendly communication and make medical texts more accessible, thereby\nimproving health literacy. But curating a quality corpus for this task requires\nthe supervision of medical experts. In this work, we present\n$\\textbf{Med-EASi}$ ($\\underline{\\textbf{Med}}$ical dataset for\n$\\underline{\\textbf{E}}$laborative and $\\underline{\\textbf{A}}$bstractive\n$\\underline{\\textbf{Si}}$mplification), a uniquely crowdsourced and finely\nannotated dataset for supervised simplification of short medical texts. Its\n$\\textit{expert-layman-AI collaborative}$ annotations facilitate\n$\\textit{controllability}$ over text simplification by marking four kinds of\ntextual transformations: elaboration, replacement, deletion, and insertion. To\nlearn medical text simplification, we fine-tune T5-large with four different\nstyles of input-output combinations, leading to two control-free and two\ncontrollable versions of the model. We add two types of\n$\\textit{controllability}$ into text simplification, by using a multi-angle\ntraining approach: $\\textit{position-aware}$, which uses in-place annotated\ninputs and outputs, and $\\textit{position-agnostic}$, where the model only\nknows the contents to be edited, but not their positions. Our results show that\nour fine-grained annotations improve learning compared to the unannotated\nbaseline. Furthermore, $\\textit{position-aware}$ control generates better\nsimplification than the $\\textit{position-agnostic}$ one. The data and code are\navailable at https://github.com/Chandrayee/CTRL-SIMP.\n",
                "publicationDate": "2023-02-17T21:50:13Z",
                "Link": "http://arxiv.org/pdf/2302.09155v1",
                "arxiv_id": "2302.09155v1"
            },
            {
                "Title": "A Corpus for Automatic Readability Assessment and Text Simplification of\n  German",
                "Authors": "Alessia Battisti, Sarah Ebling",
                "Abstract": "  In this paper, we present a corpus for use in automatic readability\nassessment and automatic text simplification of German. The corpus is compiled\nfrom web sources and consists of approximately 211,000 sentences. As a novel\ncontribution, it contains information on text structure, typography, and\nimages, which can be exploited as part of machine learning approaches to\nreadability assessment and text simplification. The focus of this publication\nis on representing such information as an extension to an existing corpus\nstandard.\n",
                "publicationDate": "2019-09-19T16:07:32Z",
                "Link": "http://arxiv.org/pdf/1909.09067v1",
                "arxiv_id": "1909.09067v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform",
                "Authors": "Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar, Mita Nasipuri",
                "Abstract": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n",
                "publicationDate": "2020-09-17T03:14:27Z",
                "Link": "http://arxiv.org/pdf/2009.08037v1",
                "arxiv_id": "2009.08037v1"
            },
            {
                "Title": "Lexical Simplification Benchmarks for English, Portuguese, and Spanish",
                "Authors": "Sanja Stajner, Daniel Ferres, Matthew Shardlow, Kai North, Marcos Zampieri, Horacio Saggion",
                "Abstract": "  Even in highly-developed countries, as many as 15-30\\% of the population can\nonly understand texts written using a basic vocabulary. Their understanding of\neveryday texts is limited, which prevents them from taking an active role in\nsociety and making informed decisions regarding healthcare, legal\nrepresentation, or democratic choice. Lexical simplification is a natural\nlanguage processing task that aims to make text understandable to everyone by\nreplacing complex vocabulary and expressions with simpler ones, while\npreserving the original meaning. It has attracted considerable attention in the\nlast 20 years, and fully automatic lexical simplification systems have been\nproposed for various languages. The main obstacle for the progress of the field\nis the absence of high-quality datasets for building and evaluating lexical\nsimplification systems. We present a new benchmark dataset for lexical\nsimplification in English, Spanish, and (Brazilian) Portuguese, and provide\ndetails about data selection and annotation procedures. This is the first\ndataset that offers a direct comparison of lexical simplification systems for\nthree languages. To showcase the usability of the dataset, we adapt two\nstate-of-the-art lexical simplification systems with differing architectures\n(neural vs.\\ non-neural) to all three languages (English, Spanish, and\nBrazilian Portuguese) and evaluate their performances on our new dataset. For a\nfairer comparison, we use several evaluation measures which capture varied\naspects of the systems' efficacy, and discuss their strengths and weaknesses.\nWe find a state-of-the-art neural lexical simplification system outperforms a\nstate-of-the-art non-neural lexical simplification system in all three\nlanguages. More importantly, we find that the state-of-the-art neural lexical\nsimplification systems perform significantly better for English than for\nSpanish and Portuguese.\n",
                "publicationDate": "2022-09-12T15:06:26Z",
                "Link": "http://arxiv.org/pdf/2209.05301v1",
                "arxiv_id": "2209.05301v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Simple-QE: Better Automatic Quality Estimation for Text Simplification",
                "Authors": "Reno Kriz, Marianna Apidianaki, Chris Callison-Burch",
                "Abstract": "  Text simplification systems generate versions of texts that are easier to\nunderstand for a broader audience. The quality of simplified texts is generally\nestimated using metrics that compare to human references, which can be\ndifficult to obtain. We propose Simple-QE, a BERT-based quality estimation (QE)\nmodel adapted from prior summarization QE work, and show that it correlates\nwell with human quality judgments. Simple-QE does not require human references,\nwhich makes the model useful in a practical setting where users would need to\nbe informed about the quality of generated simplifications. We also show that\nwe can adapt this approach to accurately predict the complexity of\nhuman-written texts.\n",
                "publicationDate": "2020-12-22T22:02:37Z",
                "Link": "http://arxiv.org/pdf/2012.12382v1",
                "arxiv_id": "2012.12382v1"
            },
            {
                "Title": "ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification\n  Models with Multiple Rewriting Transformations",
                "Authors": "Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot, Lucia Specia",
                "Abstract": "  In order to simplify a sentence, human editors perform multiple rewriting\ntransformations: they split it into several shorter sentences, paraphrase words\n(i.e. replacing complex words or phrases by simpler synonyms), reorder\ncomponents, and/or delete information deemed unnecessary. Despite these varied\nrange of possible text alterations, current models for automatic sentence\nsimplification are evaluated using datasets that are focused on a single\ntransformation, such as lexical paraphrasing or splitting. This makes it\nimpossible to understand the ability of simplification models in more realistic\nsettings. To alleviate this limitation, this paper introduces ASSET, a new\ndataset for assessing sentence simplification in English. ASSET is a\ncrowdsourced multi-reference corpus where each simplification was produced by\nexecuting several rewriting transformations. Through quantitative and\nqualitative experiments, we show that simplifications in ASSET are better at\ncapturing characteristics of simplicity when compared to other standard\nevaluation datasets for the task. Furthermore, we motivate the need for\ndeveloping better methods for automatic evaluation using ASSET, since we show\nthat current popular metrics may not be suitable when multiple simplification\ntransformations are performed.\n",
                "publicationDate": "2020-05-01T16:44:54Z",
                "Link": "http://arxiv.org/pdf/2005.00481v1",
                "arxiv_id": "2005.00481v1"
            },
            {
                "Title": "Dancing Between Success and Failure: Edit-level Simplification\n  Evaluation using SALSA",
                "Authors": "David Heineman, Yao Dou, Mounica Maddela, Wei Xu",
                "Abstract": "  Large language models (e.g., GPT-4) are uniquely capable of producing highly\nrated text simplification, yet current human evaluation methods fail to provide\na clear understanding of systems' specific strengths and weaknesses. To address\nthis limitation, we introduce SALSA, an edit-based human annotation framework\nthat enables holistic and fine-grained text simplification evaluation. We\ndevelop twenty one linguistically grounded edit types, covering the full\nspectrum of success and failure across dimensions of conceptual, syntactic and\nlexical simplicity. Using SALSA, we collect 19K edit annotations on 840\nsimplifications, revealing discrepancies in the distribution of simplification\nstrategies performed by fine-tuned models, prompted LLMs and humans, and find\nGPT-3.5 performs more quality edits than humans, but still exhibits frequent\nerrors. Using our fine-grained annotations, we develop LENS-SALSA, a\nreference-free automatic simplification metric, trained to predict sentence-\nand word-level quality simultaneously. Additionally, we introduce word-level\nquality estimation for simplification and report promising baseline results.\nOur data, new metric, and annotation toolkit are available at\nhttps://salsa-eval.com.\n",
                "publicationDate": "2023-05-23T18:30:49Z",
                "Link": "http://arxiv.org/pdf/2305.14458v2",
                "arxiv_id": "2305.14458v2"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "DEPLAIN: A German Parallel Corpus with Intralingual Translations into\n  Plain Language for Sentence and Document Simplification",
                "Authors": "Regina Stodden, Omar Momen, Laura Kallmeyer",
                "Abstract": "  Text simplification is an intralingual translation task in which documents,\nor sentences of a complex source text are simplified for a target audience. The\nsuccess of automatic text simplification systems is highly dependent on the\nquality of parallel data used for training and evaluation. To advance sentence\nsimplification and document simplification in German, this paper presents\nDEplain, a new dataset of parallel, professionally written and manually aligned\nsimplifications in plain German (\"plain DE\" or in German: \"Einfache Sprache\").\nDEplain consists of a news domain (approx. 500 document pairs, approx. 13k\nsentence pairs) and a web-domain corpus (approx. 150 aligned documents, approx.\n2k aligned sentence pairs). In addition, we are building a web harvester and\nexperimenting with automatic alignment methods to facilitate the integration of\nnon-aligned and to be published parallel documents. Using this approach, we are\ndynamically increasing the web domain corpus, so it is currently extended to\napprox. 750 document pairs and approx. 3.5k aligned sentence pairs. We show\nthat using DEplain to train a transformer-based seq2seq text simplification\nmodel can achieve promising results. We make available the corpus, the adapted\nalignment methods for German, the web harvester and the trained models here:\nhttps://github.com/rstodden/DEPlain.\n",
                "publicationDate": "2023-05-30T11:07:46Z",
                "Link": "http://arxiv.org/pdf/2305.18939v1",
                "arxiv_id": "2305.18939v1"
            },
            {
                "Title": "Document-Level Text Simplification: Dataset, Criteria and Baseline",
                "Authors": "Renliang Sun, Hanqi Jin, Xiaojun Wan",
                "Abstract": "  Text simplification is a valuable technique. However, current research is\nlimited to sentence simplification. In this paper, we define and investigate a\nnew task of document-level text simplification, which aims to simplify a\ndocument consisting of multiple sentences. Based on Wikipedia dumps, we first\nconstruct a large-scale dataset named D-Wikipedia and perform analysis and\nhuman evaluation on it to show that the dataset is reliable. Then, we propose a\nnew automatic evaluation metric called D-SARI that is more suitable for the\ndocument-level simplification task. Finally, we select several representative\nmodels as baseline models for this task and perform automatic evaluation and\nhuman evaluation. We analyze the results and point out the shortcomings of the\nbaseline models.\n",
                "publicationDate": "2021-10-11T08:15:31Z",
                "Link": "http://arxiv.org/pdf/2110.05071v1",
                "arxiv_id": "2110.05071v1"
            },
            {
                "Title": "Digital Comprehensibility Assessment of Simplified Texts among Persons\n  with Intellectual Disabilities",
                "Authors": "Andreas S\u00e4uberli, Franz Holzknecht, Patrick Haller, Silvana Deilen, Laura Schiffl, Silvia Hansen-Schirra, Sarah Ebling",
                "Abstract": "  Text simplification refers to the process of increasing the comprehensibility\nof texts. Automatic text simplification models are most commonly evaluated by\nexperts or crowdworkers instead of the primary target groups of simplified\ntexts, such as persons with intellectual disabilities. We conducted an\nevaluation study of text comprehensibility including participants with and\nwithout intellectual disabilities reading unsimplified, automatically and\nmanually simplified German texts on a tablet computer. We explored four\ndifferent approaches to measuring comprehensibility: multiple-choice\ncomprehension questions, perceived difficulty ratings, response time, and\nreading speed. The results revealed significant variations in these\nmeasurements, depending on the reader group and whether the text had undergone\nautomatic or manual simplification. For the target group of persons with\nintellectual disabilities, comprehension questions emerged as the most reliable\nmeasure, while analyzing reading speed provided valuable insights into\nparticipants' reading behavior.\n",
                "publicationDate": "2024-02-20T15:37:08Z",
                "Link": "http://arxiv.org/pdf/2402.13094v1",
                "arxiv_id": "2402.13094v1"
            },
            {
                "Title": "Evaluating Factuality in Text Simplification",
                "Authors": "Ashwin Devaraj, William Sheffield, Byron C. Wallace, Junyi Jessy Li",
                "Abstract": "  Automated simplification models aim to make input texts more readable. Such\nmethods have the potential to make complex information accessible to a wider\naudience, e.g., providing access to recent medical literature which might\notherwise be impenetrable for a lay reader. However, such models risk\nintroducing errors into automatically simplified texts, for instance by\ninserting statements unsupported by the corresponding original text, or by\nomitting key information. Providing more readable but inaccurate versions of\ntexts may in many cases be worse than providing no such access at all. The\nproblem of factual accuracy (and the lack thereof) has received heightened\nattention in the context of summarization models, but the factuality of\nautomatically simplified texts has not been investigated. We introduce a\ntaxonomy of errors that we use to analyze both references drawn from standard\nsimplification datasets and state-of-the-art model outputs. We find that errors\noften appear in both that are not captured by existing evaluation metrics,\nmotivating a need for research into ensuring the factual accuracy of automated\nsimplification models.\n",
                "publicationDate": "2022-04-15T17:37:09Z",
                "Link": "http://arxiv.org/pdf/2204.07562v1",
                "arxiv_id": "2204.07562v1"
            },
            {
                "Title": "Neural CRF Model for Sentence Alignment in Text Simplification",
                "Authors": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu",
                "Abstract": "  The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.\n",
                "publicationDate": "2020-05-05T16:47:51Z",
                "Link": "http://arxiv.org/pdf/2005.02324v4",
                "arxiv_id": "2005.02324v4"
            },
            {
                "Title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages",
                "Authors": "Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu",
                "Abstract": "  Text simplification research has mostly focused on sentence-level\nsimplification, even though many desirable edits - such as adding relevant\nbackground information or reordering content - may require document-level\ncontext. Prior work has also predominantly framed simplification as a\nsingle-step, input-to-output task, only implicitly modeling the fine-grained,\nspan-level edits that elucidate the simplification process. To address both\ngaps, we introduce the SWiPE dataset, which reconstructs the document-level\nediting process from English Wikipedia (EW) articles to paired Simple Wikipedia\n(SEW) articles. In contrast to prior work, SWiPE leverages the entire revision\nhistory when pairing pages in order to better identify simplification edits. We\nwork with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling\nmore than 40,000 edits with proposed 19 categories. To scale our efforts, we\npropose several models to automatically label edits, achieving an F-1 score of\nup to 70.6, indicating that this is a tractable but challenging NLU task.\nFinally, we categorize the edits produced by several simplification models and\nfind that SWiPE-trained models generate more complex edits while reducing\nunwanted edits.\n",
                "publicationDate": "2023-05-30T16:52:42Z",
                "Link": "http://arxiv.org/pdf/2305.19204v1",
                "arxiv_id": "2305.19204v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Revisiting non-English Text Simplification: A Unified Multilingual\n  Benchmark",
                "Authors": "Michael J. Ryan, Tarek Naous, Wei Xu",
                "Abstract": "  Recent advancements in high-quality, large-scale English resources have\npushed the frontier of English Automatic Text Simplification (ATS) research.\nHowever, less work has been done on multilingual text simplification due to the\nlack of a diverse evaluation benchmark that covers complex-simple sentence\npairs in many languages. This paper introduces the MultiSim benchmark, a\ncollection of 27 resources in 12 distinct languages containing over 1.7 million\ncomplex-simple sentence pairs. This benchmark will encourage research in\ndeveloping more effective multilingual text simplification models and\nevaluation metrics. Our experiments using MultiSim with pre-trained\nmultilingual language models reveal exciting performance improvements from\nmultilingual training in non-English settings. We observe strong performance\nfrom Russian in zero-shot cross-lingual transfer to low-resource languages. We\nfurther show that few-shot prompting with BLOOM-176b achieves comparable\nquality to reference simplifications outperforming fine-tuned models in most\nlanguages. We validate these findings through human evaluation.\n",
                "publicationDate": "2023-05-25T03:03:29Z",
                "Link": "http://arxiv.org/pdf/2305.15678v1",
                "arxiv_id": "2305.15678v1"
            }
        ]
    },
    {
        "topic_name": "Speech Recognition",
        "summary": "default",
        "papers": [
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Investigating self-supervised, weakly supervised and fully supervised\n  training approaches for multi-domain automatic speech recognition: a study on\n  Bangladeshi Bangla",
                "Authors": "Ahnaf Mozib Samin, M. Humayon Kobir, Md. Mushtaq Shahriyar Rafee, M. Firoz Ahmed, Mehedi Hasan, Partha Ghosh, Shafkat Kibria, M. Shahidur Rahman",
                "Abstract": "  Despite huge improvements in automatic speech recognition (ASR) employing\nneural networks, ASR systems still suffer from a lack of robustness and\ngeneralizability issues due to domain shifting. This is mainly because\nprincipal corpus design criteria are often not identified and examined\nadequately while compiling ASR datasets. In this study, we investigate the\nrobustness of the state-of-the-art transfer learning approaches such as\nself-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully\nsupervised convolutional neural networks (CNNs) for multi-domain ASR. We also\ndemonstrate the significance of domain selection while building a corpus by\nassessing these models on a novel multi-domain Bangladeshi Bangla ASR\nevaluation benchmark - BanSpeech, which contains approximately 6.52 hours of\nhuman-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO,\na mostly read speech corpus for the morphologically rich language Bangla, has\nbeen used to train the ASR systems. Experimental evaluation reveals that\nself-supervised cross-lingual pre-training is the best strategy compared to\nweak supervision and full supervision to tackle the multi-domain ASR task.\nMoreover, the ASR models trained on SUBAK.KO face difficulty recognizing speech\nfrom domains with mostly spontaneous speech. The BanSpeech will be publicly\navailable to meet the need for a challenging evaluation benchmark for Bangla\nASR.\n",
                "publicationDate": "2022-10-24T02:18:03Z",
                "Link": "http://arxiv.org/pdf/2210.12921v3",
                "arxiv_id": "2210.12921v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?",
                "Authors": "Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda",
                "Abstract": "  This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.\n",
                "publicationDate": "2013-10-05T00:39:02Z",
                "Link": "http://arxiv.org/pdf/1310.1426v1",
                "arxiv_id": "1310.1426v1"
            },
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "publicationDate": "2023-12-21T16:52:41Z",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition",
                "Authors": "Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  One of the major challenges for developing automatic speech recognition (ASR)\nfor low-resource languages is the limited access to labeled data with\ndomain-specific variations. In this study, we propose a pseudo-labeling\napproach to develop a large-scale domain-agnostic ASR dataset. With the\nproposed methodology, we developed a 20k+ hours labeled Bangla speech dataset\ncovering diverse topics, speaking styles, dialects, noisy environments, and\nconversational scenarios. We then exploited the developed corpus to design a\nconformer-based ASR system. We benchmarked the trained ASR with publicly\navailable datasets and compared it with other available models. To investigate\nthe efficacy, we designed and developed a human-annotated domain-agnostic test\nset composed of news, telephony, and conversational data among others. Our\nresults demonstrate the efficacy of the model trained on psuedo-label data for\nthe designed test-set along with publicly-available Bangla datasets. The\nexperimental resources will be publicly\navailable.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)\n",
                "publicationDate": "2023-11-06T15:37:14Z",
                "Link": "http://arxiv.org/pdf/2311.03196v1",
                "arxiv_id": "2311.03196v1"
            },
            {
                "Title": "A Case Study on the Independence of Speech Emotion Recognition in Bangla\n  and English Languages using Language-Independent Prosodic Features",
                "Authors": "Fardin Saad, Hasan Mahmud, Mohammad Ridwan Kabir, Md. Alamin Shaheen, Paresha Farastu, Md. Kamrul Hasan",
                "Abstract": "  A language agnostic approach to recognizing emotions from speech remains an\nincomplete and challenging task. In this paper, we performed a step-by-step\ncomparative analysis of Speech Emotion Recognition (SER) using Bangla and\nEnglish languages to assess whether distinguishing emotions from speech is\nindependent of language. Six emotions were categorized for this study, such as\n- happy, angry, neutral, sad, disgust, and fear. We employed three Emotional\nSpeech Sets (ESS), of which the first two were developed by native Bengali\nspeakers in Bangla and English languages separately. The third was a subset of\nthe Toronto Emotional Speech Set (TESS), which was developed by native English\nspeakers from Canada. We carefully selected language-independent prosodic\nfeatures, adopted a Support Vector Machine (SVM) model, and conducted three\nexperiments to carry out our proposition. In the first experiment, we measured\nthe performance of the three speech sets individually, followed by the second\nexperiment, where different ESS pairs were integrated to analyze the impact on\nSER. Finally, we measured the recognition rate by training and testing the\nmodel with different speech sets in the third experiment. Although this study\nreveals that SER in Bangla and English languages is mostly\nlanguage-independent, some disparities were observed while recognizing\nemotional states like disgust and fear in these two languages. Moreover, our\ninvestigations revealed that non-native speakers convey emotions through\nspeech, much like expressing themselves in their native tongue.\n",
                "publicationDate": "2021-11-21T09:28:49Z",
                "Link": "http://arxiv.org/pdf/2111.10776v3",
                "arxiv_id": "2111.10776v3"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "A Comprehensive Survey on Bengali Phoneme Recognition",
                "Authors": "Sadia Tasnim Swarna, Shamim Ehsan, Md. Saiful Islam, Marium E Jannat",
                "Abstract": "  Hidden Markov model based various phoneme recognition methods for Bengali\nlanguage is reviewed. Automatic phoneme recognition for Bengali language using\nmultilayer neural network is reviewed. Usefulness of multilayer neural network\nover single layer neural network is discussed. Bangla phonetic feature table\nconstruction and enhancement for Bengali speech recognition is also discussed.\nComparison among these methods is discussed.\n",
                "publicationDate": "2017-01-27T12:38:47Z",
                "Link": "http://arxiv.org/pdf/1701.08156v2",
                "arxiv_id": "1701.08156v2"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "Image Pre-processing on NumtaDB for Bengali Handwritten Digit\n  Recognition",
                "Authors": "Ovi Paul",
                "Abstract": "  NumtaDB is by far the largest data-set collection for handwritten digits in\nBengali. This is a diverse dataset containing more than 85000 images. But this\ndiversity also makes this dataset very difficult to work with. The goal of this\npaper is to find the benchmark for pre-processed images which gives good\naccuracy on any machine learning models. The reason being, there are no\navailable pre-processed data for Bengali digit recognition to work with like\nthe English digits for MNIST.\n",
                "publicationDate": "2020-08-18T11:02:25Z",
                "Link": "http://arxiv.org/pdf/2008.07853v1",
                "arxiv_id": "2008.07853v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "publicationDate": "2021-08-01T17:16:03Z",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Ensemble of Anchor-Free Models for Robust Bangla Document Layout\n  Segmentation",
                "Authors": "U Mong Sain Chak, Md. Asib Rahman",
                "Abstract": "  In this research paper, we introduce a novel approach designed for the\npurpose of segmenting the layout of Bangla documents. Our methodology involves\nthe utilization of a sophisticated ensemble of YOLOv8 models, which were\ntrained for the DL Sprint 2.0 - BUET CSE Fest 2023 Competition focused on\nBangla document layout segmentation. Our primary emphasis lies in enhancing\nvarious aspects of the task, including techniques such as image augmentation,\nmodel architecture, and the incorporation of model ensembles. We deliberately\nreduce the quality of a subset of document images to enhance the resilience of\nmodel training, thereby resulting in an improvement in our cross-validation\nscore. By employing Bayesian optimization, we determine the optimal confidence\nand Intersection over Union (IoU) thresholds for our model ensemble. Through\nour approach, we successfully demonstrate the effectiveness of anchor-free\nmodels in achieving robust layout segmentation in Bangla documents.\n",
                "publicationDate": "2023-08-28T08:24:25Z",
                "Link": "http://arxiv.org/pdf/2308.14397v2",
                "arxiv_id": "2308.14397v2"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
                "Authors": "Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam",
                "Abstract": "  Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.\n",
                "publicationDate": "2024-01-22T18:52:51Z",
                "Link": "http://arxiv.org/pdf/2401.12210v1",
                "arxiv_id": "2401.12210v1"
            },
            {
                "Title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset",
                "Authors": "M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir",
                "Abstract": "  This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.\n",
                "publicationDate": "2020-11-15T11:08:53Z",
                "Link": "http://arxiv.org/pdf/2011.07499v3",
                "arxiv_id": "2011.07499v3"
            }
        ]
    },
    {
        "topic_name": "Error Detection and Correction",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "DPCSpell: A Transformer-based Detector-Purificator-Corrector Framework\n  for Spelling Error Correction of Bangla and Resource Scarce Indic Languages",
                "Authors": "Mehedi Hasan Bijoy, Nahid Hossain, Salekul Islam, Swakkhar Shatabda",
                "Abstract": "  Spelling error correction is the task of identifying and rectifying\nmisspelled words in texts. It is a potential and active research topic in\nNatural Language Processing because of numerous applications in human language\nunderstanding. The phonetically or visually similar yet semantically distinct\ncharacters make it an arduous task in any language. Earlier efforts on spelling\nerror correction in Bangla and resource-scarce Indic languages focused on\nrule-based, statistical, and machine learning-based methods which we found\nrather inefficient. In particular, machine learning-based approaches, which\nexhibit superior performance to rule-based and statistical methods, are\nineffective as they correct each character regardless of its appropriateness.\nIn this work, we propose a novel detector-purificator-corrector framework based\non denoising transformers by addressing previous issues. Moreover, we present a\nmethod for large-scale corpus creation from scratch which in turn resolves the\nresource limitation problem of any left-to-right scripted language. The\nempirical outcomes demonstrate the effectiveness of our approach that\noutperforms previous state-of-the-art methods by a significant margin for\nBangla spelling error correction. The models and corpus are publicly available\nat https://tinyurl.com/DPCSpell.\n",
                "publicationDate": "2022-11-07T17:59:05Z",
                "Link": "http://arxiv.org/pdf/2211.03730v1",
                "arxiv_id": "2211.03730v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "BanglaSarc: A Dataset for Sarcasm Detection",
                "Authors": "Tasnim Sakib Apon, Ramisa Anan, Elizabeth Antora Modhu, Arjun Suter, Ifrit Jamal Sneha, MD. Golam Rabiul Alam",
                "Abstract": "  Being one of the most widely spoken language in the world, the use of Bangla\nhas been increasing in the world of social media as well. Sarcasm is a positive\nstatement or remark with an underlying negative motivation that is extensively\nemployed in today's social media platforms. There has been a significant\nimprovement in sarcasm detection in English over the previous many years,\nhowever the situation regarding Bangla sarcasm detection remains unchanged. As\na result, it is still difficult to identify sarcasm in bangla, and a lack of\nhigh-quality data is a major contributing factor. This article proposes\nBanglaSarc, a dataset constructed specifically for bangla textual data sarcasm\ndetection. This dataset contains of 5112 comments/status and contents collected\nfrom various online social platforms such as Facebook, YouTube, along with a\nfew online blogs. Due to the limited amount of data collection of categorized\ncomments in Bengali, this dataset will aid in the of study identifying sarcasm,\nrecognizing people's emotion, detecting various types of Bengali expressions,\nand other domains. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/sakibapon/banglasarc.\n",
                "publicationDate": "2022-09-27T15:28:21Z",
                "Link": "http://arxiv.org/pdf/2209.13461v1",
                "arxiv_id": "2209.13461v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models",
                "Authors": "Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman",
                "Abstract": "  Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.\n",
                "publicationDate": "2016-02-25T05:35:16Z",
                "Link": "http://arxiv.org/pdf/1602.07803v1",
                "arxiv_id": "1602.07803v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "Approaches for Improving the Performance of Fake News Detection in\n  Bangla: Imbalance Handling and Model Stacking",
                "Authors": "Md Muzakker Hossain, Zahin Awosaf, Md. Salman Hossan Prottoy, Abu Saleh Muhammod Alvy, Md. Kishor Morol",
                "Abstract": "  Imbalanced datasets can lead to biasedness into the detection of fake news.\nIn this work, we present several strategies for resolving the imbalance issue\nfor fake news detection in Bangla with a comparative assessment of proposed\nmethodologies. Additionally, we propose a technique for improving performance\neven when the dataset is imbalanced. We applied our proposed approaches to\nBanFakeNews, a dataset developed for the purpose of detecting fake news in\nBangla comprising of 50K instances but is significantly skewed, with 97% of\nmajority instances. We obtained a 93.1% F1-score using data manipulation\nmanipulation techniques such as SMOTE, and a 79.1% F1-score using without data\nmanipulation approaches such as Stacked Generalization. Without implementing\nthese techniques, the F1-score would have been 67.6% for baseline models. We\nsee this work as an important step towards paving the way of fake news\ndetection in Bangla. By implementing these strategies the obstacles of\nimbalanced dataset can be removed and improvement in the performance can be\nachieved.\n",
                "publicationDate": "2022-03-22T06:33:01Z",
                "Link": "http://arxiv.org/pdf/2203.11486v1",
                "arxiv_id": "2203.11486v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent\n  Units and Deep Learning Techniques",
                "Authors": "Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan Talukder",
                "Abstract": "  The rise of fake news has made the need for effective detection methods,\nincluding in languages other than English, increasingly important. The study\naims to address the challenges of Bangla which is considered a less important\nlanguage. To this end, a complete dataset containing about 50,000 news items is\nproposed. Several deep learning models have been tested on this dataset,\nincluding the bidirectional gated recurrent unit (GRU), the long short-term\nmemory (LSTM), the 1D convolutional neural network (CNN), and hybrid\narchitectures. For this research, we assessed the efficacy of the model\nutilizing a range of useful measures, including recall, precision, F1 score,\nand accuracy. This was done by employing a big application. We carry out\ncomprehensive trials to show the effectiveness of these models in identifying\nbogus news in Bangla, with the Bidirectional GRU model having a stunning\naccuracy of 99.16%. Our analysis highlights the importance of dataset balance\nand the need for continual improvement efforts to a substantial degree. This\nstudy makes a major contribution to the creation of Bangla fake news detecting\nsystems with limited resources, thereby setting the stage for future\nimprovements in the detection process.\n",
                "publicationDate": "2024-03-31T09:52:25Z",
                "Link": "http://arxiv.org/pdf/2404.01345v1",
                "arxiv_id": "2404.01345v1"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "BanFakeNews: A Dataset for Detecting Fake News in Bangla",
                "Authors": "Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, Sudipta Kar",
                "Abstract": "  Observing the damages that can be done by the rapid propagation of fake news\nin various sectors like politics and finance, automatic identification of fake\nnews using linguistic analysis has drawn the attention of the research\ncommunity. However, such methods are largely being developed for English where\nlow resource languages remain out of the focus. But the risks spawned by fake\nand manipulative news are not confined by languages. In this work, we propose\nan annotated dataset of ~50K news that can be used for building automated fake\nnews detection systems for a low resource language like Bangla. Additionally,\nwe provide an analysis of the dataset and develop a benchmark system with state\nof the art NLP techniques to identify Bangla fake news. To create this system,\nwe explore traditional linguistic features and neural network based methods. We\nexpect this dataset will be a valuable resource for building technologies to\nprevent the spreading of fake news and contribute in research with low resource\nlanguages.\n",
                "publicationDate": "2020-04-19T07:42:22Z",
                "Link": "http://arxiv.org/pdf/2004.08789v1",
                "arxiv_id": "2004.08789v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "publicationDate": "2024-04-17T21:09:13Z",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BDSL 49: A Comprehensive Dataset of Bangla Sign Language",
                "Authors": "Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein",
                "Abstract": "  Language is a method by which individuals express their thoughts. Each\nlanguage has its own set of alphabetic and numeric characters. People can\ncommunicate with one another through either oral or written communication.\nHowever, each language has a sign language counterpart. Individuals who are\ndeaf and/or mute communicate through sign language. The Bangla language also\nhas a sign language, which is called BDSL. The dataset is about Bangla hand\nsign images. The collection contains 49 individual Bangla alphabet images in\nsign language. BDSL49 is a dataset that consists of 29,490 images with 49\nlabels. Images of 14 different adult individuals, each with a distinct\nbackground and appearance, have been recorded during data collection. Several\nstrategies have been used to eliminate noise from datasets during preparation.\nThis dataset is available to researchers for free. They can develop automated\nsystems using machine learning, computer vision, and deep learning techniques.\nIn addition, two models were used in this dataset. The first is for detection,\nwhile the second is for recognition.\n",
                "publicationDate": "2022-08-14T10:54:49Z",
                "Link": "http://arxiv.org/pdf/2208.06827v1",
                "arxiv_id": "2208.06827v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?",
                "Authors": "Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda",
                "Abstract": "  This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.\n",
                "publicationDate": "2013-10-05T00:39:02Z",
                "Link": "http://arxiv.org/pdf/1310.1426v1",
                "arxiv_id": "1310.1426v1"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            }
        ]
    },
    {
        "topic_name": "Named Entity Normalization",
        "summary": "default",
        "papers": [
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "LLM-RM at SemEval-2023 Task 2: Multilingual Complex NER using\n  XLM-RoBERTa",
                "Authors": "Rahul Mehta, Vasudeva Varma",
                "Abstract": "  Named Entity Recognition(NER) is a task of recognizing entities at a token\nlevel in a sentence. This paper focuses on solving NER tasks in a multilingual\nsetting for complex named entities. Our team, LLM-RM participated in the\nrecently organized SemEval 2023 task, Task 2: MultiCoNER II,Multilingual\nComplex Named Entity Recognition. We approach the problem by leveraging\ncross-lingual representation provided by fine-tuning XLM-Roberta base model on\ndatasets of all of the 12 languages provided -- Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian, Portuguese, Spanish, Swedish and\nUkrainian\n",
                "publicationDate": "2023-05-05T06:05:45Z",
                "Link": "http://arxiv.org/pdf/2305.03300v1",
                "arxiv_id": "2305.03300v1"
            },
            {
                "Title": "USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration\n  Network for Multilingual Complex Named Entity Recognition",
                "Authors": "Beiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-Hua Ling, Quan Liu",
                "Abstract": "  This paper describes the system developed by the USTC-NELSLIP team for\nSemEval-2022 Task 11 Multilingual Complex Named Entity Recognition\n(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to\nimprove the performance of language models for recognizing complex named\nentities. The method first adapts the representations of gazetteer networks to\nthose of language models by minimizing the KL divergence between them. After\nadaptation, these two networks are then integrated for backend supervised named\nentity recognition (NER) training. The proposed method is applied to several\nstate-of-the-art Transformer-based NER models with a gazetteer built from\nWikidata, and shows great generalization ability across them. The final\npredictions are derived from an ensemble of these trained models. Experimental\nresults and detailed analysis verify the effectiveness of the proposed method.\nThe official results show that our system ranked 1st on three tracks (Chinese,\nCode-mixed and Bangla) and 2nd on the other ten tracks in this task.\n",
                "publicationDate": "2022-03-07T09:05:37Z",
                "Link": "http://arxiv.org/pdf/2203.03216v2",
                "arxiv_id": "2203.03216v2"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "publicationDate": "2022-04-21T08:40:17Z",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "Named Entity Normalization Model Using Edge Weight Updating Neural\n  Network: Assimilation Between Knowledge-Driven Graph and Data-Driven Graph",
                "Authors": "Sung Hwan Jeon, Sungzoon Cho",
                "Abstract": "  Discriminating the matched named entity pairs or identifying the entities'\ncanonical forms are critical in text mining tasks. More precise named entity\nnormalization in text mining will benefit other subsequent text analytic\napplications. We built the named entity normalization model with a novel Edge\nWeight Updating Neural Network. Our proposed model when tested on four\ndifferent datasets achieved state-of-the-art results. We, next, verify our\nmodel's performance on NCBI Disease, BC5CDR Disease, and BC5CDR Chemical\ndatabases, which are widely used named entity normalization datasets in the\nbioinformatics field. We also tested our model with our own financial named\nentity normalization dataset to validate the efficacy for more general\napplications. Using the constructed dataset, we differentiate named entity\npairs. Our model achieved the highest named entity normalization performances\nin terms of various evaluation metrics.\n",
                "publicationDate": "2021-06-14T16:14:58Z",
                "Link": "http://arxiv.org/pdf/2106.07549v1",
                "arxiv_id": "2106.07549v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition\n  (MultiCoNER 2)",
                "Authors": "Besnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg Rokhlenko, Shervin Malmasi",
                "Abstract": "  We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual\nNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task\nfocused on methods to identify complex fine-grained named entities (like\nWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and\nmultilingual scenarios, as well as noisy settings. The task used the MultiCoNER\nV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,\nFarsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and\nUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It\nattracted 842 submissions from 47 teams, and 34 teams submitted system papers.\nResults showed that complex entity types such as media titles and product names\nwere the most challenging. Methods fusing external knowledge into transformer\nmodels achieved the best performance, and the largest gains were on the\nCreative Work and Group classes, which are still challenging even with external\nknowledge. Some fine-grained classes proved to be more challenging than others,\nsuch as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy data\nhas a significant impact on model performance, with an average drop of 10% on\nthe noisy subset. The task highlights the need for future research on improving\nNER robustness on noisy data containing complex entities.\n",
                "publicationDate": "2023-05-11T05:56:08Z",
                "Link": "http://arxiv.org/pdf/2305.06586v2",
                "arxiv_id": "2305.06586v2"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition",
                "Authors": "Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen",
                "Abstract": "  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n",
                "publicationDate": "2024-03-26T12:27:32Z",
                "Link": "http://arxiv.org/pdf/2403.17645v3",
                "arxiv_id": "2403.17645v3"
            },
            {
                "Title": "Chemical Identification and Indexing in PubMed Articles via BERT and\n  Text-to-Text Approaches",
                "Authors": "Virginia Adams, Hoo-Chang Shin, Carol Anderson, Bo Liu, Anas Abidin",
                "Abstract": "  The Biocreative VII Track-2 challenge consists of named entity recognition,\nentity-linking (or entity-normalization), and topic indexing tasks -- with\nentities and topics limited to chemicals for this challenge. Named entity\nrecognition is a well-established problem and we achieve our best performance\nwith BERT-based BioMegatron models. We extend our BERT-based approach to the\nentity linking task. After the second stage of pretraining BioBERT with a\nmetric-learning loss strategy called self-alignment pretraining (SAP), we link\nentities based on the cosine similarity between their SAP-BioBERT word\nembeddings. Despite the success of our named entity recognition experiments, we\nfind the chemical indexing task generally more challenging.\n  In addition to conventional NER methods, we attempt both named entity\nrecognition and entity linking with a novel text-to-text or \"prompt\" based\nmethod that uses generative language models such as T5 and GPT. We achieve\nencouraging results with this new approach.\n",
                "publicationDate": "2021-11-30T18:21:06Z",
                "Link": "http://arxiv.org/pdf/2111.15622v1",
                "arxiv_id": "2111.15622v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Learning Structured Representations of Entity Names using Active\n  Learning and Weak Supervision",
                "Authors": "Kun Qian, Poornima Chozhiyath Raman, Yunyao Li, Lucian Popa",
                "Abstract": "  Structured representations of entity names are useful for many entity-related\ntasks such as entity normalization and variant generation. Learning the\nimplicit structured representations of entity names without context and\nexternal knowledge is particularly challenging. In this paper, we present a\nnovel learning framework that combines active learning and weak supervision to\nsolve this problem. Our experimental evaluation show that this framework\nenables the learning of high-quality models from merely a dozen or so labeled\nexamples.\n",
                "publicationDate": "2020-10-30T21:01:22Z",
                "Link": "http://arxiv.org/pdf/2011.00105v1",
                "arxiv_id": "2011.00105v1"
            },
            {
                "Title": "Named Entity Sequence Classification",
                "Authors": "Mahdi Namazifar",
                "Abstract": "  Named Entity Recognition (NER) aims at locating and classifying named\nentities in text. In some use cases of NER, including cases where detected\nnamed entities are used in creating content recommendations, it is crucial to\nhave a reliable confidence level for the detected named entities. In this work\nwe study the problem of finding confidence levels for detected named entities.\nWe refer to this problem as Named Entity Sequence Classification (NESC). We\nframe NESC as a binary classification problem and we use NER as well as\nrecurrent neural networks to find the probability of candidate named entity is\na real named entity. We apply this approach to Tweet texts and we show how we\ncould find named entities with high confidence levels from Tweets.\n",
                "publicationDate": "2017-12-06T18:33:55Z",
                "Link": "http://arxiv.org/pdf/1712.02316v1",
                "arxiv_id": "1712.02316v1"
            },
            {
                "Title": "Open Named Entity Modeling from Embedding Distribution",
                "Authors": "Ying Luo, Hai Zhao, Zhuosheng Zhang, Bingjie Tang",
                "Abstract": "  In this paper, we report our discovery on named entity distribution in a\ngeneral word embedding space, which helps an open definition on multilingual\nnamed entity definition rather than previous closed and constraint definition\non named entities through a named entity dictionary, which is usually derived\nfrom human labor and replies on schedule update. Our initial visualization of\nmonolingual word embeddings indicates named entities tend to gather together\ndespite of named entity types and language difference, which enable us to model\nall named entities using a specific geometric structure inside embedding space,\nnamely, the named entity hypersphere. For monolingual cases, the proposed named\nentity model gives an open description of diverse named entity types and\ndifferent languages. For cross-lingual cases, mapping the proposed named entity\nmodel provides a novel way to build a named entity dataset for resource-poor\nlanguages. At last, the proposed named entity model may be shown as a handy\nclue to enhance state-of-the-art named entity recognition systems generally.\n",
                "publicationDate": "2019-08-31T08:56:46Z",
                "Link": "http://arxiv.org/pdf/1909.00170v2",
                "arxiv_id": "1909.00170v2"
            },
            {
                "Title": "Linguistically Informed Relation Extraction and Neural Architectures for\n  Nested Named Entity Recognition in BioNLP-OST 2019",
                "Authors": "Usama Yaseen, Pankaj Gupta, Hinrich Sch\u00fctze",
                "Abstract": "  Named Entity Recognition (NER) and Relation Extraction (RE) are essential\ntools in distilling knowledge from biomedical literature. This paper presents\nour findings from participating in BioNLP Shared Tasks 2019. We addressed Named\nEntity Recognition including nested entities extraction, Entity Normalization\nand Relation Extraction. Our proposed approach of Named Entities can be\ngeneralized to different languages and we have shown it's effectiveness for\nEnglish and Spanish text. We investigated linguistic features, hybrid loss\nincluding ranking and Conditional Random Fields (CRF), multi-task objective and\ntoken-level ensembling strategy to improve NER. We employed dictionary based\nfuzzy and semantic search to perform Entity Normalization. Finally, our RE\nsystem employed Support Vector Machine (SVM) with linguistic features.\n  Our NER submission (team:MIC-CIS) ranked first in BB-2019 norm+NER task with\nstandard error rate (SER) of 0.7159 and showed competitive performance on\nPharmaCo NER task with F1-score of 0.8662. Our RE system ranked first in the\nSeeDev-binary Relation Extraction Task with F1-score of 0.3738.\n",
                "publicationDate": "2019-10-08T13:33:48Z",
                "Link": "http://arxiv.org/pdf/1910.03385v1",
                "arxiv_id": "1910.03385v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset",
                "Authors": "M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir",
                "Abstract": "  This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.\n",
                "publicationDate": "2020-11-15T11:08:53Z",
                "Link": "http://arxiv.org/pdf/2011.07499v3",
                "arxiv_id": "2011.07499v3"
            },
            {
                "Title": "Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to\n  Rank",
                "Authors": "Zainab Awan, Tim Kahlke, Peter Ralph, Paul Kennedy",
                "Abstract": "  Motivation: Biomedical named-entity normalization involves connecting\nbiomedical entities with distinct database identifiers in order to facilitate\ndata integration across various fields of biology. Existing systems for\nbiomedical named entity normalization heavily rely on dictionaries, manually\ncreated rules, and high-quality representative features such as lexical or\nmorphological characteristics. However, recent research has investigated the\nuse of neural network-based models to reduce dependence on dictionaries,\nmanually crafted rules, and features. Despite these advancements, the\nperformance of these models is still limited due to the lack of sufficiently\nlarge training datasets. These models have a tendency to overfit small training\ncorpora and exhibit poor generalization when faced with previously unseen\nentities, necessitating the redesign of rules and features. Contribution: We\npresent a novel deep learning approach for named entity normalization, treating\nit as a pair-wise learning to rank problem. Our method utilizes the widely-used\ninformation retrieval algorithm Best Matching 25 to generate candidate\nconcepts, followed by the application of bi-directional encoder representation\nfrom the encoder (BERT) to re-rank the candidate list. Notably, our approach\neliminates the need for feature-engineering or rule creation. We conduct\nexperiments on species entity types and evaluate our method against\nstate-of-the-art techniques using LINNAEUS and S800 biomedical corpora. Our\nproposed approach surpasses existing methods in linking entities to the NCBI\ntaxonomy. To the best of our knowledge, there is no existing neural\nnetwork-based approach for species normalization in the literature.\n",
                "publicationDate": "2023-10-22T17:30:16Z",
                "Link": "http://arxiv.org/pdf/2310.14366v1",
                "arxiv_id": "2310.14366v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "An Entity-based Claim Extraction Pipeline for Real-world Biomedical\n  Fact-checking",
                "Authors": "Amelie W\u00fchrl, Lara Grimminger, Roman Klinger",
                "Abstract": "  Existing fact-checking models for biomedical claims are typically trained on\nsynthetic or well-worded data and hardly transfer to social media content. This\nmismatch can be mitigated by adapting the social media input to mimic the\nfocused nature of common training claims. To do so, Wuehrl & Klinger (2022)\npropose to extract concise claims based on medical entities in the text.\nHowever, their study has two limitations: First, it relies on gold-annotated\nentities. Therefore, its feasibility for a real-world application cannot be\nassessed since this requires detecting relevant entities automatically. Second,\nthey represent claim entities with the original tokens. This constitutes a\nterminology mismatch which potentially limits the fact-checking performance. To\nunderstand both challenges, we propose a claim extraction pipeline for medical\ntweets that incorporates named entity recognition and terminology normalization\nvia entity linking. We show that automatic NER does lead to a performance drop\nin comparison to using gold annotations but the fact-checking performance still\nimproves considerably over inputting the unchanged tweets. Normalizing entities\nto their canonical forms does, however, not improve the performance.\n",
                "publicationDate": "2023-04-11T15:07:24Z",
                "Link": "http://arxiv.org/pdf/2304.05268v1",
                "arxiv_id": "2304.05268v1"
            },
            {
                "Title": "BERN2: an advanced neural biomedical named entity recognition and\n  normalization tool",
                "Authors": "Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, Jaewoo Kang",
                "Abstract": "  In biomedical natural language processing, named entity recognition (NER) and\nnamed entity normalization (NEN) are key tasks that enable the automatic\nextraction of biomedical entities (e.g. diseases and drugs) from the\never-growing biomedical literature. In this article, we present BERN2 (Advanced\nBiomedical Entity Recognition and Normalization), a tool that improves the\nprevious neural network-based NER tool by employing a multi-task NER model and\nneural network-based NEN models to achieve much faster and more accurate\ninference. We hope that our tool can help annotate large-scale biomedical texts\nfor various tasks such as biomedical knowledge graph construction.\n",
                "publicationDate": "2022-01-06T14:53:30Z",
                "Link": "http://arxiv.org/pdf/2201.02080v3",
                "arxiv_id": "2201.02080v3"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Experiments to Improve Named Entity Recognition on Turkish Tweets",
                "Authors": "Dilek K\u00fc\u00e7\u00fck, Ralf Steinberger",
                "Abstract": "  Social media texts are significant information sources for several\napplication areas including trend analysis, event monitoring, and opinion\nmining. Unfortunately, existing solutions for tasks such as named entity\nrecognition that perform well on formal texts usually perform poorly when\napplied to social media texts. In this paper, we report on experiments that\nhave the purpose of improving named entity recognition on Turkish tweets, using\ntwo different annotated data sets. In these experiments, starting with a\nbaseline named entity recognition system, we adapt its recognition rules and\nresources to better fit Twitter language by relaxing its capitalization\nconstraint and by diacritics-based expansion of its lexical resources, and we\nemploy a simplistic normalization scheme on tweets to observe the effects of\nthese on the overall named entity recognition performance on Turkish tweets.\nThe evaluation results of the system with these different settings are provided\nwith discussions of these results.\n",
                "publicationDate": "2014-10-31T08:35:55Z",
                "Link": "http://arxiv.org/pdf/1410.8668v1",
                "arxiv_id": "1410.8668v1"
            },
            {
                "Title": "Distantly supervised end-to-end medical entity extraction from\n  electronic health records with human-level quality",
                "Authors": "Alexander Nesterov, Dmitry Umerenkov",
                "Abstract": "  Medical entity extraction (EE) is a standard procedure used as a first stage\nin medical texts processing. Usually Medical EE is a two-step process: named\nentity recognition (NER) and named entity normalization (NEN). We propose a\nnovel method of doing medical EE from electronic health records (EHR) as a\nsingle-step multi-label classification task by fine-tuning a transformer model\npretrained on a large EHR dataset. Our model is trained end-to-end in an\ndistantly supervised manner using targets automatically extracted from medical\nknowledge base. We show that our model learns to generalize for entities that\nare present frequently enough, achieving human-level classification quality for\nmost frequent entities. Our work demonstrates that medical entity extraction\ncan be done end-to-end without human supervision and with human quality given\nthe availability of a large enough amount of unlabeled EHR and a medical\nknowledge base.\n",
                "publicationDate": "2022-01-25T17:04:46Z",
                "Link": "http://arxiv.org/pdf/2201.10463v1",
                "arxiv_id": "2201.10463v1"
            },
            {
                "Title": "Biomedical Entity Representations with Synonym Marginalization",
                "Authors": "Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, Jaewoo Kang",
                "Abstract": "  Biomedical named entities often play important roles in many biomedical text\nmining tools. However, due to the incompleteness of provided synonyms and\nnumerous variations in their surface forms, normalization of biomedical\nentities is very challenging. In this paper, we focus on learning\nrepresentations of biomedical entities solely based on the synonyms of\nentities. To learn from the incomplete synonyms, we use a model-based candidate\nselection and maximize the marginal likelihood of the synonyms present in top\ncandidates. Our model-based candidates are iteratively updated to contain more\ndifficult negative samples as our model evolves. In this way, we avoid the\nexplicit pre-selection of negative samples from more than 400K candidates. On\nfour biomedical entity normalization datasets having three different entity\ntypes (disease, chemical, adverse reaction), our model BioSyn consistently\noutperforms previous state-of-the-art models almost reaching the upper bound on\neach dataset.\n",
                "publicationDate": "2020-05-01T06:20:36Z",
                "Link": "http://arxiv.org/pdf/2005.00239v1",
                "arxiv_id": "2005.00239v1"
            },
            {
                "Title": "How much is Wikipedia Lagging Behind News?",
                "Authors": "Besnik Fetahu, Abhijit Anand, Avishek Anand",
                "Abstract": "  Wikipedia, rich in entities and events, is an invaluable resource for various\nknowledge harvesting, extraction and mining tasks. Numerous resources like\nDBpedia, YAGO and other knowledge bases are based on extracting entity and\nevent based knowledge from it. Online news, on the other hand, is an\nauthoritative and rich source for emerging entities, events and facts relating\nto existing entities. In this work, we study the creation of entities in\nWikipedia with respect to news by studying how entity and event based\ninformation flows from news to Wikipedia.\n  We analyze the lag of Wikipedia (based on the revision history of the English\nWikipedia) with 20 years of \\emph{The New York Times} dataset (NYT). We model\nand analyze the lag of entities and events, namely their first appearance in\nWikipedia and in NYT, respectively. In our extensive experimental analysis, we\nfind that almost 20\\% of the external references in entity pages are news\narticles encoding the importance of news to Wikipedia. Second, we observe that\nthe entity-based lag follows a normal distribution with a high standard\ndeviation, whereas the lag for news-based events is typically very low.\nFinally, we find that events are responsible for creation of emergent entities\nwith as many as 12\\% of the entities mentioned in the event page are created\nafter the creation of the event page.\n",
                "publicationDate": "2017-03-30T08:05:17Z",
                "Link": "http://arxiv.org/pdf/1703.10345v1",
                "arxiv_id": "1703.10345v1"
            },
            {
                "Title": "Named Entity Inclusion in Abstractive Text Summarization",
                "Authors": "Sergey Berezin, Tatiana Batura",
                "Abstract": "  We address the named entity omission - the drawback of many current\nabstractive text summarizers. We suggest a custom pretraining objective to\nenhance the model's attention on the named entities in a text. At first, the\nnamed entity recognition model RoBERTa is trained to determine named entities\nin the text. After that, this model is used to mask named entities in the text\nand the BART model is trained to reconstruct them. Next, the BART model is\nfine-tuned on the summarization task. Our experiments showed that this\npretraining approach improves named entity inclusion precision and recall\nmetrics.\n",
                "publicationDate": "2023-07-05T18:12:31Z",
                "Link": "http://arxiv.org/pdf/2307.02570v1",
                "arxiv_id": "2307.02570v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "Named Entity Recognition in Indian court judgments",
                "Authors": "Prathamesh Kalamkar, Astha Agarwal, Aman Tiwari, Smita Gupta, Saurabh Karn, Vivek Raghavan",
                "Abstract": "  Identification of named entities from legal texts is an essential building\nblock for developing other legal Artificial Intelligence applications. Named\nEntities in legal texts are slightly different and more fine-grained than\ncommonly used named entities like Person, Organization, Location etc. In this\npaper, we introduce a new corpus of 46545 annotated legal named entities mapped\nto 14 legal entity types. The Baseline model for extracting legal named\nentities from judgment text is also developed.\n",
                "publicationDate": "2022-11-07T10:44:44Z",
                "Link": "http://arxiv.org/pdf/2211.03442v1",
                "arxiv_id": "2211.03442v1"
            },
            {
                "Title": "DaN+: Danish Nested Named Entities and Lexical Normalization",
                "Authors": "Barbara Plank, Kristian N\u00f8rgaard Jensen, Rob van der Goot",
                "Abstract": "  This paper introduces DaN+, a new multi-domain corpus and annotation\nguidelines for Danish nested named entities (NEs) and lexical normalization to\nsupport research on cross-lingual cross-domain learning for a less-resourced\nlanguage. We empirically assess three strategies to model the two-layer Named\nEntity Recognition (NER) task. We compare transfer capabilities from German\nversus in-language annotation from scratch. We examine language-specific versus\nmultilingual BERT, and study the effect of lexical normalization on NER. Our\nresults show that 1) the most robust strategy is multi-task learning which is\nrivaled by multi-label decoding, 2) BERT-based NER models are sensitive to\ndomain shifts, and 3) in-language BERT and lexical normalization are the most\nbeneficial on the least canonical data. Our results also show that an\nout-of-domain setup remains challenging, while performance on news plateaus\nquickly. This highlights the importance of cross-domain evaluation of\ncross-lingual transfer.\n",
                "publicationDate": "2021-05-24T14:35:21Z",
                "Link": "http://arxiv.org/pdf/2105.11301v1",
                "arxiv_id": "2105.11301v1"
            },
            {
                "Title": "Semi-supervised Bootstrapping approach for Named Entity Recognition",
                "Authors": "S. Thenmalar, J. Balaji, T. V. Geetha",
                "Abstract": "  The aim of Named Entity Recognition (NER) is to identify references of named\nentities in unstructured documents, and to classify them into pre-defined\nsemantic categories. NER often aids from added background knowledge in the form\nof gazetteers. However using such a collection does not deal with name variants\nand cannot resolve ambiguities associated in identifying the entities in\ncontext and associating them with predefined categories. We present a\nsemi-supervised NER approach that starts with identifying named entities with a\nsmall set of training data. Using the identified named entities, the word and\nthe context features are used to define the pattern. This pattern of each named\nentity category is used as a seed pattern to identify the named entities in the\ntest set. Pattern scoring and tuple value score enables the generation of the\nnew patterns to identify the named entity categories. We have evaluated the\nproposed system for English language with the dataset of tagged (IEER) and\nuntagged (CoNLL 2003) named entity corpus and for Tamil language with the\ndocuments from the FIRE corpus and yield an average f-measure of 75% for both\nthe languages.\n",
                "publicationDate": "2015-11-21T04:11:44Z",
                "Link": "http://arxiv.org/pdf/1511.06833v1",
                "arxiv_id": "1511.06833v1"
            },
            {
                "Title": "Sparse Named Entity Classification using Factorization Machines",
                "Authors": "Ai Hirata, Mamoru Komachi",
                "Abstract": "  Named entity classification is the task of classifying text-based elements\ninto various categories, including places, names, dates, times, and monetary\nvalues. A bottleneck in named entity classification, however, is the data\nproblem of sparseness, because new named entities continually emerge, making it\nrather difficult to maintain a dictionary for named entity classification.\nThus, in this paper, we address the problem of named entity classification\nusing matrix factorization to overcome the problem of feature sparsity.\nExperimental results show that our proposed model, with fewer features and a\nsmaller size, achieves competitive accuracy to state-of-the-art models.\n",
                "publicationDate": "2017-03-15T01:54:52Z",
                "Link": "http://arxiv.org/pdf/1703.04879v1",
                "arxiv_id": "1703.04879v1"
            },
            {
                "Title": "BdSLW60: A Word-Level Bangla Sign Language Dataset",
                "Authors": "Husne Ara Rubaiyeat, Hasan Mahmud, Ahsan Habib, Md. Kamrul Hasan",
                "Abstract": "  Sign language discourse is an essential mode of daily communication for the\ndeaf and hard-of-hearing people. However, research on Bangla Sign Language\n(BdSL) faces notable limitations, primarily due to the lack of datasets.\nRecognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of\nchallenges, including the need for well-annotated datasets, capturing the\ndynamic nature of sign gestures from facial or hand landmarks, developing\nsuitable machine learning or deep learning-based models with substantial video\nsamples, and so on. In this paper, we address these challenges by creating a\ncomprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and\nnatural setting, allowing positional and temporal variations and allowing sign\nusers to change hand dominance freely. The dataset encompasses 60 Bangla sign\nwords, with a significant scale of 9307 video trials provided by 18 signers\nunder the supervision of a sign language professional. The dataset was\nrigorously annotated and cross-checked by 60 annotators. We also introduced a\nunique approach of a relative quantization-based key frame encoding technique\nfor landmark based sign gesture recognition. We report the benchmarking of our\nBdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up\nto 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The\ndataset is available at https://www.kaggle.com/datasets/hasaniut/bdslw60 and\nthe code base is accessible from https://github.com/hasanssl/BdSLW60_Code.\n",
                "publicationDate": "2024-02-13T18:02:58Z",
                "Link": "http://arxiv.org/pdf/2402.08635v1",
                "arxiv_id": "2402.08635v1"
            },
            {
                "Title": "Entity-Switched Datasets: An Approach to Auditing the In-Domain\n  Robustness of Named Entity Recognition Models",
                "Authors": "Oshin Agarwal, Yinfei Yang, Byron C. Wallace, Ani Nenkova",
                "Abstract": "  Named entity recognition systems perform well on standard datasets comprising\nEnglish news. But given the paucity of data, it is difficult to draw\nconclusions about the robustness of systems with respect to recognizing a\ndiverse set of entities. We propose a method for auditing the in-domain\nrobustness of systems, focusing specifically on differences in performance due\nto the national origin of entities. We create entity-switched datasets, in\nwhich named entities in the original texts are replaced by plausible named\nentities of the same type but of different national origin. We find that\nstate-of-the-art systems' performance vary widely even in-domain: In the same\ncontext, entities from certain origins are more reliably recognized than\nentities from elsewhere. Systems perform best on American and Indian entities,\nand worst on Vietnamese and Indonesian entities. This auditing approach can\nfacilitate the development of more robust named entity recognition systems, and\nwill allow research in this area to consider fairness criteria that have\nreceived heightened attention in other predictive technology work.\n",
                "publicationDate": "2020-04-08T17:11:31Z",
                "Link": "http://arxiv.org/pdf/2004.04123v2",
                "arxiv_id": "2004.04123v2"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "A Boundary Offset Prediction Network for Named Entity Recognition",
                "Authors": "Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin",
                "Abstract": "  Named entity recognition (NER) is a fundamental task in natural language\nprocessing that aims to identify and classify named entities in text. However,\nspan-based methods for NER typically assign entity types to text spans,\nresulting in an imbalanced sample space and neglecting the connections between\nnon-entity and entity spans. To address these issues, we propose a novel\napproach for NER, named the Boundary Offset Prediction Network (BOPN), which\npredicts the boundary offsets between candidate spans and their nearest entity\nspans. By leveraging the guiding semantics of boundary offsets, BOPN\nestablishes connections between non-entity and entity spans, enabling\nnon-entity spans to function as additional positive samples for entity\ndetection. Furthermore, our method integrates entity type and span\nrepresentations to generate type-aware boundary offsets instead of using entity\ntypes as detection targets. We conduct experiments on eight widely-used NER\ndatasets, and the results demonstrate that our proposed BOPN outperforms\nprevious state-of-the-art methods.\n",
                "publicationDate": "2023-10-23T05:04:07Z",
                "Link": "http://arxiv.org/pdf/2310.18349v1",
                "arxiv_id": "2310.18349v1"
            },
            {
                "Title": "CMNEROne at SemEval-2022 Task 11: Code-Mixed Named Entity Recognition by\n  leveraging multilingual data",
                "Authors": "Suman Dowlagar, Radhika Mamidi",
                "Abstract": "  Identifying named entities is, in general, a practical and challenging task\nin the field of Natural Language Processing. Named Entity Recognition on the\ncode-mixed text is further challenging due to the linguistic complexity\nresulting from the nature of the mixing. This paper addresses the submission of\nteam CMNEROne to the SEMEVAL 2022 shared task 11 MultiCoNER. The Code-mixed NER\ntask aimed to identify named entities on the code-mixed dataset. Our work\nconsists of Named Entity Recognition (NER) on the code-mixed dataset by\nleveraging the multilingual data. We achieved a weighted average F1 score of\n0.7044, i.e., 6% greater than the baseline.\n",
                "publicationDate": "2022-06-15T06:33:13Z",
                "Link": "http://arxiv.org/pdf/2206.07318v1",
                "arxiv_id": "2206.07318v1"
            }
        ]
    },
    {
        "topic_name": "Knowledge Base Population",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "LILA-BOTI : Leveraging Isolated Letter Accumulations By Ordering Teacher\n  Insights for Bangla Handwriting Recognition",
                "Authors": "Md. Ismail Hossain, Mohammed Rakib, Sabbir Mollah, Fuad Rahman, Nabeel Mohammed",
                "Abstract": "  Word-level handwritten optical character recognition (OCR) remains a\nchallenge for morphologically rich languages like Bangla. The complexity arises\nfrom the existence of a large number of alphabets, the presence of several\ndiacritic forms, and the appearance of complex conjuncts. The difficulty is\nexacerbated by the fact that some graphemes occur infrequently but remain\nindispensable, so addressing the class imbalance is required for satisfactory\nresults. This paper addresses this issue by introducing two knowledge\ndistillation methods: Leveraging Isolated Letter Accumulations By Ordering\nTeacher Insights (LILA-BOTI) and Super Teacher LILA-BOTI. In both cases, a\nConvolutional Recurrent Neural Network (CRNN) student model is trained with the\ndark knowledge gained from a printed isolated character recognition teacher\nmodel. We conducted inter-dataset testing on \\emph{BN-HTRd} and\n\\emph{BanglaWriting} as our evaluation protocol, thus setting up a challenging\nproblem where the results would better reflect the performance on unseen data.\nOur evaluations achieved up to a 3.5% increase in the F1-Macro score for the\nminor classes and up to 4.5% increase in our overall word recognition rate when\ncompared with the base model (No KD) and conventional KD.\n",
                "publicationDate": "2022-05-23T16:04:25Z",
                "Link": "http://arxiv.org/pdf/2205.11420v1",
                "arxiv_id": "2205.11420v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Evolution of the Modern Phase of Written Bangla: A Statistical Study",
                "Authors": "Paheli Bhattacharya, Arnab Bhattacharya",
                "Abstract": "  Active languages such as Bangla (or Bengali) evolve over time due to a\nvariety of social, cultural, economic, and political issues. In this paper, we\nanalyze the change in the written form of the modern phase of Bangla\nquantitatively in terms of character-level, syllable-level, morpheme-level and\nword-level features. We collect three different types of corpora---classical,\nnewspapers and blogs---and test whether the differences in their features are\nstatistically significant. Results suggest that there are significant changes\nin the length of a word when measured in terms of characters, but there is not\nmuch difference in usage of different characters, syllables and morphemes in a\nword or of different words in a sentence. To the best of our knowledge, this is\nthe first work on Bangla of this kind.\n",
                "publicationDate": "2013-10-06T14:37:05Z",
                "Link": "http://arxiv.org/pdf/1310.1590v1",
                "arxiv_id": "1310.1590v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign\n  Language",
                "Authors": "Naimul Haque, Meraj Serker, Tariq Bin Bashar",
                "Abstract": "  In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches\noften imposed a burden on users, requiring them to spell words without hidden\ncharacters, which were subsequently corrected using Bangla grammar rules due to\nthe missing classes in BdSL36 dataset. However, this method posed a challenge\nin accurately guessing the incorrect spelling of words. To address this\nlimitation, we propose a novel real-time finger spelling system based on the\nYOLOv5 architecture. Our system employs specified rules and numerical classes\nas triggers to efficiently generate hidden and compound characters, eliminating\nthe necessity for additional classes and significantly enhancing user\nconvenience. Notably, our approach achieves character spelling in an impressive\n1.32 seconds with a remarkable accuracy rate of 98\\%. Furthermore, our YOLOv5\nmodel, trained on 9147 images, demonstrates an exceptional mean Average\nPrecision (mAP) of 96.4\\%. These advancements represent a substantial\nprogression in augmenting BdSL interpretation, promising increased inclusivity\nand accessibility for the linguistic minority. This innovative framework,\ncharacterized by compatibility with existing YOLO versions, stands as a\ntransformative milestone in enhancing communication modalities and linguistic\nequity within the Bangla Sign Language community.\n",
                "publicationDate": "2023-09-24T15:51:39Z",
                "Link": "http://arxiv.org/pdf/2309.13676v1",
                "arxiv_id": "2309.13676v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "Automated Word Prediction in Bangla Language Using Stochastic Language\n  Models",
                "Authors": "Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman",
                "Abstract": "  Word completion and word prediction are two important phenomena in typing\nthat benefit users who type using keyboard or other similar devices. They can\nhave profound impact on the typing of disable people. Our work is based on word\nprediction on Bangla sentence by using stochastic, i.e. N-gram language model\nsuch as unigram, bigram, trigram, deleted Interpolation and backoff models for\nauto completing a sentence by predicting a correct word in a sentence which\nsaves time and keystrokes of typing and also reduces misspelling. We use large\ndata corpus of Bangla language of different word types to predict correct word\nwith the accuracy as much as possible. We have found promising results. We hope\nthat our work will impact on the baseline for automated Bangla typing.\n",
                "publicationDate": "2016-02-25T05:35:16Z",
                "Link": "http://arxiv.org/pdf/1602.07803v1",
                "arxiv_id": "1602.07803v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Efficient approach of using CNN based pretrained model in Bangla\n  handwritten digit recognition",
                "Authors": "Muntarin Islam, Shabbir Ahmed Shuvo, Musarrat Saberin Nipun, Rejwan Bin Sulaiman, Jannatul Nayeem, Zubaer Haque, Md Mostak Shaikh, Md Sakib Ullah Sourav",
                "Abstract": "  Due to digitalization in everyday life, the need for automatically\nrecognizing handwritten digits is increasing. Handwritten digit recognition is\nessential for numerous applications in various industries. Bengali ranks the\nfifth largest language in the world with 265 million speakers (Native and\nnon-native combined) and 4 percent of the world population speaks Bengali. Due\nto the complexity of Bengali writing in terms of variety in shape, size, and\nwriting style, researchers did not get better accuracy using Supervised machine\nlearning algorithms to date. Moreover, fewer studies have been done on Bangla\nhandwritten digit recognition (BHwDR). In this paper, we proposed a novel\nCNN-based pre-trained handwritten digit recognition model which includes\nResnet-50, Inception-v3, and EfficientNetB0 on NumtaDB dataset of 17 thousand\ninstances with 10 classes.. The Result outperformed the performance of other\nmodels to date with 97% accuracy in the 10-digit classes. Furthermore, we have\nevaluated the result or our model with other research studies while suggesting\nfuture study\n",
                "publicationDate": "2022-09-19T15:58:53Z",
                "Link": "http://arxiv.org/pdf/2209.13005v1",
                "arxiv_id": "2209.13005v1"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Local Feature or Mel Frequency Cepstral Coefficients - Which One is\n  Better for MLN-Based Bangla Speech Recognition?",
                "Authors": "Foyzul Hassan, Mohammed Rokibul Alam Kotwal, Md. Mostafizur Rahman, Mohammad Nasiruddin, Md. Abdul Latif, Mohammad Nurul Huda",
                "Abstract": "  This paper discusses the dominancy of local features (LFs), as input to the\nmultilayer neural network (MLN), extracted from a Bangla input speech over mel\nfrequency cepstral coefficients (MFCCs). Here, LF-based method comprises three\nstages: (i) LF extraction from input speech, (ii) phoneme probabilities\nextraction using MLN from LF and (iii) the hidden Markov model (HMM) based\nclassifier to obtain more accurate phoneme strings. In the experiments on\nBangla speech corpus prepared by us, it is observed that the LFbased automatic\nspeech recognition (ASR) system provides higher phoneme correct rate than the\nMFCC-based system. Moreover, the proposed system requires fewer mixture\ncomponents in the HMMs.\n",
                "publicationDate": "2013-10-05T00:39:02Z",
                "Link": "http://arxiv.org/pdf/1310.1426v1",
                "arxiv_id": "1310.1426v1"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "publicationDate": "2021-08-01T17:16:03Z",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset",
                "Authors": "M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir",
                "Abstract": "  This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.\n",
                "publicationDate": "2020-11-15T11:08:53Z",
                "Link": "http://arxiv.org/pdf/2011.07499v3",
                "arxiv_id": "2011.07499v3"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            }
        ]
    },
    {
        "topic_name": "Text-to-Speech Synthesis",
        "summary": "default",
        "papers": [
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "publicationDate": "2021-08-01T17:16:03Z",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051",
                "Authors": "Nasif Muslim, Md. Tanvir Adnan, Mohammad Zahidul Kabir, Md. Humayun Kabir, Sheikh Mominul Islam",
                "Abstract": "  In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance.\n",
                "publicationDate": "2012-08-05T09:22:06Z",
                "Link": "http://arxiv.org/pdf/1208.0995v1",
                "arxiv_id": "1208.0995v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            }
        ]
    },
    {
        "topic_name": "Machine Reading Comprehension",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            }
        ]
    },
    {
        "topic_name": "Lexical Semantics",
        "summary": "default",
        "papers": [
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
                "Authors": "Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam",
                "Abstract": "  Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.\n",
                "publicationDate": "2024-01-22T18:52:51Z",
                "Link": "http://arxiv.org/pdf/2401.12210v1",
                "arxiv_id": "2401.12210v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Aligning Visual and Lexical Semantics",
                "Authors": "Fausto Giunchiglia, Mayukh Bagchi, Xiaolei Diao",
                "Abstract": "  We discuss two kinds of semantics relevant to Computer Vision (CV) systems -\nVisual Semantics and Lexical Semantics. While visual semantics focus on how\nhumans build concepts when using vision to perceive a target reality, lexical\nsemantics focus on how humans build concepts of the same target reality through\nthe use of language. The lack of coincidence between visual and lexical\nsemantics, in turn, has a major impact on CV systems in the form of the\nSemantic Gap Problem (SGP). The paper, while extensively exemplifying the lack\nof coincidence as above, introduces a general, domain-agnostic methodology to\nenforce alignment between visual and lexical semantics.\n",
                "publicationDate": "2022-12-13T15:01:22Z",
                "Link": "http://arxiv.org/pdf/2212.06629v1",
                "arxiv_id": "2212.06629v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "Frame-Based Continuous Lexical Semantics through Exponential Family\n  Tensor Factorization and Semantic Proto-Roles",
                "Authors": "Francis Ferraro, Adam Poliak, Ryan Cotterell, Benjamin Van Durme",
                "Abstract": "  We study how different frame annotations complement one another when learning\ncontinuous lexical semantics. We learn the representations from a tensorized\nskip-gram model that consistently encodes syntactic-semantic content better,\nwith multiple 10% gains over baselines.\n",
                "publicationDate": "2017-06-29T03:19:39Z",
                "Link": "http://arxiv.org/pdf/1706.09562v1",
                "arxiv_id": "1706.09562v1"
            },
            {
                "Title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
                "Authors": "Xiaoqian Li, Ercong Nie, Sheng Liang",
                "Abstract": "  The promise of Large Language Models (LLMs) in Natural Language Processing\nhas often been overshadowed by their limited performance in low-resource\nlanguages such as Bangla. To address this, our paper presents a pioneering\napproach that utilizes cross-lingual retrieval augmented in-context learning.\nBy strategically sourcing semantically similar prompts from high-resource\nlanguage, we enable multilingual pretrained language models (MPLMs), especially\nthe generative model BLOOMZ, to successfully boost performance on Bangla tasks.\nOur extensive evaluation highlights that the cross-lingual retrieval augmented\nprompts bring steady improvements to MPLMs over the zero-shot performance.\n",
                "publicationDate": "2023-11-01T15:32:50Z",
                "Link": "http://arxiv.org/pdf/2311.00587v2",
                "arxiv_id": "2311.00587v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "A Computational Treatment of HPSG Lexical Rules as Covariation in\n  Lexical Entries",
                "Authors": "Walt Detmar Meurers, Guido Minnen",
                "Abstract": "  We describe a compiler which translates a set of HPSG lexical rules and their\ninteraction into definite relations used to constrain lexical entries. The\ncompiler ensures automatic transfer of properties unchanged by a lexical rule.\nThus an operational semantics for the full lexical rule mechanism as used in\nHPSG linguistics is provided. Program transformation techniques are used to\nadvance the resulting encoding. The final output constitutes a computational\ncounterpart of the linguistic generalizations captured by lexical rules and\nallows ``on the fly'' application.\n",
                "publicationDate": "1995-04-04T18:00:36Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9504004v1",
                "arxiv_id": "9504004v1"
            },
            {
                "Title": "VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and\n  Lexical Alterations",
                "Authors": "Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, Hassan Sajjad",
                "Abstract": "  Despite their remarkable successes, state-of-the-art language models face\nchallenges in grasping certain important semantic details. This paper\nintroduces the VISLA (Variance and Invariance to Semantic and Lexical\nAlterations) benchmark, designed to evaluate the semantic and lexical\nunderstanding of language models. VISLA presents a 3-way semantic\n(in)equivalence task with a triplet of sentences associated with an image, to\nevaluate both vision-language models (VLMs) and unimodal language models\n(ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising\ndifficulties in distinguishing between lexical and semantic variations. Spatial\nsemantics encoded by language models also appear to be highly sensitive to\nlexical information. Notably, text encoders of VLMs demonstrate greater\nsensitivity to semantic and lexical variations than unimodal text encoders. Our\ncontributions include the unification of image-to-text and text-to-text\nretrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing\nLMs' semantic (in)variance in the presence of lexical alterations. The results\nhighlight strengths and weaknesses across diverse vision and unimodal language\nmodels, contributing to a deeper understanding of their capabilities. % VISLA\nenables a rigorous evaluation, shedding light on language models' capabilities\nin handling semantic and lexical nuances. Data and code will be made available\nat https://github.com/Sri-Harsha/visla_benchmark.\n",
                "publicationDate": "2024-04-25T07:08:00Z",
                "Link": "http://arxiv.org/pdf/2404.16365v1",
                "arxiv_id": "2404.16365v1"
            },
            {
                "Title": "The Typology of Polysemy: A Multilingual Distributional Framework",
                "Authors": "Ella Rabinovich, Yang Xu, Suzanne Stevenson",
                "Abstract": "  Lexical semantic typology has identified important cross-linguistic\ngeneralizations about the variation and commonalities in polysemy\npatterns---how languages package up meanings into words. Recent computational\nresearch has enabled investigation of lexical semantics at a much larger scale,\nbut little work has explored lexical typology across semantic domains, nor the\nfactors that influence cross-linguistic similarities. We present a novel\ncomputational framework that quantifies semantic affinity, the cross-linguistic\nsimilarity of lexical semantics for a concept. Our approach defines a common\nmultilingual semantic space that enables a direct comparison of the lexical\nexpression of concepts across languages. We validate our framework against\nempirical findings on lexical semantic typology at both the concept and domain\nlevels. Our results reveal an intricate interaction between semantic domains\nand extra-linguistic factors, beyond language phylogeny, that co-shape the\ntypology of polysemy across languages.\n",
                "publicationDate": "2020-06-02T22:31:40Z",
                "Link": "http://arxiv.org/pdf/2006.01966v1",
                "arxiv_id": "2006.01966v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Simulating Lexical Semantic Change from Sense-Annotated Data",
                "Authors": "Dominik Schlechtweg, Sabine Schulte im Walde",
                "Abstract": "  We present a novel procedure to simulate lexical semantic change from\nsynchronic sense-annotated data, and demonstrate its usefulness for assessing\nlexical semantic change detection models. The induced dataset represents a\nstronger correspondence to empirically observed lexical semantic change than\nprevious synthetic datasets, because it exploits the intimate relationship\nbetween synchronic polysemy and diachronic change. We publish the data and\nprovide the first large-scale evaluation gold standard for LSC detection\nmodels.\n",
                "publicationDate": "2020-01-09T20:37:49Z",
                "Link": "http://arxiv.org/pdf/2001.03216v1",
                "arxiv_id": "2001.03216v1"
            },
            {
                "Title": "ILexicOn: toward an ECD-compliant interlingual lexical ontology\n  described with semantic web formalisms",
                "Authors": "Maxime Lefran\u00e7ois, Fabien Gandon",
                "Abstract": "  We are interested in bridging the world of natural language and the world of\nthe semantic web in particular to support natural multilingual access to the\nweb of data. In this paper we introduce a new type of lexical ontology called\ninterlingual lexical ontology (ILexicOn), which uses semantic web formalisms to\nmake each interlingual lexical unit class (ILUc) support the projection of its\nsemantic decomposition on itself. After a short overview of existing lexical\nontologies, we briefly introduce the semantic web formalisms we use. We then\npresent the three layered architecture of our approach: i) the interlingual\nlexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formally\ndefined; iii) the data layer. We illustrate our approach with a standalone\nILexicOn, and introduce and explain a concise human-readable notation to\nrepresent ILexicOns. Finally, we show how semantic web formalisms enable the\nprojection of a semantic decomposition on the decomposed ILUc.\n",
                "publicationDate": "2012-04-24T09:13:59Z",
                "Link": "http://arxiv.org/pdf/1204.5316v1",
                "arxiv_id": "1204.5316v1"
            },
            {
                "Title": "Comparative Probing of Lexical Semantics Theories for Cognitive\n  Plausibility and Technological Usefulness",
                "Authors": "Ant\u00f3nio Branco, Jo\u00e3o Rodrigues, Ma\u0142gorzata Salawa, Ruben Branco, Chakaveh Saedi",
                "Abstract": "  Lexical semantics theories differ in advocating that the meaning of words is\nrepresented as an inference graph, a feature mapping or a vector space, thus\nraising the question: is it the case that one of these approaches is superior\nto the others in representing lexical semantics appropriately? Or in its non\nantagonistic counterpart: could there be a unified account of lexical semantics\nwhere these approaches seamlessly emerge as (partial) renderings of (different)\naspects of a core semantic knowledge base?\n  In this paper, we contribute to these research questions with a number of\nexperiments that systematically probe different lexical semantics theories for\ntheir levels of cognitive plausibility and of technological usefulness.\n  The empirical findings obtained from these experiments advance our insight on\nlexical semantics as the feature-based approach emerges as superior to the\nother ones, and arguably also move us closer to finding answers to the research\nquestions above.\n",
                "publicationDate": "2020-11-16T14:46:08Z",
                "Link": "http://arxiv.org/pdf/2011.07997v1",
                "arxiv_id": "2011.07997v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Time-Out: Temporal Referencing for Robust Modeling of Lexical Semantic\n  Change",
                "Authors": "Haim Dubossarsky, Simon Hengchen, Nina Tahmasebi, Dominik Schlechtweg",
                "Abstract": "  State-of-the-art models of lexical semantic change detection suffer from\nnoise stemming from vector space alignment. We have empirically tested the\nTemporal Referencing method for lexical semantic change and show that, by\navoiding alignment, it is less affected by this noise. We show that, trained on\na diachronic corpus, the skip-gram with negative sampling architecture with\ntemporal referencing outperforms alignment models on a synthetic task as well\nas a manual testset. We introduce a principled way to simulate lexical semantic\nchange and systematically control for possible biases.\n",
                "publicationDate": "2019-06-04T19:19:46Z",
                "Link": "http://arxiv.org/pdf/1906.01688v1",
                "arxiv_id": "1906.01688v1"
            },
            {
                "Title": "A Dense Representation Framework for Lexical and Semantic Matching",
                "Authors": "Sheng-Chieh Lin, Jimmy Lin",
                "Abstract": "  Lexical and semantic matching capture different successful approaches to text\nretrieval and the fusion of their results has proven to be more effective and\nrobust than either alone. Prior work performs hybrid retrieval by conducting\nlexical and semantic matching using different systems (e.g., Lucene and Faiss,\nrespectively) and then fusing their model outputs. In contrast, our work\nintegrates lexical representations with dense semantic representations by\ndensifying high-dimensional lexical representations into what we call\nlow-dimensional dense lexical representations (DLRs). Our experiments show that\nDLRs can effectively approximate the original lexical representations,\npreserving effectiveness while improving query latency. Furthermore, we can\ncombine dense lexical and semantic representations to generate dense hybrid\nrepresentations (DHRs) that are more flexible and yield faster retrieval\ncompared to existing hybrid techniques. In addition, we explore it jointly\ntraining lexical and semantic representations in a single model and empirically\nshow that the resulting DHRs are able to combine the advantages of the\nindividual components. Our best DHR model is competitive with state-of-the-art\nsingle-vector and multi-vector dense retrievers in both in-domain and zero-shot\nevaluation settings. Furthermore, our model is both faster and requires smaller\nindexes, making our dense representation framework an attractive approach to\ntext retrieval. Our code is available at https://github.com/castorini/dhr.\n",
                "publicationDate": "2022-06-20T17:29:42Z",
                "Link": "http://arxiv.org/pdf/2206.09912v2",
                "arxiv_id": "2206.09912v2"
            },
            {
                "Title": "Semantic Types, Lexical Sorts and Classifiers",
                "Authors": "Bruno Mery, Christian Retor\u00e9",
                "Abstract": "  We propose a cognitively and linguistically motivated set of sorts for\nlexical semantics in a compositional setting: the classifiers in languages that\ndo have such pronouns. These sorts are needed to include lexical considerations\nin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical\nextensions of usual Montague semantics to model restriction of selection,\nfelicitous and infelicitous copredication require a rich and refined type\nsystem whose base types are the lexical sorts, the basis of the many-sorted\nlogic in which semantical representations of sentences are stated. However,\nnone of those approaches define precisely the actual base types or sorts to be\nused in the lexicon. In this article, we shall discuss some of the options\ncommonly adopted by researchers in formal lexical semantics, and defend the\nview that classifiers in the languages which have such pronouns are an\nappealing solution, both linguistically and cognitively motivated.\n",
                "publicationDate": "2013-12-11T14:04:52Z",
                "Link": "http://arxiv.org/pdf/1312.3168v1",
                "arxiv_id": "1312.3168v1"
            },
            {
                "Title": "BOS at LSCDiscovery: Lexical Substitution for Interpretable Lexical\n  Semantic Change Detection",
                "Authors": "Artem Kudisov, Nikolay Arefyev",
                "Abstract": "  We propose a solution for the LSCDiscovery shared task on Lexical Semantic\nChange Detection in Spanish. Our approach is based on generating lexical\nsubstitutes that describe old and new senses of a given word. This approach\nachieves the second best result in sense loss and sense gain detection\nsubtasks. By observing those substitutes that are specific for only one time\nperiod, one can understand which senses were obtained or lost. This allows\nproviding more detailed information about semantic change to the user and makes\nour method interpretable.\n",
                "publicationDate": "2022-06-07T11:40:29Z",
                "Link": "http://arxiv.org/pdf/2206.11865v1",
                "arxiv_id": "2206.11865v1"
            },
            {
                "Title": "Constraining Lexical Selection Across Languages Using TAGs",
                "Authors": "Dania Egedi, Martha Palmer",
                "Abstract": "  Lexical selection in Machine Translation consists of several related\ncomponents. Two that have received a lot of attention are lexical mapping from\nan underlying concept or lexical item, and choosing the correct\nsubcategorization frame based on argument structure. Because most MT\napplications are small or relatively domain specific, a third component of\nlexical selection is generally overlooked - distinguishing between lexical\nitems that are closely related conceptually. While some MT systems have\nproposed using a 'world knowledge' module to decide which word is more\nappropriate based on various pragmatic or stylistic constraints, we are\ninterested in seeing how much we can accomplish using a combination of syntax\nand lexical semantics. By using separate ontologies for each language\nimplemented in FB-LTAGs, we are able to elegantly model the more specific and\nlanguage dependent syntactic and semantic distinctions necessary to further\nfilter the choice of the lexical item.\n",
                "publicationDate": "1994-11-03T13:56:55Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9411005v1",
                "arxiv_id": "9411005v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Using qualia information to identify lexical semantic classes in an\n  unsupervised clustering task",
                "Authors": "Lauren Romeo, Sara Mendes, N\u00faria Bel",
                "Abstract": "  Acquiring lexical information is a complex problem, typically approached by\nrelying on a number of contexts to contribute information for classification.\nOne of the first issues to address in this domain is the determination of such\ncontexts. The work presented here proposes the use of automatically obtained\nFORMAL role descriptors as features used to draw nouns from the same lexical\nsemantic class together in an unsupervised clustering task. We have dealt with\nthree lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The\nresults obtained show that it is possible to discriminate between elements from\ndifferent lexical semantic classes using only FORMAL role information, hence\nvalidating our initial hypothesis. Also, iterating our method accurately\naccounts for fine-grained distinctions within lexical classes, namely\ndistinctions involving ambiguous expressions. Moreover, a filtering and\nbootstrapping strategy employed in extracting FORMAL role descriptors proved to\nminimize effects of sparse data and noise in our task.\n",
                "publicationDate": "2013-03-11T08:21:48Z",
                "Link": "http://arxiv.org/pdf/1303.2449v1",
                "arxiv_id": "1303.2449v1"
            },
            {
                "Title": "Embedding Semantic Relations into Word Representations",
                "Authors": "Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi",
                "Abstract": "  Learning representations for semantic relations is important for various\ntasks such as analogy detection, relational search, and relation\nclassification. Although there have been several proposals for learning\nrepresentations for individual words, learning word representations that\nexplicitly capture the semantic relations between words remains under\ndeveloped. We propose an unsupervised method for learning vector\nrepresentations for words such that the learnt representations are sensitive to\nthe semantic relations that exist between two words. First, we extract lexical\npatterns from the co-occurrence contexts of two words in a corpus to represent\nthe semantic relations that exist between those two words. Second, we represent\na lexical pattern as the weighted sum of the representations of the words that\nco-occur with that lexical pattern. Third, we train a binary classifier to\ndetect relationally similar vs. non-similar lexical pattern pairs. The proposed\nmethod is unsupervised in the sense that the lexical pattern pairs we use as\ntrain data are automatically sampled from a corpus, without requiring any\nmanual intervention. Our proposed method statistically significantly\noutperforms the current state-of-the-art word representations on three\nbenchmark datasets for proportional analogy detection, demonstrating its\nability to accurately capture the semantic relations among words.\n",
                "publicationDate": "2015-05-01T11:43:34Z",
                "Link": "http://arxiv.org/pdf/1505.00161v1",
                "arxiv_id": "1505.00161v1"
            },
            {
                "Title": "Probing Cross-Lingual Lexical Knowledge from Multilingual Sentence\n  Encoders",
                "Authors": "Ivan Vuli\u0107, Goran Glava\u0161, Fangyu Liu, Nigel Collier, Edoardo Maria Ponti, Anna Korhonen",
                "Abstract": "  Pretrained multilingual language models (LMs) can be successfully transformed\ninto multilingual sentence encoders (SEs; e.g., LaBSE, xMPNet) via additional\nfine-tuning or model distillation with parallel data. However, it remains\nunclear how to best leverage them to represent sub-sentence lexical items\n(i.e., words and phrases) in cross-lingual lexical tasks. In this work, we\nprobe SEs for the amount of cross-lingual lexical knowledge stored in their\nparameters, and compare them against the original multilingual LMs. We also\ndevise a simple yet efficient method for exposing the cross-lingual lexical\nknowledge by means of additional fine-tuning through inexpensive contrastive\nlearning that requires only a small amount of word translation pairs. Using\nbilingual lexical induction (BLI), cross-lingual lexical semantic similarity,\nand cross-lingual entity linking as lexical probing tasks, we report\nsubstantial gains on standard benchmarks (e.g., +10 Precision@1 points in BLI).\nThe results indicate that the SEs such as LaBSE can be 'rewired' into effective\ncross-lingual lexical encoders via the contrastive learning procedure, and that\nthey contain more cross-lingual lexical knowledge than what 'meets the eye'\nwhen they are used as off-the-shelf SEs. This way, we also provide an effective\ntool for harnessing 'covert' multilingual lexical knowledge hidden in\nmultilingual sentence encoders.\n",
                "publicationDate": "2022-04-30T13:23:16Z",
                "Link": "http://arxiv.org/pdf/2205.00267v2",
                "arxiv_id": "2205.00267v2"
            },
            {
                "Title": "A Wind of Change: Detecting and Evaluating Lexical Semantic Change\n  across Times and Domains",
                "Authors": "Dominik Schlechtweg, Anna H\u00e4tty, Marco del Tredici, Sabine Schulte im Walde",
                "Abstract": "  We perform an interdisciplinary large-scale evaluation for detecting lexical\nsemantic divergences in a diachronic and in a synchronic task: semantic sense\nchanges across time, and semantic sense changes across domains. Our work\naddresses the superficialness and lack of comparison in assessing models of\ndiachronic lexical change, by bringing together and extending benchmark models\non a common state-of-the-art evaluation task. In addition, we demonstrate that\nthe same evaluation task and modelling approaches can successfully be utilised\nfor the synchronic detection of domain-specific sense divergences in the field\nof term extraction.\n",
                "publicationDate": "2019-06-07T09:19:47Z",
                "Link": "http://arxiv.org/pdf/1906.02979v1",
                "arxiv_id": "1906.02979v1"
            },
            {
                "Title": "KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation\n  Classification",
                "Authors": "Chengyu Wang, Minghui Qiu, Jun Huang, Xiaofeng He",
                "Abstract": "  Lexical relations describe how concepts are semantically related, in the form\nof relation triples. The accurate prediction of lexical relations between\nconcepts is challenging, due to the sparsity of patterns indicating the\nexistence of such relations. We propose the Knowledge-Enriched Meta-Learning\n(KEML) framework to address the task of lexical relation classification. In\nKEML, the LKB-BERT (Lexical Knowledge Base-BERT) model is presented to learn\nconcept representations from massive text corpora, with rich lexical knowledge\ninjected by distant supervision. A probabilistic distribution of auxiliary\ntasks is defined to increase the model's ability to recognize different types\nof lexical relations. We further combine a meta-learning process over the\nauxiliary task distribution and supervised learning to train the neural lexical\nrelation classifier. Experiments over multiple datasets show that KEML\noutperforms state-of-the-art methods.\n",
                "publicationDate": "2020-02-25T14:43:56Z",
                "Link": "http://arxiv.org/pdf/2002.10903v2",
                "arxiv_id": "2002.10903v2"
            },
            {
                "Title": "A Framework for Enriching Lexical Semantic Resources with Distributional\n  Semantics",
                "Authors": "Chris Biemann, Stefano Faralli, Alexander Panchenko, Simone Paolo Ponzetto",
                "Abstract": "  We present an approach to combining distributional semantic representations\ninduced from text corpora with manually constructed lexical-semantic networks.\nWhile both kinds of semantic resources are available with high lexical\ncoverage, our aligned resource combines the domain specificity and availability\nof contextual information from distributional models with the conciseness and\nhigh quality of manually crafted lexical networks. We start with a\ndistributional representation of induced senses of vocabulary terms, which are\naccompanied with rich context information given by related lexical items. We\nthen automatically disambiguate such representations to obtain a full-fledged\nproto-conceptualization, i.e. a typed graph of induced word senses. In a final\nstep, this proto-conceptualization is aligned to a lexical ontology, resulting\nin a hybrid aligned resource. Moreover, unmapped induced senses are associated\nwith a semantic type in order to connect them to the core resource. Manual\nevaluations against ground-truth judgments for different stages of our method\nas well as an extrinsic evaluation on a knowledge-based Word Sense\nDisambiguation benchmark all indicate the high quality of the new hybrid\nresource. Additionally, we show the benefits of enriching top-down lexical\nknowledge resources with bottom-up distributional information from text for\naddressing high-end knowledge acquisition tasks such as cleaning hypernym\ngraphs and learning taxonomies from scratch.\n",
                "publicationDate": "2017-12-23T18:46:58Z",
                "Link": "http://arxiv.org/pdf/1712.08819v1",
                "arxiv_id": "1712.08819v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "TEAM-Atreides at SemEval-2022 Task 11: On leveraging data augmentation\n  and ensemble to recognize complex Named Entities in Bangla",
                "Authors": "Nazia Tasnim, Md. Istiak Hossain Shihab, Asif Shahriyar Sushmit, Steven Bethard, Farig Sadeque",
                "Abstract": "  Many areas, such as the biological and healthcare domain, artistic works, and\norganization names, have nested, overlapping, discontinuous entity mentions\nthat may even be syntactically or semantically ambiguous in practice.\nTraditional sequence tagging algorithms are unable to recognize these complex\nmentions because they may violate the assumptions upon which sequence tagging\nschemes are founded. In this paper, we describe our contribution to SemEval\n2022 Task 11 on identifying such complex Named Entities. We have leveraged the\nensemble of multiple ELECTRA-based models that were exclusively pretrained on\nthe Bangla language with the performance of ELECTRA-based models pretrained on\nEnglish to achieve competitive performance on the Track-11. Besides providing a\nsystem description, we will also present the outcomes of our experiments on\narchitectural decisions, dataset augmentations, and post-competition findings.\n",
                "publicationDate": "2022-04-21T08:40:17Z",
                "Link": "http://arxiv.org/pdf/2204.09964v1",
                "arxiv_id": "2204.09964v1"
            },
            {
                "Title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers\n  of Generative LLMs Reflect Lexical Semantics",
                "Authors": "Zhu Liu, Cunliang Kong, Ying Liu, Maosong Sun",
                "Abstract": "  Large language models have achieved remarkable success in general language\nunderstanding tasks. However, as a family of generative methods with the\nobjective of next token prediction, the semantic evolution with the depth of\nthese models are not fully explored, unlike their predecessors, such as\nBERT-like architectures. In this paper, we specifically investigate the\nbottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by\nprobing its hidden states at the end of each layer using a contextualized word\nidentification task. Our experiments show that the representations in lower\nlayers encode lexical semantics, while the higher layers, with weaker semantic\ninduction, are responsible for prediction. This is in contrast to models with\ndiscriminative objectives, such as mask language modeling, where the higher\nlayers obtain better lexical semantics. The conclusion is further supported by\nthe monotonic increase in performance via the hidden states for the last\nmeaningless symbols, such as punctuation, in the prompting strategy.\n",
                "publicationDate": "2024-03-03T13:14:47Z",
                "Link": "http://arxiv.org/pdf/2403.01509v1",
                "arxiv_id": "2403.01509v1"
            },
            {
                "Title": "DPCSpell: A Transformer-based Detector-Purificator-Corrector Framework\n  for Spelling Error Correction of Bangla and Resource Scarce Indic Languages",
                "Authors": "Mehedi Hasan Bijoy, Nahid Hossain, Salekul Islam, Swakkhar Shatabda",
                "Abstract": "  Spelling error correction is the task of identifying and rectifying\nmisspelled words in texts. It is a potential and active research topic in\nNatural Language Processing because of numerous applications in human language\nunderstanding. The phonetically or visually similar yet semantically distinct\ncharacters make it an arduous task in any language. Earlier efforts on spelling\nerror correction in Bangla and resource-scarce Indic languages focused on\nrule-based, statistical, and machine learning-based methods which we found\nrather inefficient. In particular, machine learning-based approaches, which\nexhibit superior performance to rule-based and statistical methods, are\nineffective as they correct each character regardless of its appropriateness.\nIn this work, we propose a novel detector-purificator-corrector framework based\non denoising transformers by addressing previous issues. Moreover, we present a\nmethod for large-scale corpus creation from scratch which in turn resolves the\nresource limitation problem of any left-to-right scripted language. The\nempirical outcomes demonstrate the effectiveness of our approach that\noutperforms previous state-of-the-art methods by a significant margin for\nBangla spelling error correction. The models and corpus are publicly available\nat https://tinyurl.com/DPCSpell.\n",
                "publicationDate": "2022-11-07T17:59:05Z",
                "Link": "http://arxiv.org/pdf/2211.03730v1",
                "arxiv_id": "2211.03730v1"
            },
            {
                "Title": "Towards Universal Semantic Tagging",
                "Authors": "Lasha Abzianidze, Johan Bos",
                "Abstract": "  The paper proposes the task of universal semantic tagging---tagging word\ntokens with language-neutral, semantically informative tags. We argue that the\ntask, with its independent nature, contributes to better semantic analysis for\nwide-coverage multilingual text. We present the initial version of the semantic\ntagset and show that (a) the tags provide semantically fine-grained\ninformation, and (b) they are suitable for cross-lingual semantic parsing. An\napplication of the semantic tagging in the Parallel Meaning Bank supports both\nof these points as the tags contribute to formal lexical semantics and their\ncross-lingual projection. As a part of the application, we annotate a small\ncorpus with the semantic tags and present new baseline result for universal\nsemantic tagging.\n",
                "publicationDate": "2017-09-29T12:58:05Z",
                "Link": "http://arxiv.org/pdf/1709.10381v1",
                "arxiv_id": "1709.10381v1"
            },
            {
                "Title": "Effects of Pre- and Post-Processing on type-based Embeddings in Lexical\n  Semantic Change Detection",
                "Authors": "Jens Kaiser, Sinan Kurtyigit, Serge Kotchourko, Dominik Schlechtweg",
                "Abstract": "  Lexical semantic change detection is a new and innovative research field. The\noptimal fine-tuning of models including pre- and post-processing is largely\nunclear. We optimize existing models by (i) pre-training on large corpora and\nrefining on diachronic target corpora tackling the notorious small data\nproblem, and (ii) applying post-processing transformations that have been shown\nto improve performance on synchronic tasks. Our results provide a guide for the\napplication and optimization of lexical semantic change detection models across\nvarious learning scenarios.\n",
                "publicationDate": "2021-01-22T22:34:15Z",
                "Link": "http://arxiv.org/pdf/2101.09368v2",
                "arxiv_id": "2101.09368v2"
            },
            {
                "Title": "Morphological Cues for Lexical Semantics",
                "Authors": "Marc Light",
                "Abstract": "  Most natural language processing tasks require lexical semantic information.\nAutomated acquisition of this information would thus increase the robustness\nand portability of NLP systems. This paper describes an acquisition method\nwhich makes use of fixed correspondences between derivational affixes and\nlexical semantic information. One advantage of this method, and of other\nmethods that rely only on surface characteristics of language, is that the\nnecessary input is currently available.\n",
                "publicationDate": "1996-06-04T09:17:11Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9606003v1",
                "arxiv_id": "9606003v1"
            },
            {
                "Title": "A Lexical Semantic Database for Verbmobil",
                "Authors": "Johannes Heinecke, Karsten L. Worm",
                "Abstract": "  This paper describes the development and use of a lexical semantic database\nfor the Verbmobil speech-to-speech machine translation system. The motivation\nis to provide a common information source for the distributed development of\nthe semantics, transfer and semantic evaluation modules and to store lexical\nsemantic information application-independently.\n  The database is organized around a set of abstract semantic classes and has\nbeen used to define the semantic contributions of the lemmata in the vocabulary\nof the system, to automatically create semantic lexica and to check the\ncorrectness of the semantic representations built up. The semantic classes are\nmodelled using an inheritance hierarchy. The database is implemented using the\nlexicon formalism LeX4 developed during the project.\n",
                "publicationDate": "1996-07-30T12:33:26Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9607032v1",
                "arxiv_id": "9607032v1"
            },
            {
                "Title": "SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection",
                "Authors": "Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, Nina Tahmasebi",
                "Abstract": "  Lexical Semantic Change detection, i.e., the task of identifying words that\nchange meaning over time, is a very active research area, with applications in\nNLP, lexicography, and linguistics. Evaluation is currently the most pressing\nproblem in Lexical Semantic Change detection, as no gold standards are\navailable to the community, which hinders progress. We present the results of\nthe first shared task that addresses this gap by providing researchers with an\nevaluation framework and manually annotated, high-quality datasets for English,\nGerman, Latin, and Swedish. 33 teams submitted 186 systems, which were\nevaluated on two subtasks.\n",
                "publicationDate": "2020-07-22T14:37:42Z",
                "Link": "http://arxiv.org/pdf/2007.11464v2",
                "arxiv_id": "2007.11464v2"
            },
            {
                "Title": "Knowledge Representation for Lexical Semantics: Is Standard First Order\n  Logic Enough?",
                "Authors": "Marc Light, Lenhart Schubert",
                "Abstract": "  Natural language understanding applications such as interactive planning and\nface-to-face translation require extensive inferencing. Many of these\ninferences are based on the meaning of particular open class words. Providing a\nrepresentation that can support such lexically-based inferences is a primary\nconcern of lexical semantics. The representation language of first order logic\nhas well-understood semantics and a multitude of inferencing systems have been\nimplemented for it. Thus it is a prime candidate to serve as a lexical\nsemantics representation. However, we argue that FOL, although a good starting\npoint, needs to be extended before it can efficiently and concisely support all\nthe lexically-based inferences needed.\n",
                "publicationDate": "1994-12-10T16:53:27Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9412004v1",
                "arxiv_id": "9412004v1"
            },
            {
                "Title": "Complementing Lexical Retrieval with Semantic Residual Embedding",
                "Authors": "Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, Jamie Callan",
                "Abstract": "  This paper presents CLEAR, a retrieval model that seeks to complement\nclassical lexical exact-match models such as BM25 with semantic matching\nsignals from a neural embedding matching model. CLEAR explicitly trains the\nneural embedding to encode language structures and semantics that lexical\nretrieval fails to capture with a novel residual-based embedding learning\nmethod. Empirical evaluations demonstrate the advantages of CLEAR over\nstate-of-the-art retrieval models, and that it can substantially improve the\nend-to-end accuracy and efficiency of reranking pipelines.\n",
                "publicationDate": "2020-04-29T06:10:02Z",
                "Link": "http://arxiv.org/pdf/2004.13969v3",
                "arxiv_id": "2004.13969v3"
            },
            {
                "Title": "Diachronic Usage Relatedness (DURel): A Framework for the Annotation of\n  Lexical Semantic Change",
                "Authors": "Dominik Schlechtweg, Sabine Schulte im Walde, Stefanie Eckmann",
                "Abstract": "  We propose a framework that extends synchronic polysemy annotation to\ndiachronic changes in lexical meaning, to counteract the lack of resources for\nevaluating computational models of lexical semantic change. Our framework\nexploits an intuitive notion of semantic relatedness, and distinguishes between\ninnovative and reductive meaning changes with high inter-annotator agreement.\nThe resulting test set for German comprises ratings from five annotators for\nthe relatedness of 1,320 use pairs across 22 target words.\n",
                "publicationDate": "2018-04-18T00:50:56Z",
                "Link": "http://arxiv.org/pdf/1804.06517v1",
                "arxiv_id": "1804.06517v1"
            },
            {
                "Title": "Large Language Models on Lexical Semantic Change Detection: An\n  Evaluation",
                "Authors": "Ruiyu Wang, Matthew Choi",
                "Abstract": "  Lexical Semantic Change Detection stands out as one of the few areas where\nLarge Language Models (LLMs) have not been extensively involved. Traditional\nmethods like PPMI, and SGNS remain prevalent in research, alongside newer\nBERT-based approaches. Despite the comprehensive coverage of various natural\nlanguage processing domains by LLMs, there is a notable scarcity of literature\nconcerning their application in this specific realm. In this work, we seek to\nbridge this gap by introducing LLMs into the domain of Lexical Semantic Change\nDetection. Our work presents novel prompting solutions and a comprehensive\nevaluation that spans all three generations of language models, contributing to\nthe exploration of LLMs in this research area.\n",
                "publicationDate": "2023-12-10T21:26:35Z",
                "Link": "http://arxiv.org/pdf/2312.06002v1",
                "arxiv_id": "2312.06002v1"
            },
            {
                "Title": "How Can BERT Help Lexical Semantics Tasks?",
                "Authors": "Yile Wang, Leyang Cui, Yue Zhang",
                "Abstract": "  Contextualized embeddings such as BERT can serve as strong input\nrepresentations to NLP tasks, outperforming their static embeddings\ncounterparts such as skip-gram, CBOW and GloVe. However, such embeddings are\ndynamic, calculated according to a sentence-level context, which limits their\nuse in lexical semantics tasks. We address this issue by making use of dynamic\nembeddings as word representations in training static embeddings, thereby\nleveraging their strong representation power for disambiguating context\ninformation. Results show that this method leads to improvements over\ntraditional static embeddings on a range of lexical semantics tasks, obtaining\nthe best reported results on seven datasets.\n",
                "publicationDate": "2019-11-07T14:23:05Z",
                "Link": "http://arxiv.org/pdf/1911.02929v2",
                "arxiv_id": "1911.02929v2"
            },
            {
                "Title": "A Lexicon for Underspecified Semantic Tagging",
                "Authors": "Paul Buitelaar",
                "Abstract": "  The paper defends the notion that semantic tagging should be viewed as more\nthan disambiguation between senses. Instead, semantic tagging should be a first\nstep in the interpretation process by assigning each lexical item a\nrepresentation of all of its systematically related senses, from which further\nsemantic processing steps can derive discourse dependent interpretations. This\nleads to a new type of semantic lexicon (CoreLex) that supports underspecified\nsemantic tagging through a design based on systematic polysemous classes and a\nclass-based acquisition of lexical knowledge for specific domains.\n",
                "publicationDate": "1997-05-14T15:14:39Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9705011v1",
                "arxiv_id": "9705011v1"
            },
            {
                "Title": "Bi-Lexical Rules for Multi-Lexeme Translation in Lexicalist MT",
                "Authors": "Arturo Trujillo",
                "Abstract": "  The paper presents a prototype lexicalist Machine Translation system (based\non the so-called `Shake-and-Bake' approach of Whitelock (1992) consisting of an\nanalysis component, a dynamic bilingual lexicon, and a generation component,\nand shows how it is applied to a range of MT problems. Multi-Lexeme\ntranslations are handled through bi-lexical rules which map bilingual lexical\nsigns into new bilingual lexical signs. It is argued that much translation can\nbe handled by equating translationally equivalent lists of lexical signs,\neither directly in the bilingual lexicon, or by deriving them through\nbi-lexical rules. Lexical semantic information organized as Qualia structures\n(Pustejovsky 1991) is used as a mechanism for restricting the domain of the\nrules.\n",
                "publicationDate": "1995-08-12T13:42:24Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9508006v1",
                "arxiv_id": "9508006v1"
            },
            {
                "Title": "Exploiting Context to Identify Lexical Atoms -- A Statistical View of\n  Linguistic Context",
                "Authors": "Chengxiang Zhai",
                "Abstract": "  Interpretation of natural language is inherently context-sensitive. Most\nwords in natural language are ambiguous and their meanings are heavily\ndependent on the linguistic context in which they are used. The study of\nlexical semantics can not be separated from the notion of context. This paper\ntakes a contextual approach to lexical semantics and studies the linguistic\ncontext of lexical atoms, or \"sticky\" phrases such as \"hot dog\". Since such\nlexical atoms may occur frequently in unrestricted natural language text,\nrecognizing them is crucial for understanding naturally-occurring text. The\npaper proposes several heuristic approaches to exploiting the linguistic\ncontext to identify lexical atoms from arbitrary natural language text.\n",
                "publicationDate": "1997-01-02T16:35:24Z",
                "Link": "http://arxiv.org/pdf/cmp-lg/9701001v1",
                "arxiv_id": "9701001v1"
            }
        ]
    },
    {
        "topic_name": "Text Normalization",
        "summary": "default",
        "papers": [
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "Authorship Attribution in Bangla literature using Character-level CNN",
                "Authors": "Aisha Khatun, Anisur Rahman, Md. Saiful Islam,  Marium-E-Jannat",
                "Abstract": "  Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results.\n",
                "publicationDate": "2020-01-11T14:54:04Z",
                "Link": "http://arxiv.org/pdf/2001.05316v1",
                "arxiv_id": "2001.05316v1"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "publicationDate": "2021-08-01T17:16:03Z",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "Mapping Violence: Developing an Extensive Framework to Build a Bangla\n  Sectarian Expression Dataset from Social Media Interactions",
                "Authors": "Nazia Tasnim, Sujan Sen Gupta, Md. Istiak Hossain Shihab, Fatiha Islam Juee, Arunima Tahsin, Pritom Ghum, Kanij Fatema, Marshia Haque, Wasema Farzana, Prionti Nasir, Ashique KhudaBukhsh, Farig Sadeque, Asif Sushmit",
                "Abstract": "  Communal violence in online forums has become extremely prevalent in South\nAsia, where many communities of different cultures coexist and share resources.\nThese societies exhibit a phenomenon characterized by strong bonds within their\nown groups and animosity towards others, leading to conflicts that frequently\nescalate into violent confrontations. To address this issue, we have developed\nthe first comprehensive framework for the automatic detection of communal\nviolence markers in online Bangla content accompanying the largest collection\n(13K raw sentences) of social media interactions that fall under the definition\nof four major violence class and their 16 coarse expressions. Our workflow\nintroduces a 7-step expert annotation process incorporating insights from\nsocial scientists, linguists, and psychologists. By presenting data statistics\nand benchmarking performance using this dataset, we have determined that, aside\nfrom the category of Non-communal violence, Religio-communal violence is\nparticularly pervasive in Bangla text. Moreover, we have substantiated the\neffectiveness of fine-tuning language models in identifying violent comments by\nconducting preliminary benchmarking on the state-of-the-art Bangla deep\nlearning model.\n",
                "publicationDate": "2024-04-17T21:09:13Z",
                "Link": "http://arxiv.org/pdf/2404.11752v1",
                "arxiv_id": "2404.11752v1"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "Authorship Attribution in Bangla Literature (AABL) via Transfer Learning\n  using ULMFiT",
                "Authors": "Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury, Ayesha Tasnim",
                "Abstract": "  Authorship Attribution is the task of creating an appropriate\ncharacterization of text that captures the authors' writing style to identify\nthe original author of a given piece of text. With increased anonymity on the\ninternet, this task has become increasingly crucial in various security and\nplagiarism detection fields. Despite significant advancements in other\nlanguages such as English, Spanish, and Chinese, Bangla lacks comprehensive\nresearch in this field due to its complex linguistic feature and sentence\nstructure. Moreover, existing systems are not scalable when the number of\nauthor increases, and the performance drops for small number of samples per\nauthor. In this paper, we propose the use of Average-Stochastic Gradient\nDescent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an\neffective transfer learning approach that addresses the problem of complex\nlinguistic features extraction and scalability for authorship attribution in\nBangla Literature (AABL). We analyze the effect of different tokenization, such\nas word, sub-word, and character level tokenization, and demonstrate the\neffectiveness of these tokenizations in the proposed model. Moreover, we\nintroduce the publicly available Bangla Authorship Attribution Dataset of 16\nauthors (BAAD16) containing 17,966 sample texts and 13.4+ million words to\nsolve the standard dataset scarcity problem and release six variations of\npre-trained language models for use in any Bangla NLP downstream task. For\nevaluation, we used our developed BAAD16 dataset as well as other publicly\navailable datasets. Empirically, our proposed model outperformed\nstate-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.\nFurthermore, we showed that the proposed system scales much better even with an\nincreasing number of authors, and performance remains steady despite few\ntraining samples.\n",
                "publicationDate": "2024-03-08T18:42:59Z",
                "Link": "http://arxiv.org/pdf/2403.05519v1",
                "arxiv_id": "2403.05519v1"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language\n  Models for Violence Inciting Text Detection",
                "Authors": "Saurabh Page, Sudeep Mangalvedhekar, Kshitij Deshpande, Tanmay Chavan, Sheetal Sonawane",
                "Abstract": "  This paper presents our work for the Violence Inciting Text Detection shared\ntask in the First Workshop on Bangla Language Processing. Social media has\naccelerated the propagation of hate and violence-inciting speech in society. It\nis essential to develop efficient mechanisms to detect and curb the propagation\nof such texts. The problem of detecting violence-inciting texts is further\nexacerbated in low-resource settings due to sparse research and less data. The\ndata provided in the shared task consists of texts in the Bangla language,\nwhere each example is classified into one of the three categories defined based\non the types of violence-inciting texts. We try and evaluate several BERT-based\nmodels, and then use an ensemble of the models as our final submission. Our\nsubmission is ranked 10th in the final leaderboard of the shared task with a\nmacro F1 score of 0.737.\n",
                "publicationDate": "2023-11-30T18:23:38Z",
                "Link": "http://arxiv.org/pdf/2311.18778v1",
                "arxiv_id": "2311.18778v1"
            },
            {
                "Title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation\n  Language Models",
                "Authors": "Mitodru Niyogi, Arnab Bhattacharya",
                "Abstract": "  We present Gyan AI Paramanu (\"atom\"), a family of novel language models for\nIndian languages. It is a collection of auto-regressive monolingual, bilingual,\nand multilingual Indic language models pretrained from scratch on a single GPU\nfor 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,\nOdia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,\nTamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are\npretrained with a context size of 1024 on a single GPU. The models are very\nefficient, small, fast, and powerful. We have also developed an efficient most\nadvanced Indic tokenizer that can even tokenize unseen languages. In order to\navoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we\npretrained on comparable corpora by typological grouping using the same script.\nWe performed human evaluation of our pretrained models for open end text\ngeneration on grammar, coherence, creativity, and factuality metrics for\nBangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models\noutperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,\nGPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite\nbeing smaller in size by 66 to 20 times compared to standard 7B LLMs. To run\ninference on our pretrained models, CPU is enough, and GPU is not needed. We\nalso instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu\nmodels on 23k instructions in respective languages. Our pretrained and\ninstruction-tuned models which are first of its kind, most powerful efficient\nsmall generative language models ever developed for Indic languages, and the\nvarious results lead to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and humongous number\nof parameters. We plan to release our models at https://www.bharatgpts.com.\n",
                "publicationDate": "2024-01-31T17:58:10Z",
                "Link": "http://arxiv.org/pdf/2401.18034v1",
                "arxiv_id": "2401.18034v1"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "Document Decomposition of Bangla Printed Text",
                "Authors": "Md. Fahad Hasan, Tasmin Afroz, Sabir Ismail, Md. Saiful Islam",
                "Abstract": "  Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line.\n",
                "publicationDate": "2017-01-27T12:54:52Z",
                "Link": "http://arxiv.org/pdf/1701.08706v1",
                "arxiv_id": "1701.08706v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            }
        ]
    },
    {
        "topic_name": "Multi-document Summarization",
        "summary": "default",
        "papers": [
            {
                "Title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical\n  Approach",
                "Authors": "Lu\u00eds Marujo, Ricardo Ribeiro, David Martins de Matos, Jo\u00e3o P. Neto, Anatole Gershman, Jaime Carbonell",
                "Abstract": "  The increasing amount of online content motivated the development of\nmulti-document summarization methods. In this work, we explore straightforward\napproaches to extend single-document summarization methods to multi-document\nsummarization. The proposed methods are based on the hierarchical combination\nof single-document summaries, and achieves state of the art results.\n",
                "publicationDate": "2015-07-10T13:59:00Z",
                "Link": "http://arxiv.org/pdf/1507.02907v1",
                "arxiv_id": "1507.02907v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            }
        ]
    },
    {
        "topic_name": "Dependency Trees",
        "summary": "default",
        "papers": [
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "An Improved Feature Descriptor for Recognition of Handwritten Bangla\n  Alphabet",
                "Authors": "Nibaran Das, Subhadip Basu, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak kumar Basu",
                "Abstract": "  Appropriate feature set for representation of pattern classes is one of the\nmost important aspects of handwritten character recognition. The effectiveness\nof features depends on the discriminating power of the features chosen to\nrepresent patterns of different classes. However, discriminatory features are\nnot easily measurable. Investigative experimentation is necessary for\nidentifying discriminatory features. In the present work we have identified a\nnew variation of feature set which significantly outperforms on handwritten\nBangla alphabet from the previously used feature set. 132 number of features in\nall viz. modified shadow features, octant and centroid features, distance based\nfeatures, quad tree based longest run features are used here. Using this\nfeature set the recognition performance increases sharply from the 75.05%\nobserved in our previous work [7], to 85.40% on 50 character classes with MLP\nbased classifier on the same dataset.\n",
                "publicationDate": "2015-01-22T13:50:25Z",
                "Link": "http://arxiv.org/pdf/1501.05497v1",
                "arxiv_id": "1501.05497v1"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Implementation Of Back-Propagation Neural Network For Isolated Bangla\n  Speech Recognition",
                "Authors": "Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan",
                "Abstract": "  This paper is concerned with the development of Back-propagation Neural\nNetwork for Bangla Speech Recognition. In this paper, ten bangla digits were\nrecorded from ten speakers and have been recognized. The features of these\nspeech digits were extracted by the method of Mel Frequency Cepstral\nCoefficient (MFCC) analysis. The mfcc features of five speakers were used to\ntrain the network with Back propagation algorithm. The mfcc features of ten\nbangla digit speeches, from 0 to 9, of another five speakers were used to test\nthe system. All the methods and algorithms used in this research were\nimplemented using the features of Turbo C and C++ languages. From our\ninvestigation it is seen that the developed system can successfully encode and\nanalyze the mfcc features of the speech signal to recognition. The developed\nsystem achieved recognition rate about 96.332% for known speakers (i.e.,\nspeaker dependent) and 92% for unknown speakers (i.e., speaker independent).\n",
                "publicationDate": "2013-08-17T14:04:00Z",
                "Link": "http://arxiv.org/pdf/1308.3785v1",
                "arxiv_id": "1308.3785v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "publicationDate": "2019-07-18T01:00:42Z",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Antara Mahmud",
                "Abstract": "  One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n",
                "publicationDate": "2023-09-19T02:59:41Z",
                "Link": "http://arxiv.org/pdf/2309.10272v2",
                "arxiv_id": "2309.10272v2"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "An Enhanced Harmony Search Method for Bangla Handwritten Character\n  Recognition Using Region Sampling",
                "Authors": "Ritesh Sarkhel, Amit K Saha, Nibaran Das",
                "Abstract": "  Identification of minimum number of local regions of a handwritten character\nimage, containing well-defined discriminating features which are sufficient for\na minimal but complete description of the character is a challenging task. A\nnew region selection technique based on the idea of an enhanced Harmony Search\nmethodology has been proposed here. The powerful framework of Harmony Search\nhas been utilized to search the region space and detect only the most\ninformative regions for correctly recognizing the handwritten character. The\nproposed method has been tested on handwritten samples of Bangla Basic,\nCompound and mixed (Basic and Compound characters) characters separately with\nSVM based classifier using a longest run based feature-set obtained from the\nimage subregions formed by a CG based quad-tree partitioning approach. Applying\nthis methodology on the above mentioned three types of datasets, respectively\n43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction\nand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition\naccuracy. The results show a sizeable reduction in the minimal number of\ndescriptive regions as well a significant increase in recognition accuracy for\nall the datasets using the proposed technique. Thus the time and cost related\nto feature extraction is decreased without dampening the corresponding\nrecognition accuracy.\n",
                "publicationDate": "2016-05-02T10:28:07Z",
                "Link": "http://arxiv.org/pdf/1605.00420v1",
                "arxiv_id": "1605.00420v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "Design and implementation of a digital clock showing digits in Bangla\n  font using microcontroller AT89C4051",
                "Authors": "Nasif Muslim, Md. Tanvir Adnan, Mohammad Zahidul Kabir, Md. Humayun Kabir, Sheikh Mominul Islam",
                "Abstract": "  In this paper, a digital clock is designed where the microcontroller is used\nfor timing controller and the font of the Bangla digits are designed, and\nprogrammed within the microcontroller. The design is cost effective, simple and\neasy for maintenance.\n",
                "publicationDate": "2012-08-05T09:22:06Z",
                "Link": "http://arxiv.org/pdf/1208.0995v1",
                "arxiv_id": "1208.0995v1"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            }
        ]
    },
    {
        "topic_name": "Text Classification",
        "summary": "default",
        "papers": [
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Mixed-Distil-BERT: Code-mixed Language Modeling for Bangla, English, and\n  Hindi",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Antara Mahmud",
                "Abstract": "  One of the most popular downstream tasks in the field of Natural Language\nProcessing is text classification. Text classification tasks have become more\ndaunting when the texts are code-mixed. Though they are not exposed to such\ntext during pre-training, different BERT models have demonstrated success in\ntackling Code-Mixed NLP challenges. Again, in order to enhance their\nperformance, Code-Mixed NLP models have depended on combining synthetic data\nwith real-world data. It is crucial to understand how the BERT models'\nperformance is impacted when they are pretrained using corresponding code-mixed\nlanguages. In this paper, we introduce Tri-Distil-BERT, a multilingual model\npre-trained on Bangla, English, and Hindi, and Mixed-Distil-BERT, a model\nfine-tuned on code-mixed data. Both models are evaluated across multiple NLP\ntasks and demonstrate competitive performance against larger models like mBERT\nand XLM-R. Our two-tiered pre-training approach offers efficient alternatives\nfor multilingual and code-mixed language understanding, contributing to\nadvancements in the field.\n",
                "publicationDate": "2023-09-19T02:59:41Z",
                "Link": "http://arxiv.org/pdf/2309.10272v2",
                "arxiv_id": "2309.10272v2"
            },
            {
                "Title": "Towards Detecting, Recognizing, and Parsing the Address Information from\n  Bangla Signboard: A Deep Learning-based Approach",
                "Authors": "Hasan Murad, Mohammed Eunus Ali",
                "Abstract": "  Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.\n",
                "publicationDate": "2023-11-22T08:25:15Z",
                "Link": "http://arxiv.org/pdf/2311.13222v1",
                "arxiv_id": "2311.13222v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer",
                "Authors": "Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed",
                "Abstract": "  Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.\n",
                "publicationDate": "2023-11-06T13:02:07Z",
                "Link": "http://arxiv.org/pdf/2311.03078v1",
                "arxiv_id": "2311.03078v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT\n  Semantic Embeddings K-Means-Infused CRF Model",
                "Authors": "Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque",
                "Abstract": "  Named Entity Recognition (NER) is a sub-task of Natural Language Processing\n(NLP) that distinguishes entities from unorganized text into predefined\ncategorization. In recent years, a lot of Bangla NLP subtasks have received\nquite a lot of attention; but Named Entity Recognition in Bangla still lags\nbehind. In this research, we explored the existing state of research in Bangla\nNamed Entity Recognition. We tried to figure out the limitations that current\ntechniques and datasets face, and we would like to address these limitations in\nour research. Additionally, We developed a Gazetteer that has the ability to\nsignificantly boost the performance of NER. We also proposed a new NER solution\nby taking advantage of state-of-the-art NLP tools that outperform conventional\ntechniques.\n",
                "publicationDate": "2024-01-30T17:47:07Z",
                "Link": "http://arxiv.org/pdf/2401.17206v1",
                "arxiv_id": "2401.17206v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach",
                "Authors": "Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, Iqbal H. Sarker",
                "Abstract": "  Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.\n",
                "publicationDate": "2021-04-17T18:28:39Z",
                "Link": "http://arxiv.org/pdf/2104.08613v1",
                "arxiv_id": "2104.08613v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "HS-BAN: A Benchmark Dataset of Social Media Comments for Hate Speech\n  Detection in Bangla",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  In this paper, we present HS-BAN, a binary class hate speech (HS) dataset in\nBangla language consisting of more than 50,000 labeled comments, including\n40.17% hate and rest are non hate speech. While preparing the dataset a strict\nand detailed annotation guideline was followed to reduce human annotation bias.\nThe HS dataset was also preprocessed linguistically to extract different types\nof slang currently people write using symbols, acronyms, or alternative\nspellings. These slang words were further categorized into traditional and\nnon-traditional slang lists and included in the results of this paper. We\nexplored traditional linguistic features and neural network-based methods to\ndevelop a benchmark system for hate speech detection for the Bangla language.\nOur experimental results show that existing word embedding models trained with\ninformal texts perform better than those trained with formal text. Our\nbenchmark shows that a Bi-LSTM model on top of the FastText informal word\nembedding achieved 86.78% F1-score. We will make the dataset available for\npublic use.\n",
                "publicationDate": "2021-12-03T13:35:18Z",
                "Link": "http://arxiv.org/pdf/2112.01902v1",
                "arxiv_id": "2112.01902v1"
            },
            {
                "Title": "Authorship Attribution in Bangla literature using Character-level CNN",
                "Authors": "Aisha Khatun, Anisur Rahman, Md. Saiful Islam,  Marium-E-Jannat",
                "Abstract": "  Characters are the smallest unit of text that can extract stylometric signals\nto determine the author of a text. In this paper, we investigate the\neffectiveness of character-level signals in Authorship Attribution of Bangla\nLiterature and show that the results are promising but improvable. The time and\nmemory efficiency of the proposed model is much higher than the word level\ncounterparts but accuracy is 2-5% less than the best performing word-level\nmodels. Comparison of various word-based models is performed and shown that the\nproposed model performs increasingly better with larger datasets. We also\nanalyze the effect of pre-training character embedding of diverse Bangla\ncharacter set in authorship attribution. It is seen that the performance is\nimproved by up to 10% on pre-training. We used 2 datasets from 6 to 14 authors,\nbalancing them before training and compare the results.\n",
                "publicationDate": "2020-01-11T14:54:04Z",
                "Link": "http://arxiv.org/pdf/2001.05316v1",
                "arxiv_id": "2001.05316v1"
            },
            {
                "Title": "End to End Bangla Speech Synthesis",
                "Authors": "Prithwiraj Bhattacharjee, Rajan Saha Raju, Arif Ahmad, M. Shahidur Rahman",
                "Abstract": "  Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.\n",
                "publicationDate": "2021-08-01T17:16:03Z",
                "Link": "http://arxiv.org/pdf/2108.00500v1",
                "arxiv_id": "2108.00500v1"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            }
        ]
    },
    {
        "topic_name": "Document Clustering",
        "summary": "default",
        "papers": [
            {
                "Title": "BN-HTRd: A Benchmark Dataset for Document Level Offline Bangla\n  Handwritten Text Recognition (HTR) and Line Segmentation",
                "Authors": "Md. Ataur Rahman, Nazifa Tabassum, Mitu Paul, Riya Pal, Mohammad Khairul Islam",
                "Abstract": "  We introduce a new dataset for offline Handwritten Text Recognition (HTR)\nfrom images of Bangla scripts comprising words, lines, and document-level\nannotations. The BN-HTRd dataset is based on the BBC Bangla News corpus, meant\nto act as ground truth texts. These texts were subsequently used to generate\nthe annotations that were filled out by people with their handwriting. Our\ndataset includes 788 images of handwritten pages produced by approximately 150\ndifferent writers. It can be adopted as a basis for various handwriting\nclassification tasks such as end-to-end document recognition, word-spotting,\nword or line segmentation, and so on. We also propose a scheme to segment\nBangla handwritten document images into corresponding lines in an unsupervised\nmanner. Our line segmentation approach takes care of the variability involved\nin different writing styles, accurately segmenting complex handwritten text\nlines of curvilinear nature. Along with a bunch of pre-processing and\nmorphological operations, both Hough line and circle transforms were employed\nto distinguish different linear components. In order to arrange those\ncomponents into their corresponding lines, we followed an unsupervised\nclustering approach. The average success rate of our segmentation technique is\n81.57% in terms of FM metrics (similar to F-measure) with a mean Average\nPrecision (mAP) of 0.547.\n",
                "publicationDate": "2022-05-29T22:56:26Z",
                "Link": "http://arxiv.org/pdf/2206.08977v1",
                "arxiv_id": "2206.08977v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Ensemble of Anchor-Free Models for Robust Bangla Document Layout\n  Segmentation",
                "Authors": "U Mong Sain Chak, Md. Asib Rahman",
                "Abstract": "  In this research paper, we introduce a novel approach designed for the\npurpose of segmenting the layout of Bangla documents. Our methodology involves\nthe utilization of a sophisticated ensemble of YOLOv8 models, which were\ntrained for the DL Sprint 2.0 - BUET CSE Fest 2023 Competition focused on\nBangla document layout segmentation. Our primary emphasis lies in enhancing\nvarious aspects of the task, including techniques such as image augmentation,\nmodel architecture, and the incorporation of model ensembles. We deliberately\nreduce the quality of a subset of document images to enhance the resilience of\nmodel training, thereby resulting in an improvement in our cross-validation\nscore. By employing Bayesian optimization, we determine the optimal confidence\nand Intersection over Union (IoU) thresholds for our model ensemble. Through\nour approach, we successfully demonstrate the effectiveness of anchor-free\nmodels in achieving robust layout segmentation in Bangla documents.\n",
                "publicationDate": "2023-08-28T08:24:25Z",
                "Link": "http://arxiv.org/pdf/2308.14397v2",
                "arxiv_id": "2308.14397v2"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits",
                "Authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip Basu",
                "Abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n",
                "publicationDate": "2010-03-30T18:54:57Z",
                "Link": "http://arxiv.org/pdf/1003.5897v1",
                "arxiv_id": "1003.5897v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Document Decomposition of Bangla Printed Text",
                "Authors": "Md. Fahad Hasan, Tasmin Afroz, Sabir Ismail, Md. Saiful Islam",
                "Abstract": "  Today all kind of information is getting digitized and along with all this\ndigitization, the huge archive of various kinds of documents is being digitized\ntoo. We know that, Optical Character Recognition is the method through which,\nnewspapers and other paper documents convert into digital resources. But, it is\na fact that this method works on texts only. As a result, if we try to process\nany document which contains non-textual zones, then we will get garbage texts\nas output. That is why; in order to digitize documents properly they should be\nprepossessed carefully. And while preprocessing, segmenting document in\ndifferent regions according to the category properly is most important. But,\nthe Optical Character Recognition processes available for Bangla language have\nno such algorithm that can categorize a newspaper/book page fully. So we worked\nto decompose a document into its several parts like headlines, sub headlines,\ncolumns, images etc. And if the input is skewed and rotated, then the input was\nalso deskewed and de-rotated. To decompose any Bangla document we found out the\nedges of the input image. Then we find out the horizontal and vertical area of\nevery pixel where it lies in. Later on the input image was cut according to\nthese areas. Then we pick each and every sub image and found out their\nheight-width ratio, line height. Then according to these values the sub images\nwere categorized. To deskew the image we found out the skew angle and de skewed\nthe image according to this angle. To de-rotate the image we used the line\nheight, matra line, pixel ratio of matra line.\n",
                "publicationDate": "2017-01-27T12:54:52Z",
                "Link": "http://arxiv.org/pdf/1701.08706v1",
                "arxiv_id": "1701.08706v1"
            },
            {
                "Title": "Word level Script Identification from Bangla and Devanagri Handwritten\n  Texts mixed with Roman Script",
                "Authors": "Ram Sarkar, Nibaran Das, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  India is a multi-lingual country where Roman script is often used alongside\ndifferent Indic scripts in a text document. To develop a script specific\nhandwritten Optical Character Recognition (OCR) system, it is therefore\nnecessary to identify the scripts of handwritten text correctly. In this paper,\nwe present a system, which automatically separates the scripts of handwritten\nwords from a document, written in Bangla or Devanagri mixed with Roman scripts.\nIn this script separation technique, we first, extract the text lines and words\nfrom document pages using a script independent Neighboring Component Analysis\ntechnique. Then we have designed a Multi Layer Perceptron (MLP) based\nclassifier for script separation, trained with 8 different wordlevel holistic\nfeatures. Two equal sized datasets, one with Bangla and Roman scripts and the\nother with Devanagri and Roman scripts, are prepared for the system evaluation.\nOn respective independent text samples, word-level script identification\naccuracies of 99.29% and 98.43% are achieved.\n",
                "publicationDate": "2010-02-21T19:48:16Z",
                "Link": "http://arxiv.org/pdf/1002.4007v1",
                "arxiv_id": "1002.4007v1"
            },
            {
                "Title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model",
                "Authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam",
                "Abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n",
                "publicationDate": "2019-11-19T20:37:03Z",
                "Link": "http://arxiv.org/pdf/1911.11062v1",
                "arxiv_id": "1911.11062v1"
            },
            {
                "Title": "Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout\n  Analysis",
                "Authors": "Shrestha Datta, Md Adith Mollah, Raisa Fairooz, Tariful Islam Fahim",
                "Abstract": "  Understanding digital documents is like solving a puzzle, especially\nhistorical ones. Document Layout Analysis (DLA) helps with this puzzle by\ndividing documents into sections like paragraphs, images, and tables. This is\ncrucial for machines to read and understand these documents. In the DL Sprint\n2.0 competition, we worked on understanding Bangla documents. We used a dataset\ncalled BaDLAD with lots of examples. We trained a special model called Mask\nR-CNN to help with this understanding. We made this model better by\nstep-by-step hyperparameter tuning, and we achieved a good dice score of 0.889.\nHowever, not everything went perfectly. We tried using a model trained for\nEnglish documents, but it didn't fit well with Bangla. This showed us that each\nlanguage has its own challenges. Our solution for the DL Sprint 2.0 is publicly\navailable at https://www.kaggle.com/competitions/dlsprint2/discussion/432201\nalong with notebooks, weights, and inference notebook.\n",
                "publicationDate": "2023-08-21T06:51:58Z",
                "Link": "http://arxiv.org/pdf/2308.10511v2",
                "arxiv_id": "2308.10511v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "BN-DRISHTI: Bangla Document Recognition through Instance-level\n  Segmentation of Handwritten Text Images",
                "Authors": "Sheikh Mohammad Jubaer, Nazifa Tabassum, Md. Ataur Rahman, Mohammad Khairul Islam",
                "Abstract": "  Handwriting recognition remains challenging for some of the most spoken\nlanguages, like Bangla, due to the complexity of line and word segmentation\nbrought by the curvilinear nature of writing and lack of quality datasets. This\npaper solves the segmentation problem by introducing a state-of-the-art method\n(BN-DRISHTI) that combines a deep learning-based object detection framework\n(YOLO) with Hough and Affine transformation for skew correction. However,\ntraining deep learning models requires a massive amount of data. Thus, we also\npresent an extended version of the BN-HTRd dataset comprising 786 full-page\nhandwritten Bangla document images, line and word-level annotation for\nsegmentation, and corresponding ground truths for word recognition. Evaluation\non the test portion of our dataset resulted in an F-score of 99.97% for line\nand 98% for word segmentation. For comparative analysis, we used three external\nBangla handwritten datasets, namely BanglaWriting, WBSUBNdb_text, and ICDAR\n2013, where our system outperformed by a significant margin, further justifying\nthe performance of our approach on completely unseen samples.\n",
                "publicationDate": "2023-05-31T04:08:57Z",
                "Link": "http://arxiv.org/pdf/2306.09351v1",
                "arxiv_id": "2306.09351v1"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Word Segmentation from Unconstrained Handwritten Bangla Document Images\n  using Distance Transform",
                "Authors": "Pawan Kumar Singh, Shubham Sinha, Sagnik Pal Chowdhury, Ram Sarkar, Mita Nasipuri",
                "Abstract": "  Segmentation of handwritten document images into text lines and words is one\nof the most significant and challenging tasks in the development of a complete\nOptical Character Recognition (OCR) system. This paper addresses the automatic\nsegmentation of text words directly from unconstrained Bangla handwritten\ndocument images. The popular Distance transform (DT) algorithm is applied for\nlocating the outer boundary of the word images. This technique is free from\ngenerating the over-segmented words. A simple post-processing procedure is\napplied to isolate the under-segmented word images, if any. The proposed\ntechnique is tested on 50 random images taken from CMATERdb1.1.1 database.\nSatisfactory result is achieved with a segmentation accuracy of 91.88% which\nconfirms the robustness of the proposed methodology.\n",
                "publicationDate": "2020-09-17T03:14:27Z",
                "Link": "http://arxiv.org/pdf/2009.08037v1",
                "arxiv_id": "2009.08037v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Zone-based Keyword Spotting in Bangla and Devanagari Documents",
                "Authors": "Ayan Kumar Bhunia, Partha Pratim Roy, Umapada Pal",
                "Abstract": "  In this paper we present a word spotting system in text lines for offline\nIndic scripts such as Bangla (Bengali) and Devanagari. Recently, it was shown\nthat zone-wise recognition method improves the word recognition performance\nthan conventional full word recognition system in Indic scripts. Inspired with\nthis idea we consider the zone segmentation approach and use middle zone\ninformation to improve the traditional word spotting performance. To avoid the\nproblem of zone segmentation using heuristic approach, we propose here an HMM\nbased approach to segment the upper and lower zone components from the text\nline images. The candidate keywords are searched from a line without segmenting\ncharacters or words. Also, we propose a novel feature combining foreground and\nbackground information of text line images for keyword-spotting by character\nfiller models. A significant improvement in performance is noted by using both\nforeground and background information than their individual one. Pyramid\nHistogram of Oriented Gradient (PHOG) feature has been used in our word\nspotting framework. From the experiment, it has been noted that the proposed\nzone-segmentation based system outperforms traditional approaches of word\nspotting.\n",
                "publicationDate": "2017-12-05T01:12:25Z",
                "Link": "http://arxiv.org/pdf/1712.01434v1",
                "arxiv_id": "1712.01434v1"
            },
            {
                "Title": "N-gram Statistical Stemmer for Bangla Corpus",
                "Authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui",
                "Abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n",
                "publicationDate": "2019-12-25T07:31:44Z",
                "Link": "http://arxiv.org/pdf/1912.11612v1",
                "arxiv_id": "1912.11612v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval",
                "Authors": "Ranju Mandal, Partha Pratim Roy, Umapada Pal, Michael Blumenstein",
                "Abstract": "  An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches.\n",
                "publicationDate": "2018-07-18T04:29:20Z",
                "Link": "http://arxiv.org/pdf/1807.06772v1",
                "arxiv_id": "1807.06772v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "publicationDate": "2019-07-18T01:00:42Z",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1"
            },
            {
                "Title": "A comparison of two suffix tree-based document clustering algorithms",
                "Authors": "Muhammad Rafi, M. Maujood, M. M. Fazal, S. M. Ali",
                "Abstract": "  Document clustering as an unsupervised approach extensively used to navigate,\nfilter, summarize and manage large collection of document repositories like the\nWorld Wide Web (WWW). Recently, focuses in this domain shifted from traditional\nvector based document similarity for clustering to suffix tree based document\nsimilarity, as it offers more semantic representation of the text present in\nthe document. In this paper, we compare and contrast two recently introduced\napproaches to document clustering based on suffix tree data model. The first is\nan Efficient Phrase based document clustering, which extracts phrases from\ndocuments to form compact document representation and uses a similarity measure\nbased on common suffix tree to cluster the documents. The second approach is a\nfrequent word/word meaning sequence based document clustering, it similarly\nextracts the common word sequence from the document and uses the common\nsequence/ common word meaning sequence to perform the compact representation,\nand finally, it uses document clustering approach to cluster the compact\ndocuments. These algorithms are using agglomerative hierarchical document\nclustering to perform the actual clustering step, the difference in these\napproaches are mainly based on extraction of phrases, model representation as a\ncompact document, and the similarity measures used for clustering. This paper\ninvestigates the computational aspect of the two algorithms, and the quality of\nresults they produced.\n",
                "publicationDate": "2011-12-29T04:25:10Z",
                "Link": "http://arxiv.org/pdf/1112.6222v2",
                "arxiv_id": "1112.6222v2"
            },
            {
                "Title": "Clustering Document Parts: Detecting and Characterizing Influence\n  Campaigns from Documents",
                "Authors": "Zhengxiang Wang, Owen Rambow",
                "Abstract": "  We propose a novel clustering pipeline to detect and characterize influence\ncampaigns from documents. This approach clusters parts of document, detects\nclusters that likely reflect an influence campaign, and then identifies\ndocuments linked to an influence campaign via their association with the\nhigh-influence clusters. Our approach outperforms both the direct\ndocument-level classification and the direct document-level clustering approach\nin predicting if a document is part of an influence campaign. We propose\nvarious novel techniques to enhance our pipeline, including using an existing\nevent factuality prediction system to obtain document parts, and aggregating\nmultiple clustering experiments to improve the performance of both cluster and\ndocument classification. Classifying documents after clustering not only\naccurately extracts the parts of the documents that are relevant to influence\ncampaigns, but also captures influence campaigns as a coordinated and holistic\nphenomenon. Our approach makes possible more fine-grained and interpretable\ncharacterizations of influence campaigns from documents.\n",
                "publicationDate": "2024-02-27T02:36:43Z",
                "Link": "http://arxiv.org/pdf/2402.17151v2",
                "arxiv_id": "2402.17151v2"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "Transfer Learning for Scene Text Recognition in Indian Languages",
                "Authors": "Sanjana Gunna, Rohit Saluja, C. V. Jawahar",
                "Abstract": "  Scene text recognition in low-resource Indian languages is challenging\nbecause of complexities like multiple scripts, fonts, text size, and\norientations. In this work, we investigate the power of transfer learning for\nall the layers of deep scene text recognition networks from English to two\ncommon Indian languages. We perform experiments on the conventional CRNN model\nand STAR-Net to ensure generalisability. To study the effect of change in\ndifferent scripts, we initially run our experiments on synthetic word images\nrendered using Unicode fonts. We show that the transfer of English models to\nsimple synthetic datasets of Indian languages is not practical. Instead, we\npropose to apply transfer learning techniques among Indian languages due to\nsimilarity in their n-gram distributions and visual features like the vowels\nand conjunct characters. We then study the transfer learning among six Indian\nlanguages with varying complexities in fonts and word length statistics. We\nalso demonstrate that the learned features of the models transferred from other\nIndian languages are visually closer (and sometimes even better) to the\nindividual model features than those transferred from English. We finally set\nnew benchmarks for scene-text recognition on Hindi, Telugu, and Malayalam\ndatasets from IIIT-ILST and Bangla dataset from MLT-17 by achieving 6%, 5%, 2%,\nand 23% gains in Word Recognition Rates (WRRs) compared to previous works. We\nfurther improve the MLT-17 Bangla results by plugging in a novel correction\nBiLSTM into our model. We additionally release a dataset of around 440 scene\nimages containing 500 Gujarati and 2535 Tamil words. WRRs improve over the\nbaselines by 8%, 4%, 5%, and 3% on the MLT-19 Hindi and Bangla datasets and the\nGujarati and Tamil datasets.\n",
                "publicationDate": "2022-01-10T06:14:49Z",
                "Link": "http://arxiv.org/pdf/2201.03180v1",
                "arxiv_id": "2201.03180v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            }
        ]
    },
    {
        "topic_name": "Machine Assisted Translation",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
                "Authors": "MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee",
                "Abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
                "publicationDate": "2024-02-21T23:43:04Z",
                "Link": "http://arxiv.org/pdf/2402.14179v1",
                "arxiv_id": "2402.14179v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "Deep Learning Approach for Classifying the Aggressive Comments on Social\n  Media: Machine Translated Data Vs Real Life Data",
                "Authors": "Mst Shapna Akter, Hossain Shahriar, Nova Ahmed, Alfredo Cuzzocrea",
                "Abstract": "  Aggressive comments on social media negatively impact human life. Such\noffensive contents are responsible for depression and suicidal-related\nactivities. Since online social networking is increasing day by day, the hate\ncontent is also increasing. Several investigations have been done on the domain\nof cyberbullying, cyberaggression, hate speech, etc. The majority of the\ninquiry has been done in the English language. Some languages (Hindi and\nBangla) still lack proper investigations due to the lack of a dataset. This\npaper particularly worked on the Hindi, Bangla, and English datasets to detect\naggressive comments and have shown a novel way of generating machine-translated\ndata to resolve data unavailability issues. A fully machine-translated English\ndataset has been analyzed with the models such as the Long Short term memory\nmodel (LSTM), Bidirectional Long-short term memory model (BiLSTM),\nLSTM-Autoencoder, word2vec, Bidirectional Encoder Representations from\nTransformers (BERT), and generative pre-trained transformer (GPT-2) to make an\nobservation on how the models perform on a machine-translated noisy dataset. We\nhave compared the performance of using the noisy data with two more datasets\nsuch as raw data, which does not contain any noises, and semi-noisy data, which\ncontains a certain amount of noisy data. We have classified both the raw and\nsemi-noisy data using the aforementioned models. To evaluate the performance of\nthe models, we have used evaluation metrics such as F1-score,accuracy,\nprecision, and recall. We have achieved the highest accuracy on raw data using\nthe gpt2 model, semi-noisy data using the BERT model, and fully\nmachine-translated data using the BERT model. Since many languages do not have\nproper data availability, our approach will help researchers create\nmachine-translated datasets for several analysis purposes.\n",
                "publicationDate": "2023-03-13T21:43:08Z",
                "Link": "http://arxiv.org/pdf/2303.07484v1",
                "arxiv_id": "2303.07484v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on\n  Social Media Using Synthetic Data",
                "Authors": "Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea",
                "Abstract": "  Social media cyberbullying has a detrimental effect on human life. As online\nsocial networking grows daily, the amount of hate speech also increases. Such\nterrible content can cause depression and actions related to suicide. This\npaper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection\non social media using synthetic data. We have demonstrated a cutting-edge\nmethod to address data availability difficulties by producing\nmachine-translated data. However, several languages such as Hindi and Bangla\nstill lack adequate investigations due to a lack of datasets. We carried out\nexperimental identification of aggressive comments on Hindi, Bangla, and\nEnglish datasets using the proposed model and traditional models, including\nLong Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),\nLSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from\nTransformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.\nWe employed evaluation metrics such as f1-score, accuracy, precision, and\nrecall to assess the models performance. Our proposed model outperformed all\nthe models on all datasets, achieving the highest accuracy of 95%. Our model\nachieves state-of-the-art results among all the previous works on the dataset\nwe used in this paper.\n",
                "publicationDate": "2023-08-15T17:20:05Z",
                "Link": "http://arxiv.org/pdf/2308.09722v1",
                "arxiv_id": "2308.09722v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "An Opinion Mining of Text in COVID-19 Issues along with Comparative\n  Study in ML, BERT & RNN",
                "Authors": "Md. Mahadi Hasan Sany, Mumenunnesa Keya, Sharun Akter Khushbu, Akm Shahariar Azad Rabby, Abu Kaisar Mohammad Masum",
                "Abstract": "  The global world is crossing a pandemic situation where this is a\ncatastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a\nglobal threat all over the 212 countries that people every day meet with mighty\nsituations. On the contrary, thousands of infected people live rich in\nmountains. Mental health is also affected by this worldwide coronavirus\nsituation. Due to this situation online sources made a communicative place that\ncommon people shares their opinion in any agenda. Such as affected news related\npositive and negative, financial issues, country and family crisis, lack of\nimport and export earning system etc. different kinds of circumstances are\nrecent trendy news in anywhere. Thus, vast amounts of text are produced within\nmoments therefore, in subcontinent areas the same as situation in other\ncountries and peoples opinion of text and situation also same but the language\nis different. This article has proposed some specific inputs along with Bangla\ntext comments from individual sources which can assure the goal of illustration\nthat machine learning outcome capable of building an assistive system. Opinion\nmining assistive system can be impactful in all language preferences possible.\nTo the best of our knowledge, the article predicted the Bangla input text on\nCOVID-19 issues proposed ML algorithms and deep learning models analysis also\ncheck the future reachability with a comparative analysis. Comparative analysis\nstates a report on text prediction accuracy is 91% along with ML algorithms and\n79% along with Deep Learning Models.\n",
                "publicationDate": "2022-01-06T15:59:20Z",
                "Link": "http://arxiv.org/pdf/2201.02119v1",
                "arxiv_id": "2201.02119v1"
            },
            {
                "Title": "Improving the quality of Gujarati-Hindi Machine Translation through\n  part-of-speech tagging and stemmer-assisted transliteration",
                "Authors": "Juhi Ameta, Nisheeth Joshi, Iti Mathur",
                "Abstract": "  Machine Translation for Indian languages is an emerging research area.\nTransliteration is one such module that we design while designing a translation\nsystem. Transliteration means mapping of source language text into the target\nlanguage. Simple mapping decreases the efficiency of overall translation\nsystem. We propose the use of stemming and part-of-speech tagging for\ntransliteration. The effectiveness of translation can be improved if we use\npart-of-speech tagging and stemming assisted transliteration.We have shown that\nmuch of the content in Gujarati gets transliterated while being processed for\ntranslation to Hindi language.\n",
                "publicationDate": "2013-07-12T03:05:29Z",
                "Link": "http://arxiv.org/pdf/1307.3310v1",
                "arxiv_id": "1307.3310v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "publicationDate": "2023-12-21T16:52:41Z",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            }
        ]
    },
    {
        "topic_name": "Coreference Resolution",
        "summary": "default",
        "papers": [
            {
                "Title": "$2 * n$ is better than $n^2$: Decomposing Event Coreference Resolution\n  into Two Tractable Problems",
                "Authors": "Shafiuddin Rehan Ahmed, Abhijnan Nath, James H. Martin, Nikhil Krishnaswamy",
                "Abstract": "  Event Coreference Resolution (ECR) is the task of linking mentions of the\nsame event either within or across documents. Most mention pairs are not\ncoreferent, yet many that are coreferent can be identified through simple\ntechniques such as lemma matching of the event triggers or the sentences in\nwhich they appear. Existing methods for training coreference systems sample\nfrom a largely skewed distribution, making it difficult for the algorithm to\nlearn coreference beyond surface matching. Additionally, these methods are\nintractable because of the quadratic operations needed. To address these\nchallenges, we break the problem of ECR into two parts: a) a heuristic to\nefficiently filter out a large number of non-coreferent pairs, and b) a\ntraining approach on a balanced set of coreferent and non-coreferent mention\npairs. By following this approach, we show that we get comparable results to\nthe state of the art on two popular ECR datasets while significantly reducing\ncompute requirements. We also analyze the mention pairs that are \"hard\" to\naccurately classify as coreferent or non-coreferent. Code at\nhttps://github.com/ahmeshaf/lemma_ce_coref\n",
                "publicationDate": "2023-05-09T05:33:32Z",
                "Link": "http://arxiv.org/pdf/2305.05672v1",
                "arxiv_id": "2305.05672v1"
            },
            {
                "Title": "Integrating knowledge bases to improve coreference and bridging\n  resolution for the chemical domain",
                "Authors": "Pengcheng Lu, Massimo Poesio",
                "Abstract": "  Resolving coreference and bridging relations in chemical patents is important\nfor better understanding the precise chemical process, where chemical domain\nknowledge is very critical. We proposed an approach incorporating external\nknowledge into a multi-task learning model for both coreference and bridging\nresolution in the chemical domain. The results show that integrating external\nknowledge can benefit both chemical coreference and bridging resolution.\n",
                "publicationDate": "2024-04-16T16:15:19Z",
                "Link": "http://arxiv.org/pdf/2404.10696v1",
                "arxiv_id": "2404.10696v1"
            },
            {
                "Title": "DialogRE^C+: An Extension of DialogRE to Investigate How Much\n  Coreference Helps Relation Extraction in Dialogs",
                "Authors": "Yiyun Xiong, Mengwei Dai, Fei Li, Hao Fei, Bobo Li, Shengqiong Wu, Donghong Ji, Chong Teng",
                "Abstract": "  Dialogue relation extraction (DRE) that identifies the relations between\nargument pairs in dialogue text, suffers much from the frequent occurrence of\npersonal pronouns, or entity and speaker coreference. This work introduces a\nnew benchmark dataset DialogRE^C+, introducing coreference resolution into the\nDRE scenario. With the aid of high-quality coreference knowledge, the reasoning\nof argument relations is expected to be enhanced. In DialogRE^C+ dataset, we\nmanually annotate total 5,068 coreference chains over 36,369 argument mentions\nbased on the existing DialogRE data, where four different coreference chain\ntypes namely speaker chain, person chain, location chain and organization chain\nare explicitly marked. We further develop 4 coreference-enhanced graph-based\nDRE models, which learn effective coreference representations for improving the\nDRE task. We also train a coreference resolution model based on our annotations\nand evaluate the effect of automatically extracted coreference chains\ndemonstrating the practicality of our dataset and its potential to other\ndomains and tasks.\n",
                "publicationDate": "2023-08-08T18:03:29Z",
                "Link": "http://arxiv.org/pdf/2308.04498v2",
                "arxiv_id": "2308.04498v2"
            },
            {
                "Title": "End-to-End Neural Event Coreference Resolution",
                "Authors": "Yaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, Le Sun",
                "Abstract": "  Traditional event coreference systems usually rely on pipeline framework and\nhand-crafted features, which often face error propagation problem and have poor\ngeneralization ability. In this paper, we propose an End-to-End Event\nCoreference approach -- E3C neural network, which can jointly model event\ndetection and event coreference resolution tasks, and learn to extract features\nfrom raw text automatically. Furthermore, because event mentions are highly\ndiversified and event coreference is intricately governed by long-distance,\nsemantic-dependent decisions, a type-guided event coreference mechanism is\nfurther proposed in our E3C neural network. Experiments show that our method\nachieves new state-of-the-art performance on two standard datasets.\n",
                "publicationDate": "2020-09-17T09:00:59Z",
                "Link": "http://arxiv.org/pdf/2009.08153v1",
                "arxiv_id": "2009.08153v1"
            },
            {
                "Title": "Filling in the Gaps: Efficient Event Coreference Resolution using Graph\n  Autoencoder Networks",
                "Authors": "Loic De Langhe, Orph\u00e9e De Clercq, Veronique Hoste",
                "Abstract": "  We introduce a novel and efficient method for Event Coreference Resolution\n(ECR) applied to a lower-resourced language domain. By framing ECR as a graph\nreconstruction task, we are able to combine deep semantic embeddings with\nstructural coreference chain knowledge to create a parameter-efficient family\nof Graph Autoencoder models (GAE). Our method significantly outperforms\nclassical mention-pair methods on a large Dutch event coreference corpus in\nterms of overall score, efficiency and training speed. Additionally, we show\nthat our models are consistently able to classify more difficult coreference\nlinks and are far more robust in low-data settings when compared to\ntransformer-based mention-pair coreference algorithms.\n",
                "publicationDate": "2023-10-18T13:44:58Z",
                "Link": "http://arxiv.org/pdf/2310.11965v1",
                "arxiv_id": "2310.11965v1"
            },
            {
                "Title": "Investigating Failures to Generalize for Coreference Resolution Models",
                "Authors": "Ian Porada, Alexandra Olteanu, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung",
                "Abstract": "  Coreference resolution models are often evaluated on multiple datasets.\nDatasets vary, however, in how coreference is realized -- i.e., how the\ntheoretical concept of coreference is operationalized in the dataset -- due to\nfactors such as the choice of corpora and annotation guidelines. We investigate\nthe extent to which errors of current coreference resolution models are\nassociated with existing differences in operationalization across datasets\n(OntoNotes, PreCo, and Winogrande). Specifically, we distinguish between and\nbreak down model performance into categories corresponding to several types of\ncoreference, including coreferring generic mentions, compound modifiers, and\ncopula predicates, among others. This break down helps us investigate how\nstate-of-the-art models might vary in their ability to generalize across\ndifferent coreference types. In our experiments, for example, models trained on\nOntoNotes perform poorly on generic mentions and copula predicates in PreCo.\nOur findings help calibrate expectations of current coreference resolution\nmodels; and, future work can explicitly account for those types of coreference\nthat are empirically associated with poor generalization when developing\nmodels.\n",
                "publicationDate": "2023-03-16T05:32:02Z",
                "Link": "http://arxiv.org/pdf/2303.09092v1",
                "arxiv_id": "2303.09092v1"
            },
            {
                "Title": "Cross-document Coreference Resolution over Predicted Mentions",
                "Authors": "Arie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, Ido Dagan",
                "Abstract": "  Coreference resolution has been mostly investigated within a single document\nscope, showing impressive progress in recent years based on end-to-end models.\nHowever, the more challenging task of cross-document (CD) coreference\nresolution remained relatively under-explored, with the few recent models\napplied only to gold mentions. Here, we introduce the first end-to-end model\nfor CD coreference resolution from raw text, which extends the prominent model\nfor within-document coreference to the CD setting. Our model achieves\ncompetitive results for event and entity coreference resolution on gold\nmentions. More importantly, we set first baseline results, on the standard ECB+\ndataset, for CD coreference resolution over predicted mentions. Further, our\nmodel is simpler and more efficient than recent CD coreference resolution\nsystems, while not using any external resources.\n",
                "publicationDate": "2021-06-02T14:56:28Z",
                "Link": "http://arxiv.org/pdf/2106.01210v1",
                "arxiv_id": "2106.01210v1"
            },
            {
                "Title": "Parallel Data Helps Neural Entity Coreference Resolution",
                "Authors": "Gongbo Tang, Christian Hardmeier",
                "Abstract": "  Coreference resolution is the task of finding expressions that refer to the\nsame entity in a text. Coreference models are generally trained on monolingual\nannotated data but annotating coreference is expensive and challenging.\nHardmeier et al.(2013) have shown that parallel data contains latent anaphoric\nknowledge, but it has not been explored in end-to-end neural models yet. In\nthis paper, we propose a simple yet effective model to exploit coreference\nknowledge from parallel data. In addition to the conventional modules learning\ncoreference from annotations, we introduce an unsupervised module to capture\ncross-lingual coreference knowledge. Our proposed cross-lingual model achieves\nconsistent improvements, up to 1.74 percentage points, on the OntoNotes 5.0\nEnglish dataset using 9 different synthetic parallel datasets. These\nexperimental results confirm that parallel data can provide additional\ncoreference knowledge which is beneficial to coreference resolution tasks.\n",
                "publicationDate": "2023-05-28T12:30:23Z",
                "Link": "http://arxiv.org/pdf/2305.17709v1",
                "arxiv_id": "2305.17709v1"
            },
            {
                "Title": "Light Coreference Resolution for Russian with Hierarchical Discourse\n  Features",
                "Authors": "Elena Chistova, Ivan Smirnov",
                "Abstract": "  Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.\n",
                "publicationDate": "2023-06-02T11:41:24Z",
                "Link": "http://arxiv.org/pdf/2306.01465v1",
                "arxiv_id": "2306.01465v1"
            },
            {
                "Title": "An Annotated Dataset of Coreference in English Literature",
                "Authors": "David Bamman, Olivia Lewke, Anya Mansoor",
                "Abstract": "  We present in this work a new dataset of coreference annotations for works of\nliterature in English, covering 29,103 mentions in 210,532 tokens from 100\nworks of fiction. This dataset differs from previous coreference datasets in\ncontaining documents whose average length (2,105.3 words) is four times longer\nthan other benchmark datasets (463.7 for OntoNotes), and contains examples of\ndifficult coreference problems common in literature. This dataset allows for an\nevaluation of cross-domain performance for the task of coreference resolution,\nand analysis into the characteristics of long-distance within-document\ncoreference.\n",
                "publicationDate": "2019-12-03T00:58:01Z",
                "Link": "http://arxiv.org/pdf/1912.01140v2",
                "arxiv_id": "1912.01140v2"
            },
            {
                "Title": "Neural Coreference Resolution for Arabic",
                "Authors": "Abdulrahman Aloraini, Juntao Yu, Massimo Poesio",
                "Abstract": "  No neural coreference resolver for Arabic exists, in fact we are not aware of\nany learning-based coreference resolver for Arabic since (Bjorkelund and Kuhn,\n2014). In this paper, we introduce a coreference resolution system for Arabic\nbased on Lee et al's end to end architecture combined with the Arabic version\nof bert and an external mention detector. As far as we know, this is the first\nneural coreference resolution system aimed specifically to Arabic, and it\nsubstantially outperforms the existing state of the art on OntoNotes 5.0 with a\ngain of 15.2 points conll F1. We also discuss the current limitations of the\ntask for Arabic and possible approaches that can tackle these challenges.\n",
                "publicationDate": "2020-10-31T14:34:43Z",
                "Link": "http://arxiv.org/pdf/2011.00286v1",
                "arxiv_id": "2011.00286v1"
            },
            {
                "Title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline",
                "Authors": "Peter Sch\u00fcller, K\u00fcbra C\u0131ng\u0131ll\u0131, Ferit Tun\u00e7er, Bar\u0131\u015f G\u00fcn S\u00fcrmeli, Ay\u015feg\u00fcl Pekel, Ay\u015fe Hande Karatay, Hacer Ezgi Karaka\u015f",
                "Abstract": "  We describe the Marmara Turkish Coreference Corpus, which is an annotation of\nthe whole METU-Sabanci Turkish Treebank with mentions and coreference chains.\nCollecting eight or more independent annotations for each document allowed for\nfully automatic adjudication. We provide a baseline system for Turkish mention\ndetection and coreference resolution and evaluate it on the corpus.\n",
                "publicationDate": "2017-06-06T17:25:36Z",
                "Link": "http://arxiv.org/pdf/1706.01863v2",
                "arxiv_id": "1706.01863v2"
            },
            {
                "Title": "Coreference Resolution in Research Papers from Multiple Domains",
                "Authors": "Arthur Brack, Daniel Uwe M\u00fcller, Anett Hoppe, Ralph Ewerth",
                "Abstract": "  Coreference resolution is essential for automatic text understanding to\nfacilitate high-level information retrieval tasks such as text summarisation or\nquestion answering. Previous work indicates that the performance of\nstate-of-the-art approaches (e.g. based on BERT) noticeably declines when\napplied to scientific papers. In this paper, we investigate the task of\ncoreference resolution in research papers and subsequent knowledge graph\npopulation. We present the following contributions: (1) We annotate a corpus\nfor coreference resolution that comprises 10 different scientific disciplines\nfrom Science, Technology, and Medicine (STM); (2) We propose transfer learning\nfor automatic coreference resolution in research papers; (3) We analyse the\nimpact of coreference resolution on knowledge graph (KG) population; (4) We\nrelease a research KG that is automatically populated from 55,485 papers in 10\nSTM domains. Comprehensive experiments show the usefulness of the proposed\napproach. Our transfer learning approach considerably outperforms\nstate-of-the-art baselines on our corpus with an F1 score of 61.4 (+11.0),\nwhile the evaluation against a gold standard KG shows that coreference\nresolution improves the quality of the populated KG significantly with an F1\nscore of 63.5 (+21.8).\n",
                "publicationDate": "2021-01-04T10:52:55Z",
                "Link": "http://arxiv.org/pdf/2101.00884v1",
                "arxiv_id": "2101.00884v1"
            },
            {
                "Title": "\u00daFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual\n  Coreference Resolution",
                "Authors": "Milan Straka",
                "Abstract": "  We present CorPipe, the winning entry to the CRAC 2023 Shared Task on\nMultilingual Coreference Resolution. Our system is an improved version of our\nearlier multilingual coreference pipeline, and it surpasses other participants\nby a large margin of 4.5 percent points. CorPipe first performs mention\ndetection, followed by coreference linking via an antecedent-maximization\napproach on the retrieved spans. Both tasks are trained jointly on all\navailable corpora using a shared pretrained language model. Our main\nimprovements comprise inputs larger than 512 subwords and changing the mention\ndecoding to support ensembling. The source code is available at\nhttps://github.com/ufal/crac2023-corpipe.\n",
                "publicationDate": "2023-11-24T10:15:34Z",
                "Link": "http://arxiv.org/pdf/2311.14391v2",
                "arxiv_id": "2311.14391v2"
            },
            {
                "Title": "Lexical Features in Coreference Resolution: To be Used With Caution",
                "Authors": "Nafise Sadat Moosavi, Michael Strube",
                "Abstract": "  Lexical features are a major source of information in state-of-the-art\ncoreference resolvers. Lexical features implicitly model some of the linguistic\nphenomena at a fine granularity level. They are especially useful for\nrepresenting the context of mentions. In this paper we investigate a drawback\nof using many lexical features in state-of-the-art coreference resolvers. We\nshow that if coreference resolvers mainly rely on lexical features, they can\nhardly generalize to unseen domains. Furthermore, we show that the current\ncoreference resolution evaluation is clearly flawed by only evaluating on a\nspecific split of a specific dataset in which there is a notable overlap\nbetween the training, development and test sets.\n",
                "publicationDate": "2017-04-22T09:59:42Z",
                "Link": "http://arxiv.org/pdf/1704.06779v1",
                "arxiv_id": "1704.06779v1"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Gender Bias in Coreference Resolution",
                "Authors": "Rachel Rudinger, Jason Naradowsky, Brian Leonard, Benjamin Van Durme",
                "Abstract": "  We present an empirical study of gender bias in coreference resolution\nsystems. We first introduce a novel, Winograd schema-style set of minimal pair\nsentences that differ only by pronoun gender. With these \"Winogender schemas,\"\nwe evaluate and confirm systematic gender bias in three publicly-available\ncoreference resolution systems, and correlate this bias with real-world and\ntextual gender statistics.\n",
                "publicationDate": "2018-04-25T00:46:14Z",
                "Link": "http://arxiv.org/pdf/1804.09301v1",
                "arxiv_id": "1804.09301v1"
            },
            {
                "Title": "Investigating the Role of Centering Theory in the Context of Neural\n  Coreference Resolution Systems",
                "Authors": "Yuchen Eleanor Jiang, Ryan Cotterell, Mrinmaya Sachan",
                "Abstract": "  Centering theory (CT; Grosz et al., 1995) provides a linguistic analysis of\nthe structure of discourse. According to the theory, local coherence of\ndiscourse arises from the manner and extent to which successive utterances make\nreference to the same entities. In this paper, we investigate the connection\nbetween centering theory and modern coreference resolution systems. We provide\nan operationalization of centering and systematically investigate if neural\ncoreference resolvers adhere to the rules of centering theory by defining\nvarious discourse metrics and developing a search-based methodology. Our\ninformation-theoretic analysis reveals a positive dependence between\ncoreference and centering; but also shows that high-quality neural coreference\nresolvers may not benefit much from explicitly modeling centering ideas. Our\nanalysis further shows that contextualized embeddings contain much of the\ncoherence information, which helps explain why CT can only provide little gains\nto modern neural coreference resolvers which make use of pretrained\nrepresentations. Finally, we discuss factors that contribute to coreference\nwhich are not modeled by CT such as world knowledge and recency bias. We\nformulate a version of CT that also models recency and show that it captures\ncoreference information better compared to vanilla CT.\n",
                "publicationDate": "2022-10-26T12:55:26Z",
                "Link": "http://arxiv.org/pdf/2210.14678v1",
                "arxiv_id": "2210.14678v1"
            },
            {
                "Title": "KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels",
                "Authors": "Kyuhee Kim, Surin Lee, Sangah Lee",
                "Abstract": "  In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.\n",
                "publicationDate": "2024-04-01T14:36:35Z",
                "Link": "http://arxiv.org/pdf/2404.01140v2",
                "arxiv_id": "2404.01140v2"
            },
            {
                "Title": "Releasing the CRaQAn (Coreference Resolution in Question-Answering): An\n  open-source dataset and dataset creation methodology using\n  instruction-following models",
                "Authors": "Rob Grzywinski, Joshua D'Arcy, Rob Naidoff, Ashish Shukla, Alex Browne, Ren Gibbons, Brinnae Bent",
                "Abstract": "  Instruction-following language models demand robust methodologies for\ninformation retrieval to augment instructions for question-answering\napplications. A primary challenge is the resolution of coreferences in the\ncontext of chunking strategies for long documents. The critical barrier to\nexperimentation of handling coreferences is a lack of open source datasets,\nspecifically in question-answering tasks that require coreference resolution.\nIn this work we present our Coreference Resolution in Question-Answering\n(CRaQAn) dataset, an open-source dataset that caters to the nuanced information\nretrieval requirements of coreference resolution in question-answering tasks by\nproviding over 250 question-answer pairs containing coreferences. To develop\nthis dataset, we developed a novel approach for creating high-quality datasets\nusing an instruction-following model (GPT-4) and a Recursive Criticism and\nImprovement Loop.\n",
                "publicationDate": "2023-11-27T21:54:50Z",
                "Link": "http://arxiv.org/pdf/2311.16338v1",
                "arxiv_id": "2311.16338v1"
            },
            {
                "Title": "Graph-Based Decoding for Event Sequencing and Coreference Resolution",
                "Authors": "Zhengzhong Liu, Teruko Mitamura, Eduard Hovy",
                "Abstract": "  Events in text documents are interrelated in complex ways. In this paper, we\nstudy two types of relation: Event Coreference and Event Sequencing. We show\nthat the popular tree-like decoding structure for automated Event Coreference\nis not suitable for Event Sequencing. To this end, we propose a graph-based\ndecoding algorithm that is applicable to both tasks. The new decoding algorithm\nsupports flexible feature sets for both tasks. Empirically, our event\ncoreference system has achieved state-of-the-art performance on the TAC-KBP\n2015 event coreference task and our event sequencing system beats a strong\ntemporal-based, oracle-informed baseline. We discuss the challenges of studying\nthese event relations.\n",
                "publicationDate": "2018-06-13T15:05:39Z",
                "Link": "http://arxiv.org/pdf/1806.05099v1",
                "arxiv_id": "1806.05099v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "Word-Level Coreference Resolution",
                "Authors": "Vladimir Dobrovolskii",
                "Abstract": "  Recent coreference resolution models rely heavily on span representations to\nfind coreference links between word spans. As the number of spans is $O(n^2)$\nin the length of text and the number of potential links is $O(n^4)$, various\npruning techniques are necessary to make this approach computationally\nfeasible. We propose instead to consider coreference links between individual\nwords rather than word spans and then reconstruct the word spans. This reduces\nthe complexity of the coreference model to $O(n^2)$ and allows it to consider\nall potential mentions without pruning any of them out. We also demonstrate\nthat, with these changes, SpanBERT for coreference resolution will be\nsignificantly outperformed by RoBERTa. While being highly efficient, our model\nperforms competitively with recent coreference resolution systems on the\nOntoNotes benchmark.\n",
                "publicationDate": "2021-09-09T09:26:02Z",
                "Link": "http://arxiv.org/pdf/2109.04127v1",
                "arxiv_id": "2109.04127v1"
            },
            {
                "Title": "Solving Hard Coreference Problems",
                "Authors": "Haoruo Peng, Daniel Khashabi, Dan Roth",
                "Abstract": "  Coreference resolution is a key problem in natural language understanding\nthat still escapes reliable solutions. One fundamental difficulty has been that\nof resolving instances involving pronouns since they often require deep\nlanguage understanding and use of background knowledge. In this paper, we\npropose an algorithmic solution that involves a new representation for the\nknowledge required to address hard coreference problems, along with a\nconstrained optimization framework that uses this knowledge in coreference\ndecision making. Our representation, Predicate Schemas, is instantiated with\nknowledge acquired in an unsupervised way, and is compiled automatically into\nconstraints that impact the coreference decision. We present a general\ncoreference resolution system that significantly improves state-of-the-art\nperformance on hard, Winograd-style, pronoun resolution cases, while still\nperforming at the state-of-the-art level on standard coreference resolution\ndatasets.\n",
                "publicationDate": "2019-07-11T23:40:56Z",
                "Link": "http://arxiv.org/pdf/1907.05524v1",
                "arxiv_id": "1907.05524v1"
            },
            {
                "Title": "Cross-document Event Coreference Search: Task, Dataset and Modeling",
                "Authors": "Alon Eirew, Avi Caciularu, Ido Dagan",
                "Abstract": "  The task of Cross-document Coreference Resolution has been traditionally\nformulated as requiring to identify all coreference links across a given set of\ndocuments. We propose an appealing, and often more applicable, complementary\nset up for the task - Cross-document Coreference Search, focusing in this paper\non event coreference. Concretely, given a mention in context of an event of\ninterest, considered as a query, the task is to find all coreferring mentions\nfor the query event in a large document collection. To support research on this\ntask, we create a corresponding dataset, which is derived from Wikipedia while\nleveraging annotations in the available Wikipedia Event Coreference dataset\n(WEC-Eng). Observing that the coreference search setup is largely analogous to\nthe setting of Open Domain Question Answering, we adapt the prominent Deep\nPassage Retrieval (DPR) model to our setting, as an appealing baseline.\nFinally, we present a novel model that integrates a powerful coreference\nscoring scheme into the DPR architecture, yielding improved performance.\n",
                "publicationDate": "2022-10-23T08:21:25Z",
                "Link": "http://arxiv.org/pdf/2210.12654v1",
                "arxiv_id": "2210.12654v1"
            },
            {
                "Title": "Focus on what matters: Applying Discourse Coherence Theory to Cross\n  Document Coreference",
                "Authors": "William Held, Dan Iter, Dan Jurafsky",
                "Abstract": "  Performing event and entity coreference resolution across documents vastly\nincreases the number of candidate mentions, making it intractable to do the\nfull $n^2$ pairwise comparisons. Existing approaches simplify by considering\ncoreference only within document clusters, but this fails to handle\ninter-cluster coreference, common in many applications. As a result\ncross-document coreference algorithms are rarely applied to downstream tasks.\nWe draw on an insight from discourse coherence theory: potential coreferences\nare constrained by the reader's discourse focus. We model the entities/events\nin a reader's focus as a neighborhood within a learned latent embedding space\nwhich minimizes the distance between mentions and the centroids of their gold\ncoreference clusters. We then use these neighborhoods to sample only hard\nnegatives to train a fine-grained classifier on mention pairs and their local\ndiscourse features. Our approach achieves state-of-the-art results for both\nevents and entities on the ECB+, Gun Violence, Football Coreference, and\nCross-Domain Cross-Document Coreference corpora. Furthermore, training on\nmultiple corpora improves average performance across all datasets by 17.2 F1\npoints, leading to a robust coreference resolution model for use in downstream\ntasks where link distribution is unknown.\n",
                "publicationDate": "2021-10-11T15:41:47Z",
                "Link": "http://arxiv.org/pdf/2110.05362v1",
                "arxiv_id": "2110.05362v1"
            },
            {
                "Title": "Coreference Reasoning in Machine Reading Comprehension",
                "Authors": "Mingzhu Wu, Nafise Sadat Moosavi, Dan Roth, Iryna Gurevych",
                "Abstract": "  Coreference resolution is essential for natural language understanding and\nhas been long studied in NLP. In recent years, as the format of Question\nAnswering (QA) became a standard for machine reading comprehension (MRC), there\nhave been data collection efforts, e.g., Dasigi et al. (2019), that attempt to\nevaluate the ability of MRC models to reason about coreference. However, as we\nshow, coreference reasoning in MRC is a greater challenge than earlier thought;\nMRC datasets do not reflect the natural distribution and, consequently, the\nchallenges of coreference reasoning. Specifically, success on these datasets\ndoes not reflect a model's proficiency in coreference reasoning. We propose a\nmethodology for creating MRC datasets that better reflect the challenges of\ncoreference reasoning and use it to create a sample evaluation set. The results\non our dataset show that state-of-the-art models still struggle with these\nphenomena. Furthermore, we develop an effective way to use naturally occurring\ncoreference phenomena from existing coreference resolution datasets when\ntraining MRC models. This allows us to show an improvement in the coreference\nreasoning abilities of state-of-the-art models. The code and the resulting\ndataset are available at https://github.com/UKPLab/coref-reasoning-in-qa.\n",
                "publicationDate": "2020-12-31T12:18:41Z",
                "Link": "http://arxiv.org/pdf/2012.15573v2",
                "arxiv_id": "2012.15573v2"
            },
            {
                "Title": "Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference\n  Resolution",
                "Authors": "Altaf Rahman, Vincent Ng",
                "Abstract": "  Traditional learning-based coreference resolvers operate by training the\nmention-pair model for determining whether two mentions are coreferent or not.\nThough conceptually simple and easy to understand, the mention-pair model is\nlinguistically rather unappealing and lags far behind the heuristic-based\ncoreference models proposed in the pre-statistical NLP era in terms of\nsophistication. Two independent lines of recent research have attempted to\nimprove the mention-pair model, one by acquiring the mention-ranking model to\nrank preceding mentions for a given anaphor, and the other by training the\nentity-mention model to determine whether a preceding cluster is coreferent\nwith a given mention. We propose a cluster-ranking approach to coreference\nresolution, which combines the strengths of the mention-ranking model and the\nentity-mention model, and is therefore theoretically more appealing than both\nof these models. In addition, we seek to improve cluster rankers via two\nextensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity\nby jointly modeling anaphoricity determination and coreference resolution.\nExperimental results on the ACE data sets demonstrate the superior performance\nof cluster rankers to competing approaches as well as the effectiveness of our\ntwo extensions.\n",
                "publicationDate": "2014-01-16T05:06:09Z",
                "Link": "http://arxiv.org/pdf/1405.5202v1",
                "arxiv_id": "1405.5202v1"
            },
            {
                "Title": "Segmentation Approach for Coreference Resolution Task",
                "Authors": "Aref Jafari, Ali Ghodsi",
                "Abstract": "  In coreference resolution, it is important to consider all members of a\ncoreference cluster and decide about all of them at once. This technique can\nhelp to avoid losing precision and also in finding long-distance relations. The\npresented paper is a report of an ongoing study on an idea which proposes a new\napproach for coreference resolution which can resolve all coreference mentions\nto a given mention in the document in one pass. This has been accomplished by\ndefining an embedding method for the position of all members of a coreference\ncluster in a document and resolving all of them for a given mention. In the\nproposed method, the BERT model has been used for encoding the documents and a\nhead network designed to capture the relations between the embedded tokens.\nThese are then converted to the proposed span position embedding matrix which\nembeds the position of all coreference mentions in the document. We tested this\nidea on CoNLL 2012 dataset and although the preliminary results from this\nmethod do not quite meet the state-of-the-art results, they are promising and\nthey can capture features like long-distance relations better than the other\napproaches.\n",
                "publicationDate": "2020-06-30T16:44:28Z",
                "Link": "http://arxiv.org/pdf/2007.04301v1",
                "arxiv_id": "2007.04301v1"
            },
            {
                "Title": "Who are you referring to? Coreference resolution in image narrations",
                "Authors": "Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen",
                "Abstract": "  Coreference resolution aims to identify words and phrases which refer to same\nentity in a text, a core task in natural language processing. In this paper, we\nextend this task to resolving coreferences in long-form narrations of visual\nscenes. First we introduce a new dataset with annotated coreference chains and\ntheir bounding boxes, as most existing image-text datasets only contain short\nsentences without coreferring expressions or labeled chains. We propose a new\ntechnique that learns to identify coreference chains using weak supervision,\nonly from image-text pairs and a regularization using prior linguistic\nknowledge. Our model yields large performance gains over several strong\nbaselines in resolving coreferences. We also show that coreference resolution\nhelps improving grounding narratives in images.\n",
                "publicationDate": "2022-11-26T13:33:42Z",
                "Link": "http://arxiv.org/pdf/2211.14563v2",
                "arxiv_id": "2211.14563v2"
            },
            {
                "Title": "Using Linguistic Features to Improve the Generalization Capability of\n  Neural Coreference Resolvers",
                "Authors": "Nafise Sadat Moosavi, Michael Strube",
                "Abstract": "  Coreference resolution is an intermediate step for text understanding. It is\nused in tasks and domains for which we do not necessarily have coreference\nannotated corpora. Therefore, generalization is of special importance for\ncoreference resolution. However, while recent coreference resolvers have\nnotable improvements on the CoNLL dataset, they struggle to generalize properly\nto new domains or datasets. In this paper, we investigate the role of\nlinguistic features in building more generalizable coreference resolvers. We\nshow that generalization improves only slightly by merely using a set of\nadditional linguistic features. However, employing features and subsets of\ntheir values that are informative for coreference resolution, considerably\nimproves generalization. Thanks to better generalization, our system achieves\nstate-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our\nsystem, which is trained on CoNLL, achieves on-par performance with a system\ndesigned for this dataset.\n",
                "publicationDate": "2017-08-01T05:09:34Z",
                "Link": "http://arxiv.org/pdf/1708.00160v2",
                "arxiv_id": "1708.00160v2"
            },
            {
                "Title": "A Brief Survey and Comparative Study of Recent Development of Pronoun\n  Coreference Resolution",
                "Authors": "Hongming Zhang, Xinran Zhao, Yangqiu Song",
                "Abstract": "  Pronoun Coreference Resolution (PCR) is the task of resolving pronominal\nexpressions to all mentions they refer to. Compared with the general\ncoreference resolution task, the main challenge of PCR is the coreference\nrelation prediction rather than the mention detection. As one important natural\nlanguage understanding (NLU) component, pronoun resolution is crucial for many\ndownstream tasks and still challenging for existing models, which motivates us\nto survey existing approaches and think about how to do better. In this survey,\nwe first introduce representative datasets and models for the ordinary pronoun\ncoreference resolution task. Then we focus on recent progress on hard pronoun\ncoreference resolution problems (e.g., Winograd Schema Challenge) to analyze\nhow well current models can understand commonsense. We conduct extensive\nexperiments to show that even though current models are achieving good\nperformance on the standard evaluation set, they are still not ready to be used\nin real applications (e.g., all SOTA models struggle on correctly resolving\npronouns to infrequent objects). All experiment codes are available at\nhttps://github.com/HKUST-KnowComp/PCR.\n",
                "publicationDate": "2020-09-27T01:40:01Z",
                "Link": "http://arxiv.org/pdf/2009.12721v1",
                "arxiv_id": "2009.12721v1"
            },
            {
                "Title": "Multilingual Coreference Resolution in Low-resource South Asian\n  Languages",
                "Authors": "Ritwik Mishra, Pooja Desur, Rajiv Ratn Shah, Ponnurangam Kumaraguru",
                "Abstract": "  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n",
                "publicationDate": "2024-02-21T07:05:51Z",
                "Link": "http://arxiv.org/pdf/2402.13571v2",
                "arxiv_id": "2402.13571v2"
            },
            {
                "Title": "On Generalization in Coreference Resolution",
                "Authors": "Shubham Toshniwal, Patrick Xia, Sam Wiseman, Karen Livescu, Kevin Gimpel",
                "Abstract": "  While coreference resolution is defined independently of dataset domain, most\nmodels for performing coreference resolution do not transfer well to unseen\ndomains. We consolidate a set of 8 coreference resolution datasets targeting\ndifferent domains to evaluate the off-the-shelf performance of models. We then\nmix three datasets for training; even though their domain, annotation\nguidelines, and metadata differ, we propose a method for jointly training a\nsingle model on this heterogeneous data mixture by using data augmentation to\naccount for annotation differences and sampling to balance the data quantities.\nWe find that in a zero-shot setting, models trained on a single dataset\ntransfer poorly while joint training yields improved overall performance,\nleading to better generalization in coreference resolution models. This work\ncontributes a new benchmark for robust coreference resolution and multiple new\nstate-of-the-art results.\n",
                "publicationDate": "2021-09-20T16:33:22Z",
                "Link": "http://arxiv.org/pdf/2109.09667v1",
                "arxiv_id": "2109.09667v1"
            },
            {
                "Title": "Impact of Coreference Resolution on Slot Filling",
                "Authors": "Heike Adel, Hinrich Sch\u00fctze",
                "Abstract": "  In this paper, we demonstrate the importance of coreference resolution for\nnatural language processing on the example of the TAC Slot Filling shared task.\nWe illustrate the strengths and weaknesses of automatic coreference resolution\nsystems and provide experimental results to show that they improve performance\nin the slot filling end-to-end setting. Finally, we publish KBPchains, a\nresource containing automatically extracted coreference chains from the TAC\nsource corpus in order to support other researchers working on this topic.\n",
                "publicationDate": "2017-10-26T15:25:37Z",
                "Link": "http://arxiv.org/pdf/1710.09753v1",
                "arxiv_id": "1710.09753v1"
            },
            {
                "Title": "Graph Refinement for Coreference Resolution",
                "Authors": "Lesly Miculicich, James Henderson",
                "Abstract": "  The state-of-the-art models for coreference resolution are based on\nindependent mention pair-wise decisions. We propose a modelling approach that\nlearns coreference at the document-level and takes global decisions. For this\npurpose, we model coreference links in a graph structure where the nodes are\ntokens in the text, and the edges represent the relationship between them. Our\nmodel predicts the graph in a non-autoregressive manner, then iteratively\nrefines it based on previous predictions, allowing global dependencies between\ndecisions. The experimental results show improvements over various baselines,\nreinforcing the hypothesis that document-level information improves conference\nresolution.\n",
                "publicationDate": "2022-03-30T18:03:54Z",
                "Link": "http://arxiv.org/pdf/2203.16574v1",
                "arxiv_id": "2203.16574v1"
            },
            {
                "Title": "Coreference Resolution for the Biomedical Domain: A Survey",
                "Authors": "Pengcheng Lu, Massimo Poesio",
                "Abstract": "  Issues with coreference resolution are one of the most frequently mentioned\nchallenges for information extraction from the biomedical literature. Thus, the\nbiomedical genre has long been the second most researched genre for coreference\nresolution after the news domain, and the subject of a great deal of research\nfor NLP in general. In recent years this interest has grown enormously leading\nto the development of a number of substantial datasets, of domain-specific\ncontextual language models, and of several architectures. In this paper we\nreview the state-of-the-art of coreference in the biomedical domain with a\nparticular attention on these most recent developments.\n",
                "publicationDate": "2021-09-25T19:09:47Z",
                "Link": "http://arxiv.org/pdf/2109.12424v1",
                "arxiv_id": "2109.12424v1"
            },
            {
                "Title": "Revisiting Selectional Preferences for Coreference Resolution",
                "Authors": "Benjamin Heinzerling, Nafise Sadat Moosavi, Michael Strube",
                "Abstract": "  Selectional preferences have long been claimed to be essential for\ncoreference resolution. However, they are mainly modeled only implicitly by\ncurrent coreference resolvers. We propose a dependency-based embedding model of\nselectional preferences which allows fine-grained compatibility judgments with\nhigh coverage. We show that the incorporation of our model improves coreference\nresolution performance on the CoNLL dataset, matching the state-of-the-art\nresults of a more complex system. However, it comes with a cost that makes it\ndebatable how worthwhile such improvements are.\n",
                "publicationDate": "2017-07-20T11:54:37Z",
                "Link": "http://arxiv.org/pdf/1707.06456v1",
                "arxiv_id": "1707.06456v1"
            },
            {
                "Title": "Named Person Coreference in English News",
                "Authors": "Oshin Agarwal, Sanjay Subramanian, Ani Nenkova, Dan Roth",
                "Abstract": "  People are often entities of interest in tasks such as search and information\nextraction. In these tasks, the goal is to find as much information as possible\nabout people specified by their name. However in text, some of the references\nto people are by pronouns (she, his) or generic descriptions (the professor,\nthe German chancellor). It is therefore important that coreference resolution\nsystems are able to link these different types of mentions to the correct\nperson name. Here, we evaluate two state of the art coreference resolution\nsystems on the subtask of Named Person Coreference, in which we are interested\nin identifying a person mentioned by name, along with all other mentions of the\nperson, by pronoun or generic noun phrase. Our analysis reveals that standard\ncoreference metrics do not reflect adequately the requirements in this task:\nthey do not penalize systems for not identifying any mentions by name and they\nreward systems even if systems find correctly mentions to the same entity but\nfail to link these to a proper name (she--the student---no name). We introduce\nnew metrics for evaluating named person coreference that address these\ndiscrepancies. We present a simple rule-based named entity recognition driven\nsystem, which outperforms the current state-of-the-art systems on these\ntask-specific metrics and performs on par with them on traditional coreference\nevaluations. Finally, we present similar evaluation for coreference resolution\nof other named entities and show that the rule-based approach is effective only\nfor person named coreference, not other named entity types.\n",
                "publicationDate": "2018-10-26T18:10:03Z",
                "Link": "http://arxiv.org/pdf/1810.11476v2",
                "arxiv_id": "1810.11476v2"
            },
            {
                "Title": "CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
                "Authors": "Bo-Hsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Lin Li, Hong Yu",
                "Abstract": "  Anaphora and ellipses are two common phenomena in dialogues. Without\nresolving referring expressions and information omission, dialogue systems may\nfail to generate consistent and coherent responses. Traditionally, anaphora is\nresolved by coreference resolution and ellipses by query rewrite. In this work,\nwe propose a novel joint learning framework of modeling coreference resolution\nand query rewriting for complex, multi-turn dialogue understanding. Given an\nongoing dialogue between a user and a dialogue assistant, for the user query,\nour joint learning model first predicts coreference links between the query and\nthe dialogue context, and then generates a self-contained rewritten user query.\nTo evaluate our model, we annotate a dialogue based coreference resolution\ndataset, MuDoCo, with rewritten queries. Results show that the performance of\nquery rewrite can be substantially boosted (+2.3% F1) with the aid of\ncoreference modeling. Furthermore, our joint model outperforms the\nstate-of-the-art coreference resolution model (+2% F1) on this dataset.\n",
                "publicationDate": "2021-05-20T17:17:26Z",
                "Link": "http://arxiv.org/pdf/2105.09914v1",
                "arxiv_id": "2105.09914v1"
            },
            {
                "Title": "Towards Evaluation of Cross-document Coreference Resolution Models Using\n  Datasets with Diverse Annotation Schemes",
                "Authors": "Anastasia Zhukova, Felix Hamborg, Bela Gipp",
                "Abstract": "  Established cross-document coreference resolution (CDCR) datasets contain\nevent-centric coreference chains of events and entities with identity\nrelations. These datasets establish strict definitions of the coreference\nrelations across related tests but typically ignore anaphora with more vague\ncontext-dependent loose coreference relations. In this paper, we qualitatively\nand quantitatively compare the annotation schemes of ECB+, a CDCR dataset with\nidentity coreference relations, and NewsWCL50, a CDCR dataset with a mix of\nloose context-dependent and strict coreference relations. We propose a phrasing\ndiversity metric (PD) that encounters for the diversity of full phrases unlike\nthe previously proposed metrics and allows to evaluate lexical diversity of the\nCDCR datasets in a higher precision. The analysis shows that coreference chains\nof NewsWCL50 are more lexically diverse than those of ECB+ but annotating of\nNewsWCL50 leads to the lower inter-coder reliability. We discuss the different\ntasks that both CDCR datasets create for the CDCR models, i.e., lexical\ndisambiguation and lexical diversity. Finally, to ensure generalizability of\nthe CDCR models, we propose a direction for CDCR evaluation that combines CDCR\ndatasets with multiple annotation schemes that focus of various properties of\nthe coreference chains.\n",
                "publicationDate": "2021-09-11T10:33:17Z",
                "Link": "http://arxiv.org/pdf/2109.05250v2",
                "arxiv_id": "2109.05250v2"
            },
            {
                "Title": "LingMess: Linguistically Informed Multi Expert Scorers for Coreference\n  Resolution",
                "Authors": "Shon Otmazgin, Arie Cattan, Yoav Goldberg",
                "Abstract": "  While coreference resolution typically involves various linguistic\nchallenges, recent models are based on a single pairwise scorer for all types\nof pairs. We present LingMess, a new coreference model that defines different\ncategories of coreference cases and optimize multiple pairwise scorers, where\neach scorer learns a specific set of linguistic challenges. Our model\nsubstantially improves pairwise scores for most categories and outperforms\ncluster-level performance on Ontonotes and 5 additional datasets. Our model is\navailable in https://github.com/shon-otmazgin/lingmess-coref\n",
                "publicationDate": "2022-05-25T10:39:46Z",
                "Link": "http://arxiv.org/pdf/2205.12644v3",
                "arxiv_id": "2205.12644v3"
            },
            {
                "Title": "Bangla Text Recognition from Video Sequence: A New Focus",
                "Authors": "Souvik Bhowmick, Purnendu Banerjee",
                "Abstract": "  Extraction and recognition of Bangla text from video frame images is\nchallenging due to complex color background, low-resolution etc. In this paper,\nwe propose an algorithm for extraction and recognition of Bangla text form such\nvideo frames with complex background. Here, a two-step approach has been\nproposed. First, the text line is segmented into words using information based\non line contours. First order gradient value of the text blocks are used to\nfind the word gap. Next, a local binarization technique is applied on each word\nand text line is reconstructed using those words. Secondly, this binarized text\nblock is sent to OCR for recognition purpose.\n",
                "publicationDate": "2014-01-06T20:25:26Z",
                "Link": "http://arxiv.org/pdf/1401.1190v1",
                "arxiv_id": "1401.1190v1"
            },
            {
                "Title": "Seq2seq is All You Need for Coreference Resolution",
                "Authors": "Wenzheng Zhang, Sam Wiseman, Karl Stratos",
                "Abstract": "  Existing works on coreference resolution suggest that task-specific models\nare necessary to achieve state-of-the-art performance. In this work, we present\ncompelling evidence that such models are not necessary. We finetune a\npretrained seq2seq transformer to map an input document to a tagged sequence\nencoding the coreference annotation. Despite the extreme simplicity, our model\noutperforms or closely matches the best coreference systems in the literature\non an array of datasets. We also propose an especially simple seq2seq approach\nthat generates only tagged spans rather than the spans interleaved with the\noriginal text. Our analysis shows that the model size, the amount of\nsupervision, and the choice of sequence representations are key factors in\nperformance.\n",
                "publicationDate": "2023-10-20T19:17:22Z",
                "Link": "http://arxiv.org/pdf/2310.13774v1",
                "arxiv_id": "2310.13774v1"
            },
            {
                "Title": "XCoref: Cross-document Coreference Resolution in the Wild",
                "Authors": "Anastasia Zhukova, Felix Hamborg, Karsten Donnay, Bela Gipp",
                "Abstract": "  Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.\n",
                "publicationDate": "2021-09-11T10:41:09Z",
                "Link": "http://arxiv.org/pdf/2109.05252v1",
                "arxiv_id": "2109.05252v1"
            },
            {
                "Title": "Adapting Coreference Resolution for Processing Violent Death Narratives",
                "Authors": "Ankith Uppunda, Susan D. Cochran, Jacob G. Foster, Alina Arseniev-Koehler, Vickie M. Mays, Kai-Wei Chang",
                "Abstract": "  Coreference resolution is an important component in analyzing narrative text\nfrom administrative data (e.g., clinical or police sources). However, existing\ncoreference models trained on general language corpora suffer from poor\ntransferability due to domain gaps, especially when they are applied to\ngender-inclusive data with lesbian, gay, bisexual, and transgender (LGBT)\nindividuals. In this paper, we analyzed the challenges of coreference\nresolution in an exemplary form of administrative text written in English:\nviolent death narratives from the USA's Centers for Disease Control's (CDC)\nNational Violent Death Reporting System. We developed a set of data\naugmentation rules to improve model performance using a probabilistic data\nprogramming framework. Experiments on narratives from an administrative\ndatabase, as well as existing gender-inclusive coreference datasets,\ndemonstrate the effectiveness of data augmentation in training coreference\nmodels that can better handle text data about LGBT individuals.\n",
                "publicationDate": "2021-04-30T00:16:42Z",
                "Link": "http://arxiv.org/pdf/2104.14703v1",
                "arxiv_id": "2104.14703v1"
            },
            {
                "Title": "[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference\n  Annotation with LLMs",
                "Authors": "Rebecca M. M. Hicke, David Mimno",
                "Abstract": "  Coreference annotation and resolution is a vital component of computational\nliterary studies. However, it has previously been difficult to build high\nquality systems for fiction. Coreference requires complicated structured\noutputs, and literary text involves subtle inferences and highly varied\nlanguage. New language-model-based seq2seq systems present the opportunity to\nsolve both these problems by learning to directly generate a copy of an input\nsentence with markdown-like annotations. We create, evaluate, and release\nseveral trained models for coreference, as well as a workflow for training new\nmodels.\n",
                "publicationDate": "2024-01-31T15:35:21Z",
                "Link": "http://arxiv.org/pdf/2401.17922v1",
                "arxiv_id": "2401.17922v1"
            },
            {
                "Title": "Investigating Multilingual Coreference Resolution by Universal\n  Annotations",
                "Authors": "Haixia Chai, Michael Strube",
                "Abstract": "  Multilingual coreference resolution (MCR) has been a long-standing and\nchallenging task. With the newly proposed multilingual coreference dataset,\nCorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by\nusing its harmonized universal morphosyntactic and coreference annotations.\nFirst, we study coreference by examining the ground truth data at different\nlinguistic levels, namely mention, entity and document levels, and across\ndifferent genres, to gain insights into the characteristics of coreference\nacross multiple languages. Second, we perform an error analysis of the most\nchallenging cases that the SotA system fails to resolve in the CRAC 2022 shared\ntask using the universal annotations. Last, based on this analysis, we extract\nfeatures from universal morphosyntactic annotations and integrate these\nfeatures into a baseline system to assess their potential benefits for the MCR\ntask. Our results show that our best configuration of features improves the\nbaseline by 0.9% F1 score.\n",
                "publicationDate": "2023-10-26T18:50:04Z",
                "Link": "http://arxiv.org/pdf/2310.17734v1",
                "arxiv_id": "2310.17734v1"
            },
            {
                "Title": "Revisiting Joint Modeling of Cross-document Entity and Event Coreference\n  Resolution",
                "Authors": "Shany Barhom, Vered Shwartz, Alon Eirew, Michael Bugert, Nils Reimers, Ido Dagan",
                "Abstract": "  Recognizing coreferring events and entities across multiple texts is crucial\nfor many NLP applications. Despite the task's importance, research focus was\ngiven mostly to within-document entity coreference, with rather little\nattention to the other variants. We propose a neural architecture for\ncross-document coreference resolution. Inspired by Lee et al (2012), we jointly\nmodel entity and event coreference. We represent an event (entity) mention\nusing its lexical span, surrounding context, and relation to entity (event)\nmentions via predicate-arguments structures. Our model outperforms the previous\nstate-of-the-art event coreference model on ECB+, while providing the first\nentity coreference results on this corpus. Our analysis confirms that all our\nrepresentation elements, including the mention span itself, its context, and\nthe relation to other mentions contribute to the model's success.\n",
                "publicationDate": "2019-06-04T23:36:50Z",
                "Link": "http://arxiv.org/pdf/1906.01753v1",
                "arxiv_id": "1906.01753v1"
            },
            {
                "Title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset\n  from Wikipedia",
                "Authors": "Alon Eirew, Arie Cattan, Ido Dagan",
                "Abstract": "  Cross-document event coreference resolution is a foundational task for NLP\napplications involving multi-text processing. However, existing corpora for\nthis task are scarce and relatively small, while annotating only modest-size\nclusters of documents belonging to the same topic. To complement these\nresources and enhance future research, we present Wikipedia Event Coreference\n(WEC), an efficient methodology for gathering a large-scale dataset for\ncross-document event coreference from Wikipedia, where coreference links are\nnot restricted within predefined topics. We apply this methodology to the\nEnglish Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our\ndataset creation method is generic and can be applied with relatively little\neffort to other Wikipedia languages. To set baseline results, we develop an\nalgorithm that adapts components of state-of-the-art models for within-document\ncoreference resolution to the cross-document setting. Our model is suitably\nefficient and outperforms previously published state-of-the-art results for the\ntask.\n",
                "publicationDate": "2021-04-11T14:54:35Z",
                "Link": "http://arxiv.org/pdf/2104.05022v2",
                "arxiv_id": "2104.05022v2"
            }
        ]
    },
    {
        "topic_name": "Argument Mining",
        "summary": "default",
        "papers": [
            {
                "Title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining",
                "Authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n",
                "publicationDate": "2010-09-23T11:42:41Z",
                "Link": "http://arxiv.org/pdf/1009.4586v1",
                "arxiv_id": "1009.4586v1"
            },
            {
                "Title": "Optimal Bangla Keyboard Layout using Data Mining Technique",
                "Authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Md. Mahadi Hassan",
                "Abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n",
                "publicationDate": "2010-09-25T06:55:27Z",
                "Link": "http://arxiv.org/pdf/1009.4982v1",
                "arxiv_id": "1009.4982v1"
            },
            {
                "Title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique",
                "Authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, S. M. Kamruzzaman",
                "Abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n",
                "publicationDate": "2010-09-26T02:09:41Z",
                "Link": "http://arxiv.org/pdf/1009.5048v1",
                "arxiv_id": "1009.5048v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "AutoAM: An End-To-End Neural Model for Automatic and Universal Argument\n  Mining",
                "Authors": "Lang Cao",
                "Abstract": "  Argument mining is to analyze argument structure and extract important\nargument information from unstructured text. An argument mining system can help\npeople automatically gain causal and logical information behind the text. As\nargumentative corpus gradually increases, like more people begin to argue and\ndebate on social media, argument mining from them is becoming increasingly\ncritical. However, argument mining is still a big challenge in natural language\ntasks due to its difficulty, and relative techniques are not mature. For\nexample, research on non-tree argument mining needs to be done more. Most works\njust focus on extracting tree structure argument information. Moreover, current\nmethods cannot accurately describe and capture argument relations and do not\npredict their types. In this paper, we propose a novel neural model called\nAutoAM to solve these problems. We first introduce the argument component\nattention mechanism in our model. It can capture the relevant information\nbetween argument components, so our model can better perform argument mining.\nOur model is a universal end-to-end framework, which can analyze argument\nstructure without constraints like tree structure and complete three subtasks\nof argument mining in one model. The experiment results show that our model\noutperforms the existing works on several metrics in two public datasets.\n",
                "publicationDate": "2023-09-17T15:26:21Z",
                "Link": "http://arxiv.org/pdf/2309.09300v1",
                "arxiv_id": "2309.09300v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time",
                "Authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam",
                "Abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n",
                "publicationDate": "2017-01-27T06:30:21Z",
                "Link": "http://arxiv.org/pdf/1701.07955v1",
                "arxiv_id": "1701.07955v1"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Social Media Argumentation Mining: The Quest for Deliberateness in\n  Raucousness",
                "Authors": "Jan \u0160najder",
                "Abstract": "  Argumentation mining from social media content has attracted increasing\nattention. The task is both challenging and rewarding. The informal nature of\nuser-generated content makes the task dauntingly difficult. On the other hand,\nthe insights that could be gained by a large-scale analysis of social media\nargumentation make it a very worthwhile task. In this position paper I discuss\nthe motivation for social media argumentation mining, as well as the tasks and\nchallenges involved.\n",
                "publicationDate": "2016-12-31T21:37:10Z",
                "Link": "http://arxiv.org/pdf/1701.00168v1",
                "arxiv_id": "1701.00168v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio\n  Features for Argument Mining",
                "Authors": "Ramon Ruiz-Dolz, Javier Iranzo-S\u00e1nchez",
                "Abstract": "  In this paper, we describe VivesDebate-Speech, a corpus of spoken\nargumentation created to leverage audio features for argument mining tasks. The\ncreation of this corpus represents an important contribution to the\nintersection of speech processing and argument mining communities, and one of\nthe most complete publicly available resources in this topic. Moreover, we have\nperformed a set of first-of-their-kind experiments which show an improvement\nwhen integrating audio features into the argument mining pipeline. The provided\nresults can be used as a baseline for future research.\n",
                "publicationDate": "2023-02-24T11:44:24Z",
                "Link": "http://arxiv.org/pdf/2302.12584v2",
                "arxiv_id": "2302.12584v2"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n",
                "publicationDate": "2012-03-05T12:22:23Z",
                "Link": "http://arxiv.org/pdf/1203.0882v1",
                "arxiv_id": "1203.0882v1"
            },
            {
                "Title": "Recognizing Bangla Grammar using Predictive Parser",
                "Authors": "K. M. Azharul Hasan,  Al-Mahmud, Amit Mondal, Amit Saha",
                "Abstract": "  We describe a Context Free Grammar (CFG) for Bangla language and hence we\npropose a Bangla parser based on the grammar. Our approach is very much general\nto apply in Bangla Sentences and the method is well accepted for parsing a\nlanguage of a grammar. The proposed parser is a predictive parser and we\nconstruct the parse table for recognizing Bangla grammar. Using the parse table\nwe recognize syntactical mistakes of Bangla sentences when there is no entry\nfor a terminal in the parse table. If a natural language can be successfully\nparsed then grammar checking from this language becomes possible. The proposed\nscheme is based on Top down parsing method and we have avoided the left\nrecursion of the CFG using the idea of left factoring.\n",
                "publicationDate": "2012-01-10T10:33:18Z",
                "Link": "http://arxiv.org/pdf/1201.2010v1",
                "arxiv_id": "1201.2010v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals",
                "Authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu",
                "Abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n",
                "publicationDate": "2012-03-05T12:06:54Z",
                "Link": "http://arxiv.org/pdf/1203.0876v1",
                "arxiv_id": "1203.0876v1"
            },
            {
                "Title": "BSpell: A CNN-Blended BERT Based Bangla Spell Checker",
                "Authors": "Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali",
                "Abstract": "  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n",
                "publicationDate": "2022-08-20T15:21:35Z",
                "Link": "http://arxiv.org/pdf/2208.09709v2",
                "arxiv_id": "2208.09709v2"
            },
            {
                "Title": "Fine-Grained Image Generation from Bangla Text Description using\n  Attentional Generative Adversarial Network",
                "Authors": "Md Aminul Haque Palash, Md Abdullah Al Nasim, Aditi Dhali, Faria Afrin",
                "Abstract": "  Generating fine-grained, realistic images from text has many applications in\nthe visual and semantic realm. Considering that, we propose Bangla Attentional\nGenerative Adversarial Network (AttnGAN) that allows intensified, multi-stage\nprocessing for high-resolution Bangla text-to-image generation. Our model can\nintegrate the most specific details at different sub-regions of the image. We\ndistinctively concentrate on the relevant words in the natural language\ndescription. This framework has achieved a better inception score on the CUB\ndataset. For the first time, a fine-grained image is generated from Bangla text\nusing attentional GAN. Bangla has achieved 7th position among 100 most spoken\nlanguages. This inspires us to explicitly focus on this language, which will\nensure the inevitable need of many people. Moreover, Bangla has a more complex\nsyntactic structure and less natural language processing resource that\nvalidates our work more.\n",
                "publicationDate": "2021-09-24T05:31:01Z",
                "Link": "http://arxiv.org/pdf/2109.11749v1",
                "arxiv_id": "2109.11749v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "Multi-granularity Argument Mining in Legal Texts",
                "Authors": "Huihui Xu, Kevin Ashley",
                "Abstract": "  In this paper, we explore legal argument mining using multiple levels of\ngranularity. Argument mining has usually been conceptualized as a sentence\nclassification problem. In this work, we conceptualize argument mining as a\ntoken-level (i.e., word-level) classification problem. We use a Longformer\nmodel to classify the tokens. Results show that token-level text classification\nidentifies certain legal argument elements more accurately than sentence-level\ntext classification. Token-level classification also provides greater\nflexibility to analyze legal texts and to gain more insight into what the model\nfocuses on when processing a large amount of input data.\n",
                "publicationDate": "2022-10-17T23:28:22Z",
                "Link": "http://arxiv.org/pdf/2210.09472v2",
                "arxiv_id": "2210.09472v2"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "Argument Mining using BERT and Self-Attention based Embeddings",
                "Authors": "Pranjal Srivastava, Pranav Bhatnagar, Anurag Goel",
                "Abstract": "  Argument mining automatically identifies and extracts the structure of\ninference and reasoning conveyed in natural language arguments. To the best of\nour knowledge, most of the state-of-the-art works in this field have focused on\nusing tree-like structures and linguistic modeling. But, these approaches are\nnot able to model more complex structures which are often found in online\nforums and real world argumentation structures. In this paper, a novel\nmethodology for argument mining is proposed which employs attention-based\nembeddings for link prediction to model the causational hierarchies in typical\nargument structures prevalent in online discourse.\n",
                "publicationDate": "2023-02-27T15:52:31Z",
                "Link": "http://arxiv.org/pdf/2302.13906v1",
                "arxiv_id": "2302.13906v1"
            },
            {
                "Title": "Aspect-Based Argument Mining",
                "Authors": "Dietrich Trautmann",
                "Abstract": "  Computational Argumentation in general and Argument Mining in particular are\nimportant research fields. In previous works, many of the challenges to\nautomatically extract and to some degree reason over natural language arguments\nwere addressed. The tools to extract argument units are increasingly available\nand further open problems can be addressed. In this work, we are presenting the\ntask of Aspect-Based Argument Mining (ABAM), with the essential subtasks of\nAspect Term Extraction (ATE) and Nested Segmentation (NS). At the first\ninstance, we create and release an annotated corpus with aspect information on\nthe token-level. We consider aspects as the main point(s) argument units are\naddressing. This information is important for further downstream tasks such as\nargument ranking, argument summarization and generation, as well as the search\nfor counter-arguments on the aspect-level. We present several experiments using\nstate-of-the-art supervised architectures and demonstrate their performance for\nboth of the subtasks. The annotated benchmark is available at\nhttps://github.com/trtm/ABAM.\n",
                "publicationDate": "2020-11-01T21:57:51Z",
                "Link": "http://arxiv.org/pdf/2011.00633v1",
                "arxiv_id": "2011.00633v1"
            },
            {
                "Title": "Towards a Holistic View on Argument Quality Prediction",
                "Authors": "Michael Fromm, Max Berrendorf, Johanna Reiml, Isabelle Mayerhofer, Siddharth Bhargava, Evgeniy Faerman, Thomas Seidl",
                "Abstract": "  Argumentation is one of society's foundational pillars, and, sparked by\nadvances in NLP and the vast availability of text data, automated mining of\narguments receives increasing attention. A decisive property of arguments is\ntheir strength or quality. While there are works on the automated estimation of\nargument strength, their scope is narrow: they focus on isolated datasets and\nneglect the interactions with related argument mining tasks, such as argument\nidentification, evidence detection, or emotional appeal. In this work, we close\nthis gap by approaching argument quality estimation from multiple different\nangles: Grounded on rich results from thorough empirical evaluations, we assess\nthe generalization capabilities of argument quality estimation across diverse\ndomains, the interplay with related argument mining tasks, and the impact of\nemotions on perceived argument strength. We find that generalization depends on\na sufficient representation of different domains in the training part. In\nzero-shot transfer and multi-task experiments, we reveal that argument quality\nis among the more challenging tasks but can improve others. Finally, we show\nthat emotions play a minor role in argument quality than is often assumed.\n",
                "publicationDate": "2022-05-19T18:44:23Z",
                "Link": "http://arxiv.org/pdf/2205.09803v1",
                "arxiv_id": "2205.09803v1"
            },
            {
                "Title": "Multi-Task Learning Improves Performance In Deep Argument Mining Models",
                "Authors": "Amirhossein Farzam, Shashank Shekhar, Isaac Mehlhaff, Marco Morucci",
                "Abstract": "  The successful analysis of argumentative techniques from user-generated text\nis central to many downstream tasks such as political and market analysis.\nRecent argument mining tools use state-of-the-art deep learning methods to\nextract and annotate argumentative techniques from various online text corpora,\nhowever each task is treated as separate and different bespoke models are\nfine-tuned for each dataset. We show that different argument mining tasks share\ncommon semantic and logical structure by implementing a multi-task approach to\nargument mining that achieves better performance than state-of-the-art methods\nfor the same problems. Our model builds a shared representation of the input\ntext that is common to all tasks and exploits similarities between tasks in\norder to further boost performance via parameter-sharing. Our results are\nimportant for argument mining as they show that different tasks share\nsubstantial similarities and suggest a holistic approach to the extraction of\nargumentative techniques from text.\n",
                "publicationDate": "2023-07-03T23:42:29Z",
                "Link": "http://arxiv.org/pdf/2307.01401v1",
                "arxiv_id": "2307.01401v1"
            },
            {
                "Title": "BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection\n  on Bangla Clickbait Dataset",
                "Authors": "Md. Motahar Mahtab, Monirul Haque, Mehedi Hasan, Farig Sadeque",
                "Abstract": "  Intentionally luring readers to click on a particular content by exploiting\ntheir curiosity defines a title as clickbait. Although several studies focused\non detecting clickbait titles in English articles, low resource language like\nBangla has not been given adequate attention. To tackle clickbait titles in\nBangla, we have constructed the first Bangla clickbait detection dataset\ncontaining 15,056 labeled news articles and 65,406 unlabelled news articles\nextracted from clickbait dense news sites. Each article has been labeled by\nthree expert linguists and includes an article's title, body, and other\nmetadata. By incorporating labeled and unlabelled data, we finetune a\npretrained Bangla transformer model in an adversarial fashion using Semi\nSupervised Generative Adversarial Networks (SS GANs). The proposed model acts\nas a good baseline for this dataset, outperforming traditional neural network\nmodels (LSTM, GRU, CNN) and linguistic feature based models. We expect that\nthis dataset and the detailed analysis and comparison of these clickbait\ndetection models will provide a fundamental basis for future research into\ndetecting clickbait titles in Bengali articles. We have released the\ncorresponding code and dataset.\n",
                "publicationDate": "2023-11-10T17:38:46Z",
                "Link": "http://arxiv.org/pdf/2311.06204v1",
                "arxiv_id": "2311.06204v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Diversity Over Size: On the Effect of Sample and Topic Sizes for\n  Argument Mining Datasets",
                "Authors": "Benjamin Schiller, Johannes Daxenberger, Iryna Gurevych",
                "Abstract": "  The task of Argument Mining, that is extracting argumentative sentences for a\nspecific topic from large document sources, is an inherently difficult task for\nmachine learning models and humans alike, as large Argument Mining datasets are\nrare and recognition of argumentative sentences requires expert knowledge. The\ntask becomes even more difficult if it also involves stance detection of\nretrieved arguments. Given the cost and complexity of creating suitably large\nArgument Mining datasets, we ask whether it is necessary for acceptable\nperformance to have datasets growing in size. Our findings show that, when\nusing carefully composed training samples and a model pretrained on related\ntasks, we can reach 95% of the maximum performance while reducing the training\nsample size by at least 85%. This gain is consistent across three Argument\nMining tasks on three different datasets. We also publish a new dataset for\nfuture benchmarking.\n",
                "publicationDate": "2022-05-23T17:14:32Z",
                "Link": "http://arxiv.org/pdf/2205.11472v2",
                "arxiv_id": "2205.11472v2"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Recognition of Handwritten Bangla Basic Characters and Digits using\n  Convex Hull based Feature Set",
                "Authors": "Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  In dealing with the problem of recognition of handwritten character patterns\nof varying shapes and sizes, selection of a proper feature set is important to\nachieve high recognition performance. The current research aims to evaluate the\nperformance of the convex hull based feature set, i.e. 125 features in all\ncomputed over different bays attributes of the convex hull of a pattern, for\neffective recognition of isolated handwritten Bangla basic characters and\ndigits. On experimentation with a database of 10000 samples, the maximum\nrecognition rate of 76.86% is observed for handwritten Bangla characters. For\nBangla numerals the maximum success rate of 99.45%. is achieved on a database\nof 12000 sample. The current work validates the usefulness of a new kind of\nfeature set for recognition of handwritten Bangla basic characters and\nnumerals.\n",
                "publicationDate": "2014-10-02T08:26:38Z",
                "Link": "http://arxiv.org/pdf/1410.0478v1",
                "arxiv_id": "1410.0478v1"
            },
            {
                "Title": "Chittron: An Automatic Bangla Image Captioning System",
                "Authors": "Motiur Rahman, Nabeel Mohammed, Nafees Mansoor, Sifat Momen",
                "Abstract": "  Automatic image caption generation aims to produce an accurate description of\nan image in natural language automatically. However, Bangla, the fifth most\nwidely spoken language in the world, is lagging considerably in the research\nand development of such domain. Besides, while there are many established data\nsets to related to image annotation in English, no such resource exists for\nBangla yet. Hence, this paper outlines the development of \"Chittron\", an\nautomatic image captioning system in Bangla. Moreover, to address the data set\navailability issue, a collection of 16,000 Bangladeshi contextual images has\nbeen accumulated and manually annotated in Bangla. This data set is then used\nto train a model which integrates a pre-trained VGG16 image embedding model\nwith stacked LSTM layers. The model is trained to predict the caption when the\ninput is an image, one word at a time. The results show that the model has\nsuccessfully been able to learn a working language model and to generate\ncaptions of images quite accurately in many cases. The results are evaluated\nmainly qualitatively. However, BLEU scores are also reported. It is expected\nthat a better result can be obtained with a bigger and more varied data set.\n",
                "publicationDate": "2018-09-02T14:03:30Z",
                "Link": "http://arxiv.org/pdf/1809.00339v1",
                "arxiv_id": "1809.00339v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating\n  Low-Resource Natural Language Generation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Rifat Shahriyar",
                "Abstract": "  This work presents BanglaNLG, a comprehensive benchmark for evaluating\nnatural language generation (NLG) models in Bangla, a widely spoken yet\nlow-resource language. We aggregate six challenging conditional text generation\ntasks under the BanglaNLG benchmark, introducing a new dataset on dialogue\ngeneration in the process. Furthermore, using a clean corpus of 27.5 GB of\nBangla data, we pretrain BanglaT5, a sequence-to-sequence Transformer language\nmodel for Bangla. BanglaT5 achieves state-of-the-art performance in all of\nthese tasks, outperforming several multilingual models by up to 9% absolute\ngain and 32% relative gain. We are making the new dialogue dataset and the\nBanglaT5 model publicly available at https://github.com/csebuetnlp/BanglaNLG in\nthe hope of advancing future research on Bangla NLG.\n",
                "publicationDate": "2022-05-23T06:54:56Z",
                "Link": "http://arxiv.org/pdf/2205.11081v4",
                "arxiv_id": "2205.11081v4"
            },
            {
                "Title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models",
                "Authors": "Mohammed Rakib, Md. Ismail Hossain, Nabeel Mohammed, Fuad Rahman",
                "Abstract": "  Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.\n",
                "publicationDate": "2022-09-13T17:59:21Z",
                "Link": "http://arxiv.org/pdf/2209.12650v1",
                "arxiv_id": "2209.12650v1"
            },
            {
                "Title": "TACAM: Topic And Context Aware Argument Mining",
                "Authors": "Michael Fromm, Evgeniy Faerman, Thomas Seidl",
                "Abstract": "  In this work we address the problem of argument search. The purpose of\nargument search is the distillation of pro and contra arguments for requested\ntopics from large text corpora. In previous works, the usual approach is to use\na standard search engine to extract text parts which are relevant to the given\ntopic and subsequently use an argument recognition algorithm to select\narguments from them. The main challenge in the argument recognition task, which\nis also known as argument mining, is that often sentences containing arguments\nare structurally similar to purely informative sentences without any stance\nabout the topic. In fact, they only differ semantically. Most approaches use\ntopic or search term information only for the first search step and therefore\nassume that arguments can be classified independently of a topic. We argue that\ntopic information is crucial for argument mining, since the topic defines the\nsemantic context of an argument. Precisely, we propose different models for the\nclassification of arguments, which take information about a topic of an\nargument into account. Moreover, to enrich the context of a topic and to let\nmodels understand the context of the potential argument better, we integrate\ninformation from different external sources such as Knowledge Graphs or\npre-trained NLP models. Our evaluation shows that considering topic\ninformation, especially in connection with external information, provides a\nsignificant performance boost for the argument mining task.\n",
                "publicationDate": "2019-05-26T07:06:58Z",
                "Link": "http://arxiv.org/pdf/1906.00923v2",
                "arxiv_id": "1906.00923v2"
            },
            {
                "Title": "The evolution of argumentation mining: From models to social media and\n  emerging tools",
                "Authors": "Anastasios Lytos, Thomas Lagkas, Panagiotis Sarigiannidis, Kalina Bontcheva",
                "Abstract": "  Argumentation mining is a rising subject in the computational linguistics\ndomain focusing on extracting structured arguments from natural text, often\nfrom unstructured or noisy text. The initial approaches on modeling arguments\nwas aiming to identify a flawless argument on specific fields (Law, Scientific\nPapers) serving specific needs (completeness, effectiveness). With the emerge\nof Web 2.0 and the explosion in the use of social media both the diffusion of\nthe data and the argument structure have changed. In this survey article, we\nbridge the gap between theoretical approaches of argumentation mining and\npragmatic schemes that satisfy the needs of social media generated data,\nrecognizing the need for adapting more flexible and expandable schemes, capable\nto adjust to the argumentation conditions that exist in social media. We\nreview, compare, and classify existing approaches, techniques and tools,\nidentifying the positive outcome of combining tasks and features, and\neventually propose a conceptual architecture framework. The proposed\ntheoretical framework is an argumentation mining scheme able to identify the\ndistinct sub-tasks and capture the needs of social media text, revealing the\nneed for adopting more flexible and extensible frameworks.\n",
                "publicationDate": "2019-07-04T07:39:23Z",
                "Link": "http://arxiv.org/pdf/1907.02258v1",
                "arxiv_id": "1907.02258v1"
            },
            {
                "Title": "Multilingual Argument Mining: Datasets and Analysis",
                "Authors": "Orith Toledo-Ronen, Matan Orbach, Yonatan Bilu, Artem Spector, Noam Slonim",
                "Abstract": "  The growing interest in argument mining and computational argumentation\nbrings with it a plethora of Natural Language Understanding (NLU) tasks and\ncorresponding datasets. However, as with many other NLU tasks, the dominant\nlanguage is English, with resources in other languages being few and far\nbetween. In this work, we explore the potential of transfer learning using the\nmultilingual BERT model to address argument mining tasks in non-English\nlanguages, based on English datasets and the use of machine translation. We\nshow that such methods are well suited for classifying the stance of arguments\nand detecting evidence, but less so for assessing the quality of arguments,\npresumably because quality is harder to preserve under translation. In\naddition, focusing on the translate-train approach, we show how the choice of\nlanguages for translation, and the relations among them, affect the accuracy of\nthe resultant model. Finally, to facilitate evaluation of transfer learning on\nargument mining tasks, we provide a human-generated dataset with more than 10k\narguments in multiple languages, as well as machine translation of the English\ndatasets.\n",
                "publicationDate": "2020-10-13T14:49:10Z",
                "Link": "http://arxiv.org/pdf/2010.06432v1",
                "arxiv_id": "2010.06432v1"
            },
            {
                "Title": "Argumentation Mining in User-Generated Web Discourse",
                "Authors": "Ivan Habernal, Iryna Gurevych",
                "Abstract": "  The goal of argumentation mining, an evolving research field in computational\nlinguistics, is to design methods capable of analyzing people's argumentation.\nIn this article, we go beyond the state of the art in several ways. (i) We deal\nwith actual Web data and take up the challenges given by the variety of\nregisters, multiple domains, and unrestricted noisy user-generated Web\ndiscourse. (ii) We bridge the gap between normative argumentation theories and\nargumentation phenomena encountered in actual data by adapting an argumentation\nmodel tested in an extensive annotation study. (iii) We create a new gold\nstandard corpus (90k tokens in 340 documents) and experiment with several\nmachine learning methods to identify argument components. We offer the data,\nsource codes, and annotation guidelines to the community under free licenses.\nOur findings show that argumentation mining in user-generated Web discourse is\na feasible but challenging task.\n",
                "publicationDate": "2016-01-11T11:28:49Z",
                "Link": "http://arxiv.org/pdf/1601.02403v5",
                "arxiv_id": "1601.02403v5"
            }
        ]
    },
    {
        "topic_name": "Machine Translation",
        "summary": "default",
        "papers": [
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "English to Bangla Machine Translation Using Recurrent Neural Network",
                "Authors": "Shaykh Siddique, Tahmid Ahmed, Md. Rifayet Azam Talukder, Md. Mohsin Uddin",
                "Abstract": "  The applications of recurrent neural networks in machine translation are\nincreasing in natural language processing. Besides other languages, Bangla\nlanguage contains a large amount of vocabulary. Improvement of English to\nBangla machine translation would be a significant contribution to Bangla\nLanguage processing. This paper describes an architecture of English to Bangla\nmachine translation system. The system has been implemented with the\nencoder-decoder recurrent neural network. The model uses a knowledge-based\ncontext vector for the mapping of English and Bangla words. Performances of the\nmodel based on activation functions are measured here. The best performance is\nachieved for the linear activation function in encoder layer and the tanh\nactivation function in decoder layer. From the execution of GRU and LSTM layer,\nGRU performed better than LSTM. The attention layers are enacted with softmax\nand sigmoid activation function. The approach of the model outperforms the\nprevious state-of-the-art systems in terms of cross-entropy loss metrics. The\nreader can easily find out the structure of the machine translation of English\nto Bangla and the efficient activation functions from the paper.\n",
                "publicationDate": "2021-06-14T08:26:50Z",
                "Link": "http://arxiv.org/pdf/2106.07225v1",
                "arxiv_id": "2106.07225v1"
            },
            {
                "Title": "BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset",
                "Authors": "Mohammad Faiyaz Khan, S. M. Sadiq-Ur-Rahman Shifath, Md Saiful Islam",
                "Abstract": "  As computers have become efficient at understanding visual information and\ntransforming it into a written representation, research interest in tasks like\nautomatic image captioning has seen a significant leap over the last few years.\nWhile most of the research attention is given to the English language in a\nmonolingual setting, resource-constrained languages like Bangla remain out of\nfocus, predominantly due to a lack of standard datasets. Addressing this issue,\nwe present a new dataset BAN-Cap following the widely used Flickr8k dataset,\nwhere we collect Bangla captions of the images provided by qualified\nannotators. Our dataset represents a wider variety of image caption styles\nannotated by trained people from different backgrounds. We present a\nquantitative and qualitative analysis of the dataset and the baseline\nevaluation of the recent models in Bangla image captioning. We investigate the\neffect of text augmentation and demonstrate that an adaptive attention-based\nmodel combined with text augmentation using Contextualized Word Replacement\n(CWR) outperforms all state-of-the-art models for Bangla image captioning. We\nalso present this dataset's multipurpose nature, especially on machine\ntranslation for Bangla-English and English-Bangla. This dataset and all the\nmodels will be useful for further research.\n",
                "publicationDate": "2022-05-28T15:39:09Z",
                "Link": "http://arxiv.org/pdf/2205.14462v1",
                "arxiv_id": "2205.14462v1"
            },
            {
                "Title": "Handwritten Bangla Digit Recognition Using Deep Learning",
                "Authors": "Md Zahangir Alom, Paheding Sidike, Tarek M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of the advances in pattern recognition technology, Handwritten\nBangla Character Recognition (HBCR) (such as alpha-numeric and special\ncharacters) remains largely unsolved due to the presence of many perplexing\ncharacters and excessive cursive in Bangla handwriting. Even the best existing\nrecognizers do not lead to satisfactory performance for practical applications.\nTo improve the performance of Handwritten Bangla Digit Recognition (HBDR), we\nherein present a new approach based on deep neural networks which have recently\nshown excellent performance in many pattern recognition and machine learning\napplications, but has not been throughly attempted for HBDR. We introduce\nBangla digit recognition techniques based on Deep Belief Network (DBN),\nConvolutional Neural Networks (CNN), CNN with dropout, CNN with dropout and\nGaussian filters, and CNN with dropout and Gabor filters. These networks have\nthe advantage of extracting and using feature information, improving the\nrecognition of two dimensional shapes with a high degree of invariance to\ntranslation, scaling and other pattern distortions. We systematically evaluated\nthe performance of our method on publicly available Bangla numeral image\ndatabase named CMATERdb 3.1.1. From experiments, we achieved 98.78% recognition\nrate using the proposed method: CNN with Gabor features and dropout, which\noutperforms the state-of-the-art algorithms for HDBR.\n",
                "publicationDate": "2017-05-07T18:49:27Z",
                "Link": "http://arxiv.org/pdf/1705.02680v1",
                "arxiv_id": "1705.02680v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language\n  Models for Ethnic Media",
                "Authors": "MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee",
                "Abstract": "  Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.\n",
                "publicationDate": "2024-02-21T23:43:04Z",
                "Link": "http://arxiv.org/pdf/2402.14179v1",
                "arxiv_id": "2402.14179v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
                "Authors": "H. A. Z. Sameen Shahgir, Khondker Salman Sayeed",
                "Abstract": "  This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.\n",
                "publicationDate": "2023-03-19T09:24:48Z",
                "Link": "http://arxiv.org/pdf/2303.10612v1",
                "arxiv_id": "2303.10612v1"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "Supervised learning Methods for Bangla Web Document Categorization",
                "Authors": "Ashis Kumar Mandal, Rikta Sen",
                "Abstract": "  This paper explores the use of machine learning approaches, or more\nspecifically, four supervised learning Methods, namely Decision Tree(C 4.5),\nK-Nearest Neighbour (KNN), Na\\\"ive Bays (NB), and Support Vector Machine (SVM)\nfor categorization of Bangla web documents. This is a task of automatically\nsorting a set of documents into categories from a predefined set. Whereas a\nwide range of methods have been applied to English text categorization,\nrelatively few studies have been conducted on Bangla language text\ncategorization. Hence, we attempt to analyze the efficiency of those four\nmethods for categorization of Bangla documents. In order to validate, Bangla\ncorpus from various websites has been developed and used as examples for the\nexperiment. For Bangla, empirical results support that all four methods produce\nsatisfactory performance with SVM attaining good result in terms of high\ndimensional and relatively noisy document feature vectors.\n",
                "publicationDate": "2014-10-08T10:01:47Z",
                "Link": "http://arxiv.org/pdf/1410.2045v1",
                "arxiv_id": "1410.2045v1"
            },
            {
                "Title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network",
                "Authors": "Md Aminul Haque Palash, Akib Khan, Kawsarul Islam, MD Abdullah Al Nasim, Ryan Mohammad Bin Shahjahan",
                "Abstract": "  Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.\n",
                "publicationDate": "2022-10-26T20:57:45Z",
                "Link": "http://arxiv.org/pdf/2211.07709v1",
                "arxiv_id": "2211.07709v1"
            },
            {
                "Title": "Byakto Speech: Real-time long speech synthesis with convolutional neural\n  network: Transfer learning from English to Bangla",
                "Authors": "Zabir Al Nazi, Sayed Mohammed Tasmimul Huda",
                "Abstract": "  Speech synthesis is one of the challenging tasks to automate by deep\nlearning, also being a low-resource language there are very few attempts at\nBangla speech synthesis. Most of the existing works can't work with anything\nother than simple Bangla characters script, very short sentences, etc. This\nwork attempts to solve these problems by introducing Byakta, the first-ever\nopen-source deep learning-based bilingual (Bangla and English) text to a speech\nsynthesis system. A speech recognition model-based automated scoring metric was\nalso proposed to evaluate the performance of a TTS model. We also introduce a\ntest benchmark dataset for Bangla speech synthesis models for evaluating speech\nquality. The TTS is available at https://github.com/zabir-nabil/bangla-tts\n",
                "publicationDate": "2021-05-31T20:39:35Z",
                "Link": "http://arxiv.org/pdf/2106.03937v1",
                "arxiv_id": "2106.03937v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with\n  Semantic Neural Graph Filtering",
                "Authors": "Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Dong-Kyu Chae",
                "Abstract": "  Knowledge Graphs (KGs) have proven essential in information processing and\nreasoning applications because they link related entities and give context-rich\ninformation, supporting efficient information retrieval and knowledge\ndiscovery; presenting information flow in a very effective manner. Despite\nbeing widely used globally, Bangla is relatively underrepresented in KGs due to\na lack of comprehensive datasets, encoders, NER (named entity recognition)\nmodels, POS (part-of-speech) taggers, and lemmatizers, hindering efficient\ninformation processing and reasoning applications in the language. Addressing\nthe KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework\nthat is able to automatically construct Bengali KGs from any Bangla text. We\nutilize multilingual LLMs to understand various languages and correlate\nentities and relations universally. By employing a translation dictionary to\nidentify English equivalents and extracting word features from pre-trained BERT\nmodels, we construct the foundational KG. To reduce noise and align word\nembeddings with our goal, we employ graph-based polynomial filters. Lastly, we\nimplement a GNN-based semantic filter, which elevates contextual understanding\nand trims unnecessary edges, culminating in the formation of the definitive KG.\nEmpirical findings and case studies demonstrate the universal effectiveness of\nour model, capable of autonomously constructing semantically enriched KGs from\nany text.\n",
                "publicationDate": "2024-04-04T15:31:21Z",
                "Link": "http://arxiv.org/pdf/2404.03528v2",
                "arxiv_id": "2404.03528v2"
            },
            {
                "Title": "Handwritten Bangla Basic and Compound character recognition using MLP\n  and SVM classifier",
                "Authors": "Nibaran Das, Bindaban Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri",
                "Abstract": "  A novel approach for recognition of handwritten compound Bangla characters,\nalong with the Basic characters of Bangla alphabet, is presented here. Compared\nto English like Roman script, one of the major stumbling blocks in Optical\nCharacter Recognition (OCR) of handwritten Bangla script is the large number of\ncomplex shaped character classes of Bangla alphabet. In addition to 50 basic\ncharacter classes, there are nearly 160 complex shaped compound character\nclasses in Bangla alphabet. Dealing with such a large varieties of handwritten\ncharacters with a suitably designed feature set is a challenging problem.\nUncertainty and imprecision are inherent in handwritten script. Moreover, such\na large varieties of complex shaped characters, some of which have close\nresemblance, makes the problem of OCR of handwritten Bangla characters more\ndifficult. Considering the complexity of the problem, the present approach\nmakes an attempt to identify compound character classes from most frequently to\nless frequently occurred ones, i.e., in order of importance. This is to develop\na frame work for incrementally increasing the number of learned classes of\ncompound characters from more frequently occurred ones to less frequently\noccurred ones along with Basic characters. On experimentation, the technique is\nobserved produce an average recognition rate of 79.25 after three fold cross\nvalidation of data with future scope of improvement and extension.\n",
                "publicationDate": "2010-02-22T02:58:49Z",
                "Link": "http://arxiv.org/pdf/1002.4040v2",
                "arxiv_id": "1002.4040v2"
            },
            {
                "Title": "A Convolutional Neural Network Based Approach to Recognize Bangla Spoken\n  Digits from Speech Signal",
                "Authors": "Ovishake Sen,  Al-Mahmud, Pias Roy",
                "Abstract": "  Speech recognition is a technique that converts human speech signals into\ntext or words or in any form that can be easily understood by computers or\nother machines. There have been a few studies on Bangla digit recognition\nsystems, the majority of which used small datasets with few variations in\ngenders, ages, dialects, and other variables. Audio recordings of Bangladeshi\npeople of various genders, ages, and dialects were used to create a large\nspeech dataset of spoken '0-9' Bangla digits in this study. Here, 400 noisy and\nnoise-free samples per digit have been recorded for creating the dataset. Mel\nFrequency Cepstrum Coefficients (MFCCs) have been utilized for extracting\nmeaningful features from the raw speech data. Then, to detect Bangla numeral\ndigits, Convolutional Neural Networks (CNNs) were utilized. The suggested\ntechnique recognizes '0-9' Bangla spoken digits with 97.1% accuracy throughout\nthe whole dataset. The efficiency of the model was also assessed using 10-fold\ncrossvalidation, which yielded a 96.7% accuracy.\n",
                "publicationDate": "2021-11-12T09:38:15Z",
                "Link": "http://arxiv.org/pdf/2111.06625v1",
                "arxiv_id": "2111.06625v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Intent Detection and Slot Filling for Home Assistants: Dataset and\n  Analysis for Bangla and Sylheti",
                "Authors": "Fardin Ahsan Sakib, A H M Rezaul Karim, Saadat Hasan Khan, Md Mushfiqur Rahman",
                "Abstract": "  As voice assistants cement their place in our technologically advanced\nsociety, there remains a need to cater to the diverse linguistic landscape,\nincluding colloquial forms of low-resource languages. Our study introduces the\nfirst-ever comprehensive dataset for intent detection and slot filling in\nformal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples\nacross 10 unique intents. Our analysis reveals the robustness of large language\nmodels for tackling downstream tasks with inadequate data. The GPT-3.5 model\nachieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot\nfilling for colloquial Bangla.\n",
                "publicationDate": "2023-10-17T02:12:12Z",
                "Link": "http://arxiv.org/pdf/2310.10935v1",
                "arxiv_id": "2310.10935v1"
            },
            {
                "Title": "Bangla Handwritten Digit Recognition and Generation",
                "Authors": "Md Fahim Sikder",
                "Abstract": "  Handwritten digit or numeral recognition is one of the classical issues in\nthe area of pattern recognition and has seen tremendous advancement because of\nthe recent wide availability of computing resources. Plentiful works have\nalready done on English, Arabic, Chinese, Japanese handwritten script. Some\nwork on Bangla also have been done but there is space for development. From\nthat angle, in this paper, an architecture has been implemented which achieved\nthe validation accuracy of 99.44% on BHAND dataset and outperforms Alexnet and\nInception V3 architecture. Beside digit recognition, digit generation is\nanother field which has recently caught the attention of the researchers though\nnot many works have been done in this field especially on Bangla. In this\npaper, a Semi-Supervised Generative Adversarial Network or SGAN has been\napplied to generate Bangla handwritten numerals and it successfully generated\nBangla digits.\n",
                "publicationDate": "2021-03-14T12:11:21Z",
                "Link": "http://arxiv.org/pdf/2103.07905v1",
                "arxiv_id": "2103.07905v1"
            },
            {
                "Title": "Deep Learning Approach for Classifying the Aggressive Comments on Social\n  Media: Machine Translated Data Vs Real Life Data",
                "Authors": "Mst Shapna Akter, Hossain Shahriar, Nova Ahmed, Alfredo Cuzzocrea",
                "Abstract": "  Aggressive comments on social media negatively impact human life. Such\noffensive contents are responsible for depression and suicidal-related\nactivities. Since online social networking is increasing day by day, the hate\ncontent is also increasing. Several investigations have been done on the domain\nof cyberbullying, cyberaggression, hate speech, etc. The majority of the\ninquiry has been done in the English language. Some languages (Hindi and\nBangla) still lack proper investigations due to the lack of a dataset. This\npaper particularly worked on the Hindi, Bangla, and English datasets to detect\naggressive comments and have shown a novel way of generating machine-translated\ndata to resolve data unavailability issues. A fully machine-translated English\ndataset has been analyzed with the models such as the Long Short term memory\nmodel (LSTM), Bidirectional Long-short term memory model (BiLSTM),\nLSTM-Autoencoder, word2vec, Bidirectional Encoder Representations from\nTransformers (BERT), and generative pre-trained transformer (GPT-2) to make an\nobservation on how the models perform on a machine-translated noisy dataset. We\nhave compared the performance of using the noisy data with two more datasets\nsuch as raw data, which does not contain any noises, and semi-noisy data, which\ncontains a certain amount of noisy data. We have classified both the raw and\nsemi-noisy data using the aforementioned models. To evaluate the performance of\nthe models, we have used evaluation metrics such as F1-score,accuracy,\nprecision, and recall. We have achieved the highest accuracy on raw data using\nthe gpt2 model, semi-noisy data using the BERT model, and fully\nmachine-translated data using the BERT model. Since many languages do not have\nproper data availability, our approach will help researchers create\nmachine-translated datasets for several analysis purposes.\n",
                "publicationDate": "2023-03-13T21:43:08Z",
                "Link": "http://arxiv.org/pdf/2303.07484v1",
                "arxiv_id": "2303.07484v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters",
                "Authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed Akhter Hossain",
                "Abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n",
                "publicationDate": "2020-04-29T06:38:12Z",
                "Link": "http://arxiv.org/pdf/2005.02155v2",
                "arxiv_id": "2005.02155v2"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Detection of Bangla Fake News using MNB and SVM Classifier",
                "Authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, Sakib Al Hasan",
                "Abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n",
                "publicationDate": "2020-05-29T15:38:54Z",
                "Link": "http://arxiv.org/pdf/2005.14627v1",
                "arxiv_id": "2005.14627v1"
            },
            {
                "Title": "Handwritten Bangla Character Recognition Using The State-of-Art Deep\n  Convolutional Neural Networks",
                "Authors": "Md Zahangir Alom, Peheding Sidike, Mahmudul Hasan, Tark M. Taha, Vijayan K. Asari",
                "Abstract": "  In spite of advances in object recognition technology, Handwritten Bangla\nCharacter Recognition (HBCR) remains largely unsolved due to the presence of\nmany ambiguous handwritten characters and excessively cursive Bangla\nhandwritings. Even the best existing recognizers do not lead to satisfactory\nperformance for practical applications related to Bangla character recognition\nand have much lower performance than those developed for English alpha-numeric\ncharacters. To improve the performance of HBCR, we herein present the\napplication of the state-of-the-art Deep Convolutional Neural Networks (DCNN)\nincluding VGG Network, All Convolution Network (All-Conv Net), Network in\nNetwork (NiN), Residual Network, FractalNet, and DenseNet for HBCR. The deep\nlearning approaches have the advantage of extracting and using feature\ninformation, improving the recognition of 2D shapes with a high degree of\ninvariance to translation, scaling and other distortions. We systematically\nevaluated the performance of DCNN models on publicly available Bangla\nhandwritten character dataset called CMATERdb and achieved the superior\nrecognition accuracy when using DCNN models. This improvement would help in\nbuilding an automatic HBCR system for practical applications.\n",
                "publicationDate": "2017-12-28T14:31:56Z",
                "Link": "http://arxiv.org/pdf/1712.09872v3",
                "arxiv_id": "1712.09872v3"
            },
            {
                "Title": "BanglaCoNER: Towards Robust Bangla Complex Named Entity Recognition",
                "Authors": "HAZ Sameen Shahgir, Ramisa Alam, Md. Zarif Ul Alam",
                "Abstract": "  Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing that involves identifying and classifying named entities in text.\nBut much work hasn't been done for complex named entity recognition in Bangla,\ndespite being the seventh most spoken language globally. CNER is a more\nchallenging task than traditional NER as it involves identifying and\nclassifying complex and compound entities, which are not common in Bangla\nlanguage. In this paper, we present the winning solution of Bangla Complex\nNamed Entity Recognition Challenge - addressing the CNER task on BanglaCoNER\ndataset using two different approaches, namely Conditional Random Fields (CRF)\nand finetuning transformer based Deep Learning models such as BanglaBERT.\n  The dataset consisted of 15300 sentences for training and 800 sentences for\nvalidation, in the .conll format. Exploratory Data Analysis (EDA) on the\ndataset revealed that the dataset had 7 different NER tags, with notable\npresence of English words, suggesting that the dataset is synthetic and likely\na product of translation.\n  We experimented with a variety of feature combinations including Part of\nSpeech (POS) tags, word suffixes, Gazetteers, and cluster information from\nembeddings, while also finetuning the BanglaBERT (large) model for NER. We\nfound that not all linguistic patterns are immediately apparent or even\nintuitive to humans, which is why Deep Learning based models has proved to be\nthe more effective model in NLP, including CNER task. Our fine tuned BanglaBERT\n(large) model achieves an F1 Score of 0.79 on the validation set. Overall, our\nstudy highlights the importance of Bangla Complex Named Entity Recognition,\nparticularly in the context of synthetic datasets. Our findings also\ndemonstrate the efficacy of Deep Learning models such as BanglaBERT for NER in\nBangla language.\n",
                "publicationDate": "2023-03-16T13:31:31Z",
                "Link": "http://arxiv.org/pdf/2303.09306v2",
                "arxiv_id": "2303.09306v2"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "A Trustable LSTM-Autoencoder Network for Cyberbullying Detection on\n  Social Media Using Synthetic Data",
                "Authors": "Mst Shapna Akter, Hossain Shahriar, Alfredo Cuzzocrea",
                "Abstract": "  Social media cyberbullying has a detrimental effect on human life. As online\nsocial networking grows daily, the amount of hate speech also increases. Such\nterrible content can cause depression and actions related to suicide. This\npaper proposes a trustable LSTM-Autoencoder Network for cyberbullying detection\non social media using synthetic data. We have demonstrated a cutting-edge\nmethod to address data availability difficulties by producing\nmachine-translated data. However, several languages such as Hindi and Bangla\nstill lack adequate investigations due to a lack of datasets. We carried out\nexperimental identification of aggressive comments on Hindi, Bangla, and\nEnglish datasets using the proposed model and traditional models, including\nLong Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM),\nLSTM-Autoencoder, Word2vec, Bidirectional Encoder Representations from\nTransformers (BERT), and Generative Pre-trained Transformer 2 (GPT-2) models.\nWe employed evaluation metrics such as f1-score, accuracy, precision, and\nrecall to assess the models performance. Our proposed model outperformed all\nthe models on all datasets, achieving the highest accuracy of 95%. Our model\nachieves state-of-the-art results among all the previous works on the dataset\nwe used in this paper.\n",
                "publicationDate": "2023-08-15T17:20:05Z",
                "Link": "http://arxiv.org/pdf/2308.09722v1",
                "arxiv_id": "2308.09722v1"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in\n  Bangla with Multi-Feature and Multi-Modal Analysis",
                "Authors": "Abdullah Al Imran, Md Sakib Hossain Shovon, M. F. Mridha",
                "Abstract": "  This study presents a large multi-modal Bangla YouTube clickbait dataset\nconsisting of 253,070 data points collected through an automated process using\nthe YouTube API and Python web automation frameworks. The dataset contains 18\ndiverse features categorized into metadata, primary content, engagement\nstatistics, and labels for individual videos from 58 Bangla YouTube channels. A\nrigorous preprocessing step has been applied to denoise, deduplicate, and\nremove bias from the features, ensuring unbiased and reliable analysis. As the\nlargest and most robust clickbait corpus in Bangla to date, this dataset\nprovides significant value for natural language processing and data science\nresearchers seeking to advance modeling of clickbait phenomena in low-resource\nlanguages. Its multi-modal nature allows for comprehensive analyses of\nclickbait across content, user interactions, and linguistic dimensions to\ndevelop more sophisticated detection methods with cross-linguistic\napplications.\n",
                "publicationDate": "2023-10-13T13:25:16Z",
                "Link": "http://arxiv.org/pdf/2310.11465v1",
                "arxiv_id": "2310.11465v1"
            },
            {
                "Title": "UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser\n  Approach",
                "Authors": "Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear",
                "Abstract": "  Universal Networking Language (UNL) is a declarative formal language that is\nused to represent semantic data extracted from natural language texts. This\npaper presents a novel approach to converting Bangla natural language text into\nUNL using a method known as Predicate Preserving Parser (PPP) technique. PPP\nperforms morphological, syntactic and semantic, and lexical analysis of text\nsynchronously. This analysis produces a semantic-net like structure represented\nusing UNL. We demonstrate how Bangla texts are analyzed following the PPP\ntechnique to produce UNL documents which can then be translated into any other\nsuitable natural language facilitating the opportunity to develop a universal\nlanguage translation method via UNL.\n",
                "publicationDate": "2012-06-02T13:23:18Z",
                "Link": "http://arxiv.org/pdf/1206.0381v1",
                "arxiv_id": "1206.0381v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "Robust and Consistent Estimation of Word Embedding for Bangla Language\n  by fine-tuning Word2Vec Model",
                "Authors": "Rifat Rahman",
                "Abstract": "  Word embedding or vector representation of word holds syntactical and\nsemantic characteristics of a word which can be an informative feature for any\nmachine learning-based models of natural language processing. There are several\ndeep learning-based models for the vectorization of words like word2vec,\nfasttext, gensim, glove, etc. In this study, we analyze word2vec model for\nlearning word vectors by tuning different hyper-parameters and present the most\neffective word embedding for Bangla language. For testing the performances of\ndifferent word embeddings generated by fine-tuning of word2vec model, we\nperform both intrinsic and extrinsic evaluations. We cluster the word vectors\nto examine the relational similarity of words for intrinsic evaluation and also\nuse different word embeddings as the feature of news article classifier for\nextrinsic evaluation. From our experiment, we discover that the word vectors\nwith 300 dimensions, generated from \"skip-gram\" method of word2vec model using\nthe sliding window size of 4, are giving the most robust vector representations\nfor Bangla language.\n",
                "publicationDate": "2020-10-26T08:00:48Z",
                "Link": "http://arxiv.org/pdf/2010.13404v3",
                "arxiv_id": "2010.13404v3"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset",
                "Authors": "Ajwad Akil, Najrin Sultana, Abhik Bhattacharjee, Rifat Shahriyar",
                "Abstract": "  In this work, we present BanglaParaphrase, a high-quality synthetic Bangla\nParaphrase dataset curated by a novel filtering pipeline. We aim to take a step\ntowards alleviating the low resource status of the Bangla language in the NLP\ndomain through the introduction of BanglaParaphrase, which ensures quality by\npreserving both semantics and diversity, making it particularly useful to\nenhance other Bangla datasets. We show a detailed comparative analysis between\nour dataset and models trained on it with other existing works to establish the\nviability of our synthetic paraphrase data generation pipeline. We are making\nthe dataset and models publicly available at\nhttps://github.com/csebuetnlp/banglaparaphrase to further the state of Bangla\nNLP.\n",
                "publicationDate": "2022-10-11T02:52:31Z",
                "Link": "http://arxiv.org/pdf/2210.05109v1",
                "arxiv_id": "2210.05109v1"
            },
            {
                "Title": "Hand Sign to Bangla Speech: A Deep Learning in Vision based system for\n  Recognizing Hand Sign Digits and Generating Bangla Speech",
                "Authors": "Shahjalal Ahmed, Md. Rafiqul Islam, Jahid Hassan, Minhaz Uddin Ahmed, Bilkis Jamal Ferdosi, Sanjay Saha, Md. Shopon",
                "Abstract": "  Recent advancements in the field of computer vision with the help of deep\nneural networks have led us to explore and develop many existing challenges\nthat were once unattended due to the lack of necessary technologies. Hand\nSign/Gesture Recognition is one of the significant areas where the deep neural\nnetwork is making a substantial impact. In the last few years, a large number\nof researches has been conducted to recognize hand signs and hand gestures,\nwhich we aim to extend to our mother-tongue, Bangla (also known as Bengali).\nThe primary goal of our work is to make an automated tool to aid the people who\nare unable to speak. We developed a system that automatically detects hand sign\nbased digits and speaks out the result in Bangla language. According to the\nreport of the World Health Organization (WHO), 15% of people in the world live\nwith some kind of disabilities. Among them, individuals with communication\nimpairment such as speech disabilities experience substantial barrier in social\ninteraction. The proposed system can be invaluable to mitigate such a barrier.\nThe core of the system is built with a deep learning model which is based on\nconvolutional neural networks (CNN). The model classifies hand sign based\ndigits with 92% accuracy over validation data which ensures it a highly\ntrustworthy system. Upon classification of the digits, the resulting output is\nfed to the text to speech engine and the translator unit eventually which\ngenerates audio output in Bangla language. A web application to demonstrate our\ntool is available at http://bit.ly/signdigits2banglaspeech.\n",
                "publicationDate": "2019-01-17T04:27:34Z",
                "Link": "http://arxiv.org/pdf/1901.05613v1",
                "arxiv_id": "1901.05613v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer\n  Architecture with Sequence Alignment",
                "Authors": "Jakir Hasan, Shrestha Datta, Ameya Debnath",
                "Abstract": "  The International Phonetic Alphabet (IPA) is indispensable in language\nlearning and understanding, aiding users in accurate pronunciation and\ncomprehension. Additionally, it plays a pivotal role in speech therapy,\nlinguistic research, accurate transliteration, and the development of\ntext-to-speech systems, making it an essential tool across diverse fields.\nBangla being 7th as one of the widely used languages, gives rise to the need\nfor IPA in its domain. Its IPA mapping is too diverse to be captured manually\ngiving the need for Artificial Intelligence and Machine Learning in this field.\nIn this study, we have utilized a transformer-based sequence-to-sequence model\nat the letter and symbol level to get the IPA of each Bangla word as the\nvariation of IPA in association of different words is almost null. Our\ntransformer model only consisted of 8.5 million parameters with only a single\ndecoder and encoder layer. Additionally, to handle the punctuation marks and\nthe occurrence of foreign languages in the text, we have utilized manual\nmapping as the model won't be able to learn to separate them from Bangla words\nwhile decreasing our required computational resources. Finally, maintaining the\nrelative position of the sentence component IPAs and generation of the combined\nIPA has led us to achieve the top position with a word error rate of 0.10582 in\nthe public ranking of DataVerse Challenge - ITVerse 2023\n(https://www.kaggle.com/competitions/dataverse_2023/).\n",
                "publicationDate": "2023-11-07T08:20:06Z",
                "Link": "http://arxiv.org/pdf/2311.03792v1",
                "arxiv_id": "2311.03792v1"
            },
            {
                "Title": "BANSpEmo: A Bangla Emotional Speech Recognition Dataset",
                "Authors": "Md Gulzar Hussain, Mahmuda Rahman, Babe Sultana, Ye Shiren",
                "Abstract": "  In the field of audio and speech analysis, the ability to identify emotions\nfrom acoustic signals is essential. Human-computer interaction (HCI) and\nbehavioural analysis are only a few of the many areas where the capacity to\ndistinguish emotions from speech signals has an extensive range of\napplications. Here, we are introducing BanSpEmo, a corpus of emotional speech\nthat only consists of audio recordings and has been created specifically for\nthe Bangla language. This corpus contains 792 audio recordings over a duration\nof more than 1 hour and 23 minutes. 22 native speakers took part in the\nrecording of two sets of sentences that represent the six desired emotions. The\ndata set consists of 12 Bangla sentences which are uttered in 6 emotions as\nDisgust, Happy, Sad, Surprised, Anger, and Fear. This corpus is not also gender\nbalanced. Ten individuals who either have experience in related field or have\nacting experience took part in the assessment of this corpus. It has a balanced\nnumber of audio recordings in each emotion class. BanSpEmo can be considered as\na useful resource to promote emotion and speech recognition research and\nrelated applications in the Bangla language. The dataset can be found here:\nhttps://data.mendeley.com/datasets/rdwn4bs5ky and might be employed for\nacademic research.\n",
                "publicationDate": "2023-12-21T16:52:41Z",
                "Link": "http://arxiv.org/pdf/2312.14020v1",
                "arxiv_id": "2312.14020v1"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate\n  Speech in Different Social Contexts",
                "Authors": "Nauros Romim, Mosahed Ahmed, Md. Saiful Islam, Arnab Sen Sharma, Hriteshwar Talukder, Mohammad Ruhul Amin",
                "Abstract": "  Social media platforms and online streaming services have spawned a new breed\nof Hate Speech (HS). Due to the massive amount of user-generated content on\nthese sites, modern machine learning techniques are found to be feasible and\ncost-effective to tackle this problem. However, linguistically diverse datasets\ncovering different social contexts in which offensive language is typically\nused are required to train generalizable models. In this paper, we identify the\nshortcomings of existing Bangla HS datasets and introduce a large manually\nlabeled dataset BD-SHS that includes HS in different social contexts. The\nlabeling criteria were prepared following a hierarchical annotation process,\nwhich is the first of its kind in Bangla HS to the best of our knowledge. The\ndataset includes more than 50,200 offensive comments crawled from online social\nnetworking sites and is at least 60% larger than any existing Bangla HS\ndatasets. We present the benchmark result of our dataset by training different\nNLP models resulting in the best one achieving an F1-score of 91.0%. In our\nexperiments, we found that a word embedding trained exclusively using 1.47\nmillion comments from social media and streaming sites consistently resulted in\nbetter modeling of HS detection in comparison to other pre-trained embeddings.\nOur dataset and all accompanying codes is publicly available at\ngithub.com/naurosromim/hate-speech-dataset-for-Bengali-social-media\n",
                "publicationDate": "2022-06-01T10:10:15Z",
                "Link": "http://arxiv.org/pdf/2206.00372v1",
                "arxiv_id": "2206.00372v1"
            },
            {
                "Title": "BanglaWriting: A multi-purpose offline Bangla handwriting dataset",
                "Authors": "M. F. Mridha, Abu Quwsar Ohi, M. Ameer Ali, Mazedul Islam Emon, Muhammad Mohsin Kabir",
                "Abstract": "  This article presents a Bangla handwriting dataset named BanglaWriting that\ncontains single-page handwritings of 260 individuals of different personalities\nand ages. Each page includes bounding-boxes that bounds each word, along with\nthe unicode representation of the writing. This dataset contains 21,234 words\nand 32,787 characters in total. Moreover, this dataset includes 5,470 unique\nwords of Bangla vocabulary. Apart from the usual words, the dataset comprises\n261 comprehensible overwriting and 450 handwritten strikes and mistakes. All of\nthe bounding-boxes and word labels are manually-generated. The dataset can be\nused for complex optical character/word recognition, writer identification,\nhandwritten word segmentation, and word generation. Furthermore, this dataset\nis suitable for extracting age-based and gender-based variation of handwriting.\n",
                "publicationDate": "2020-11-15T11:08:53Z",
                "Link": "http://arxiv.org/pdf/2011.07499v3",
                "arxiv_id": "2011.07499v3"
            },
            {
                "Title": "BDSL 49: A Comprehensive Dataset of Bangla Sign Language",
                "Authors": "Ayman Hasib, Saqib Sizan Khan, Jannatul Ferdous Eva, Mst. Nipa Khatun, Ashraful Haque, Nishat Shahrin, Rashik Rahman, Hasan Murad, Md. Rajibul Islam, Molla Rashied Hussein",
                "Abstract": "  Language is a method by which individuals express their thoughts. Each\nlanguage has its own set of alphabetic and numeric characters. People can\ncommunicate with one another through either oral or written communication.\nHowever, each language has a sign language counterpart. Individuals who are\ndeaf and/or mute communicate through sign language. The Bangla language also\nhas a sign language, which is called BDSL. The dataset is about Bangla hand\nsign images. The collection contains 49 individual Bangla alphabet images in\nsign language. BDSL49 is a dataset that consists of 29,490 images with 49\nlabels. Images of 14 different adult individuals, each with a distinct\nbackground and appearance, have been recorded during data collection. Several\nstrategies have been used to eliminate noise from datasets during preparation.\nThis dataset is available to researchers for free. They can develop automated\nsystems using machine learning, computer vision, and deep learning techniques.\nIn addition, two models were used in this dataset. The first is for detection,\nwhile the second is for recognition.\n",
                "publicationDate": "2022-08-14T10:54:49Z",
                "Link": "http://arxiv.org/pdf/2208.06827v1",
                "arxiv_id": "2208.06827v1"
            }
        ]
    },
    {
        "topic_name": "Sentiment Classification",
        "summary": "default",
        "papers": [
            {
                "Title": "A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis\n  on Noisy Bangla Texts",
                "Authors": "Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar",
                "Abstract": "  While Bangla is considered a language with limited resources, sentiment\nanalysis has been a subject of extensive research in the literature.\nNevertheless, there is a scarcity of exploration into sentiment analysis\nspecifically in the realm of noisy Bangla texts. In this paper, we introduce a\ndataset (NC-SentNoB) that we annotated manually to identify ten different types\nof noise found in a pre-existing sentiment analysis dataset comprising of\naround 15K noisy Bangla texts. At first, given an input noisy text, we identify\nthe noise type, addressing this as a multi-label classification task. Then, we\nintroduce baseline noise reduction methods to alleviate noise prior to\nconducting sentiment analysis. Finally, we assess the performance of fine-tuned\nsentiment analysis models with both noisy and noise-reduced texts to make\ncomparisons. The experimental findings indicate that the noise reduction\nmethods utilized are not satisfactory, highlighting the need for more suitable\nnoise reduction methods in future research endeavors. We have made the\nimplementation and dataset presented in this paper publicly available at\nhttps://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bangla-Texts\n",
                "publicationDate": "2024-01-25T18:06:19Z",
                "Link": "http://arxiv.org/pdf/2401.14360v2",
                "arxiv_id": "2401.14360v2"
            },
            {
                "Title": "RSM-NLP at BLP-2023 Task 2: Bangla Sentiment Analysis using Weighted and\n  Majority Voted Fine-Tuned Transformers",
                "Authors": "Pratinav Seth, Rashi Goel, Komal Mathur, Swetha Vemulapalli",
                "Abstract": "  This paper describes our approach to submissions made at Shared Task 2 at BLP\nWorkshop - Sentiment Analysis of Bangla Social Media Posts. Sentiment Analysis\nis an action research area in the digital age. With the rapid and constant\ngrowth of online social media sites and services and the increasing amount of\ntextual data, the application of automatic Sentiment Analysis is on the rise.\nHowever, most of the research in this domain is based on the English language.\nDespite being the world's sixth most widely spoken language, little work has\nbeen done in Bangla. This task aims to promote work on Bangla Sentiment\nAnalysis while identifying the polarity of social media content by determining\nwhether the sentiment expressed in the text is Positive, Negative, or Neutral.\nOur approach consists of experimenting and finetuning various multilingual and\npre-trained BERT-based models on our downstream tasks and using a Majority\nVoting and Weighted ensemble model that outperforms individual baseline model\nscores. Our system scored 0.711 for the multiclass classification task and\nscored 10th place among the participants on the leaderboard for the shared\ntask. Our code is available at https://github.com/ptnv-s/RSM-NLP-BLP-Task2 .\n",
                "publicationDate": "2023-10-22T10:55:56Z",
                "Link": "http://arxiv.org/pdf/2310.14261v1",
                "arxiv_id": "2310.14261v1"
            },
            {
                "Title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from\n  Book Reviews",
                "Authors": "Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan",
                "Abstract": "  The analysis of consumer sentiment, as expressed through reviews, can provide\na wealth of insight regarding the quality of a product. While the study of\nsentiment analysis has been widely explored in many popular languages,\nrelatively less attention has been given to the Bangla language, mostly due to\na lack of relevant data and cross-domain adaptability. To address this\nlimitation, we present BanglaBook, a large-scale dataset of Bangla book reviews\nconsisting of 158,065 samples classified into three broad categories: positive,\nnegative, and neutral. We provide a detailed statistical analysis of the\ndataset and employ a range of machine learning models to establish baselines\nincluding SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial\nperformance advantage of pre-trained models over models that rely on manually\ncrafted features, emphasizing the necessity for additional training resources\nin this domain. Additionally, we conduct an in-depth error analysis by\nexamining sentiment unigrams, which may provide insight into common\nclassification errors in under-resourced languages like Bangla. Our codes and\ndata are publicly available at https://github.com/mohsinulkabir14/BanglaBook.\n",
                "publicationDate": "2023-05-11T06:27:38Z",
                "Link": "http://arxiv.org/pdf/2305.06595v3",
                "arxiv_id": "2305.06595v3"
            },
            {
                "Title": "SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis\n  Dataset and its Evaluation",
                "Authors": "Md. Ekramul Islam, Labib Chowdhury, Faisal Ahamed Khan, Shazzad Hossain, Sourave Hossain, Mohammad Mamun Or Rashid, Nabeel Mohammed, Mohammad Ruhul Amin",
                "Abstract": "  This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis\ndataset. Comprising 70,000 samples, it was created from diverse sources and\nannotated by a gender-balanced team of linguists. SentiGOLD adheres to\nestablished linguistic conventions agreed upon by the Government of Bangladesh\nand a Bangla linguistics committee. Unlike English and other languages, Bangla\nlacks standard sentiment analysis datasets due to the absence of a national\nlinguistics framework. The dataset incorporates data from online video\ncomments, social media posts, blogs, news, and other sources while maintaining\ndomain and class distribution rigorously. It spans 30 domains (e.g., politics,\nentertainment, sports) and includes 5 sentiment classes (strongly negative,\nweakly negative, neutral, and strongly positive). The annotation scheme,\napproved by the national linguistics committee, ensures a robust Inter\nAnnotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and\ncross-dataset evaluation protocols are applied to establish a standard\nclassification system. Cross-dataset evaluation on the noisy SentNoB dataset\npresents a challenging test scenario. Additionally, zero-shot experiments\ndemonstrate the generalizability of SentiGOLD. The top model achieves a macro\nf1 score of 0.62 (intra-dataset) across 5 classes, setting a benchmark, and\n0.61 (cross-dataset from SentNoB) across 3 classes, comparable to the\nstate-of-the-art. Fine-tuned sentiment analysis model can be accessed at\nhttps://sentiment.bangla.gov.bd.\n",
                "publicationDate": "2023-06-09T12:07:10Z",
                "Link": "http://arxiv.org/pdf/2306.06147v1",
                "arxiv_id": "2306.06147v1"
            },
            {
                "Title": "Sentiment Classification in Bangla Textual Content: A Comparative Study",
                "Authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam",
                "Abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n",
                "publicationDate": "2020-11-19T21:06:28Z",
                "Link": "http://arxiv.org/pdf/2011.10106v1",
                "arxiv_id": "2011.10106v1"
            },
            {
                "Title": "Bangla Text Classification using Transformers",
                "Authors": "Tanvirul Alam, Akib Khan, Firoj Alam",
                "Abstract": "  Text classification has been one of the earliest problems in NLP. Over time\nthe scope of application areas has broadened and the difficulty of dealing with\nnew areas (e.g., noisy social media content) has increased. The problem-solving\nstrategy switched from classical machine learning to deep learning algorithms.\nOne of the recent deep neural network architecture is the Transformer. Models\ndesigned with this type of network and its variants recently showed their\nsuccess in many downstream natural language processing tasks, especially for\nresource-rich languages, e.g., English. However, these models have not been\nexplored fully for Bangla text classification tasks. In this work, we fine-tune\nmultilingual transformer models for Bangla text classification tasks in\ndifferent domains, including sentiment analysis, emotion detection, news\ncategorization, and authorship attribution. We obtain the state of the art\nresults on six benchmark datasets, improving upon the previous results by 5-29%\naccuracy across different tasks.\n",
                "publicationDate": "2020-11-09T14:12:07Z",
                "Link": "http://arxiv.org/pdf/2011.04446v1",
                "arxiv_id": "2011.04446v1"
            },
            {
                "Title": "Machine and Deep Learning Methods with Manual and Automatic Labelling\n  for News Classification in Bangla Language",
                "Authors": "Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood",
                "Abstract": "  Research in Natural Language Processing (NLP) has increasingly become\nimportant due to applications such as text classification, text mining,\nsentiment analysis, POS tagging, named entity recognition, textual entailment,\nand many others. This paper introduces several machine and deep learning\nmethods with manual and automatic labelling for news classification in the\nBangla language. We implemented several machine (ML) and deep learning (DL)\nalgorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient\nDescent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document\nFrequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long\nShort-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit\n(GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and\nFastText word embedding models. We develop automatic labelling methods using\nLatent Dirichlet Allocation (LDA) and investigate the performance of\nsingle-label and multi-label article classification methods. To investigate\nperformance, we developed from scratch Potrika, the largest and the most\nextensive dataset for news classification in the Bangla language, comprising\n185.51 million words and 12.57 million sentences contained in 664,880 news\narticles in eight distinct categories, curated from six popular online news\nportals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83%\nachieve the highest accuracy for manually-labelled data. For the automatic\nlabelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy\nfor single-label and multi-label data, respectively. The methods developed in\nthis paper are expected to advance research in Bangla and other languages.\n",
                "publicationDate": "2022-10-19T21:53:49Z",
                "Link": "http://arxiv.org/pdf/2210.10903v1",
                "arxiv_id": "2210.10903v1"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla\n  Sentiment Analysis",
                "Authors": "Dhiman Goswami, Md Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nSentiment Analysis of Bangla Social Media Posts organized at the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The main objective\nof this task is to identify the polarity of social media content using a Bangla\ndataset annotated with positive, neutral, and negative labels provided by the\nshared task organizers. Our best system for this task is a transfer learning\napproach with data augmentation which achieved a micro F1 score of 0.71. Our\nbest system ranked 12th among 30 teams that participated in the competition.\n",
                "publicationDate": "2023-11-25T13:58:58Z",
                "Link": "http://arxiv.org/pdf/2311.15032v1",
                "arxiv_id": "2311.15032v1"
            },
            {
                "Title": "Transformer-based Text Classification on Unified Bangla Multi-class\n  Emotion Corpus",
                "Authors": "Md Sakib Ullah Sourav, Huidong Wang, Mohammad Sultan Mahmud, Hua Zheng",
                "Abstract": "  In this research, we propose a complete set of approaches for identifying and\nextracting emotions from Bangla texts. We provide a Bangla emotion classifier\nfor six classes: anger, disgust, fear, joy, sadness, and surprise, from Bangla\nwords using transformer-based models, which exhibit phenomenal results in\nrecent days, especially for high-resource languages. The Unified Bangla\nMulti-class Emotion Corpus (UBMEC) is used to assess the performance of our\nmodels. UBMEC is created by combining two previously released manually labeled\ndatasets of Bangla comments on six emotion classes with fresh manually labeled\nBangla comments created by us. The corpus dataset and code we used in this work\nare publicly available.\n",
                "publicationDate": "2022-10-12T17:01:53Z",
                "Link": "http://arxiv.org/pdf/2210.06405v3",
                "arxiv_id": "2210.06405v3"
            },
            {
                "Title": "BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models\n  for Sentiment Analysis of Bangla Social Media Posts",
                "Authors": "Saumajit Saha, Albert Nanda",
                "Abstract": "  Bangla is the 7th most widely spoken language globally, with a staggering 234\nmillion native speakers primarily hailing from India and Bangladesh. This\nmorphologically rich language boasts a rich literary tradition, encompassing\ndiverse dialects and language-specific challenges. Despite its linguistic\nrichness and history, Bangla remains categorized as a low-resource language\nwithin the natural language processing (NLP) and speech community. This paper\npresents our submission to Task 2 (Sentiment Analysis of Bangla Social Media\nPosts) of the BLP Workshop. We experiment with various Transformer-based\narchitectures to solve this task. Our quantitative results show that transfer\nlearning really helps in better learning of the models in this low-resource\nlanguage scenario. This becomes evident when we further finetune a model which\nhas already been finetuned on twitter data for sentiment analysis task and that\nfinetuned model performs the best among all other models. We also perform a\ndetailed error analysis where we find some instances where ground truth labels\nneed to be relooked at. We obtain a micro-F1 of 67.02\\% on the test set and our\nperformance in this shared task is ranked at 21 in the leaderboard.\n",
                "publicationDate": "2023-10-13T16:46:38Z",
                "Link": "http://arxiv.org/pdf/2310.09238v2",
                "arxiv_id": "2310.09238v2"
            },
            {
                "Title": "BanglaLekha-Isolated: A Comprehensive Bangla Handwritten Character\n  Dataset",
                "Authors": "Mithun Biswas, Rafiqul Islam, Gautam Kumar Shom, Md Shopon, Nabeel Mohammed, Sifat Momen, Md Anowarul Abedin",
                "Abstract": "  Bangla handwriting recognition is becoming a very important issue nowadays.\nIt is potentially a very important task specially for Bangla speaking\npopulation of Bangladesh and West Bengal. By keeping that in our mind we are\nintroducing a comprehensive Bangla handwritten character dataset named\nBanglaLekha-Isolated. This dataset contains Bangla handwritten numerals, basic\ncharacters and compound characters. This dataset was collected from multiple\ngeographical location within Bangladesh and includes sample collected from a\nvariety of aged groups. This dataset can also be used for other classification\nproblems i.e: gender, age, district. This is the largest dataset on Bangla\nhandwritten characters yet.\n",
                "publicationDate": "2017-02-22T07:57:14Z",
                "Link": "http://arxiv.org/pdf/1703.10661v1",
                "arxiv_id": "1703.10661v1"
            },
            {
                "Title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models",
                "Authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad",
                "Abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n",
                "publicationDate": "2016-10-02T23:45:23Z",
                "Link": "http://arxiv.org/pdf/1610.00369v2",
                "arxiv_id": "1610.00369v2"
            },
            {
                "Title": "A Classical Approach to Handcrafted Feature Extraction Techniques for\n  Bangla Handwritten Digit Recognition",
                "Authors": "Md. Ferdous Wahid, Md. Fahim Shahriar, Md. Shohanur Islam Sobuj",
                "Abstract": "  Bangla Handwritten Digit recognition is a significant step forward in the\ndevelopment of Bangla OCR. However, intricate shape, structural likeness and\ndistinctive composition style of Bangla digits makes it relatively challenging\nto distinguish. Thus, in this paper, we benchmarked four rigorous classifiers\nto recognize Bangla Handwritten Digit: K-Nearest Neighbor (KNN), Support Vector\nMachine (SVM), Random Forest (RF), and Gradient-Boosted Decision Trees (GBDT)\nbased on three handcrafted feature extraction techniques: Histogram of Oriented\nGradients (HOG), Local Binary Pattern (LBP), and Gabor filter on four publicly\navailable Bangla handwriting digits datasets: NumtaDB, CMARTdb, Ekush and BDRW.\nHere, handcrafted feature extraction methods are used to extract features from\nthe dataset image, which are then utilized to train machine learning\nclassifiers to identify Bangla handwritten digits. We further fine-tuned the\nhyperparameters of the classification algorithms in order to acquire the finest\nBangla handwritten digits recognition performance from these algorithms, and\namong all the models we employed, the HOG features combined with SVM model\n(HOG+SVM) attained the best performance metrics across all datasets. The\nrecognition accuracy of the HOG+SVM method on the NumtaDB, CMARTdb, Ekush and\nBDRW datasets reached 93.32%, 98.08%, 95.68% and 89.68%, respectively as well\nas we compared the model performance with recent state-of-art methods.\n",
                "publicationDate": "2022-01-25T05:27:57Z",
                "Link": "http://arxiv.org/pdf/2201.10102v1",
                "arxiv_id": "2201.10102v1"
            },
            {
                "Title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla",
                "Authors": "Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar",
                "Abstract": "  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https://github.com/csebuetnlp/banglabert to\nadvance Bangla NLP.\n",
                "publicationDate": "2021-01-01T09:28:45Z",
                "Link": "http://arxiv.org/pdf/2101.00204v4",
                "arxiv_id": "2101.00204v4"
            },
            {
                "Title": "Zero- and Few-Shot Prompting with LLMs: A Comparative Study with\n  Fine-tuned Models for Bangla Sentiment Analysis",
                "Authors": "Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori",
                "Abstract": "  The rapid expansion of the digital world has propelled sentiment analysis\ninto a critical tool across diverse sectors such as marketing, politics,\ncustomer service, and healthcare. While there have been significant\nadvancements in sentiment analysis for widely spoken languages, low-resource\nlanguages, such as Bangla, remain largely under-researched due to resource\nconstraints. Furthermore, the recent unprecedented performance of Large\nLanguage Models (LLMs) in various applications highlights the need to evaluate\nthem in the context of low-resource languages. In this study, we present a\nsizeable manually annotated dataset encompassing 33,606 Bangla news tweets and\nFacebook comments. We also investigate zero- and few-shot in-context learning\nwith several language models, including Flan-T5, GPT-4, and Bloomz, offering a\ncomparative analysis against fine-tuned models. Our findings suggest that\nmonolingual transformer-based models consistently outperform other models, even\nin zero and few-shot scenarios. To foster continued exploration, we intend to\nmake this dataset and our research tools publicly available to the broader\nresearch community.\n",
                "publicationDate": "2023-08-21T15:19:10Z",
                "Link": "http://arxiv.org/pdf/2308.10783v2",
                "arxiv_id": "2308.10783v2"
            },
            {
                "Title": "MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection",
                "Authors": "Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty",
                "Abstract": "  In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have\nbeen increasingly popular in the Bangla language, which is the seventh most\nspoken language throughout the entire world. However, the language is\nstructurally complicated, which makes this field arduous to extract emotions in\nan accurate manner. Several distinct approaches such as the extraction of\npositive and negative sentiments as well as multiclass emotions, have been\nimplemented in this field of study. Nevertheless, the extraction of multiple\nsentiments is an almost untouched area in this language. Which involves\nidentifying several feelings based on a single piece of text. Therefore, this\nstudy demonstrates a thorough method for constructing an annotated corpus based\non scrapped data from Facebook to bridge the gaps in this subject area to\novercome the challenges. To make this annotation more fruitful, the\ncontext-based approach has been used. Bidirectional Encoder Representations\nfrom Transformers (BERT), a well-known methodology of transformers, have been\nshown the best results of all methods implemented. Finally, a web application\nhas been developed to demonstrate the performance of the pre-trained\ntop-performer model (BERT) for multi-label ER in Bangla.\n",
                "publicationDate": "2023-09-27T14:10:57Z",
                "Link": "http://arxiv.org/pdf/2309.15670v1",
                "arxiv_id": "2309.15670v1"
            },
            {
                "Title": "Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated\n  Translation of Bangla Regional Dialects to Bangla Language",
                "Authors": "Fatema Tuj Johora Faria, Mukaffi Bin Moin, Ahmed Al Wase, Mehidi Ahmmed, Md. Rabius Sani, Tashreef Muhammad",
                "Abstract": "  The Bangla linguistic variety is a fascinating mix of regional dialects that\nadds to the cultural diversity of the Bangla-speaking community. Despite\nextensive study into translating Bangla to English, English to Bangla, and\nBanglish to Bangla in the past, there has been a noticeable gap in translating\nBangla regional dialects into standard Bangla. In this study, we set out to\nfill this gap by creating a collection of 32,500 sentences, encompassing\nBangla, Banglish, and English, representing five regional Bangla dialects. Our\naim is to translate these regional dialects into standard Bangla and detect\nregions accurately. To achieve this, we proposed models known as mT5 and\nBanglaT5 for translating regional dialects into standard Bangla. Additionally,\nwe employed mBERT and Bangla-bert-base to determine the specific regions from\nwhere these dialects originated. Our experimental results showed the highest\nBLEU score of 69.06 for Mymensingh regional dialects and the lowest BLEU score\nof 36.75 for Chittagong regional dialects. We also observed the lowest average\nword error rate of 0.1548 for Mymensingh regional dialects and the highest of\n0.3385 for Chittagong regional dialects. For region detection, we achieved an\naccuracy of 85.86% for Bangla-bert-base and 84.36% for mBERT. This is the first\nlarge-scale investigation of Bangla regional dialects to Bangla machine\ntranslation. We believe our findings will not only pave the way for future work\non Bangla regional dialects to Bangla machine translation, but will also be\nuseful in solving similar language-related challenges in low-resource language\nconditions.\n",
                "publicationDate": "2023-11-18T18:36:16Z",
                "Link": "http://arxiv.org/pdf/2311.11142v1",
                "arxiv_id": "2311.11142v1"
            },
            {
                "Title": "End-to-End Natural Language Understanding Pipeline for Bangla\n  Conversational Agents",
                "Authors": "Fahim Shahriar Khan, Mueeze Al Mushabbir, Mohammad Sabik Irbaz, MD Abdullah Al Nasim",
                "Abstract": "  Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n",
                "publicationDate": "2021-07-12T16:09:22Z",
                "Link": "http://arxiv.org/pdf/2107.05541v6",
                "arxiv_id": "2107.05541v6"
            },
            {
                "Title": "Zero-shot Aspect-level Sentiment Classification via Explicit Utilization\n  of Aspect-to-Document Sentiment Composition",
                "Authors": "Pengfei Deng, Jianhua Yuan, Yanyan Zhao, Bing Qin",
                "Abstract": "  As aspect-level sentiment labels are expensive and labor-intensive to\nacquire, zero-shot aspect-level sentiment classification is proposed to learn\nclassifiers applicable to new domains without using any annotated aspect-level\ndata. In contrast, document-level sentiment data with ratings are more easily\naccessible. In this work, we achieve zero-shot aspect-level sentiment\nclassification by only using document-level reviews. Our key intuition is that\nthe sentiment representation of a document is composed of the sentiment\nrepresentations of all the aspects of that document. Based on this, we propose\nthe AF-DSC method to explicitly model such sentiment composition in reviews.\nAF-DSC first learns sentiment representations for all potential aspects and\nthen aggregates aspect-level sentiments into a document-level one to perform\ndocument-level sentiment classification. In this way, we obtain the\naspect-level sentiment classifier as the by-product of the document-level\nsentiment classifier. Experimental results on aspect-level sentiment\nclassification benchmarks demonstrate the effectiveness of explicit utilization\nof sentiment composition in document-level sentiment classification. Our model\nwith only 30k training data outperforms previous work utilizing millions of\ndata.\n",
                "publicationDate": "2022-09-06T08:02:55Z",
                "Link": "http://arxiv.org/pdf/2209.02276v1",
                "arxiv_id": "2209.02276v1"
            },
            {
                "Title": "BanglaNet: Bangla Handwritten Character Recognition using Ensembling of\n  Convolutional Neural Network",
                "Authors": "Chandrika Saha, Md Mostafijur Rahman",
                "Abstract": "  Handwritten character recognition is a crucial task because of its abundant\napplications. The recognition task of Bangla handwritten characters is\nespecially challenging because of the cursive nature of Bangla characters and\nthe presence of compound characters with more than one way of writing. In this\npaper, a classification model based on the ensembling of several Convolutional\nNeural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic\ncharacters, compound characters, numerals, and modifiers. Three different\nmodels based on the idea of state-of-the-art CNN models like Inception, ResNet,\nand DenseNet have been trained with both augmented and non-augmented inputs.\nFinally, all these models are averaged or ensembled to get the finishing model.\nRigorous experimentation on three benchmark Bangla handwritten characters\ndatasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited\nsignificant recognition accuracies compared to some recent CNN-based research.\nThe top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and\nthe top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,\nBanglaLekha-Isolated, and Ekush datasets respectively.\n",
                "publicationDate": "2024-01-16T01:08:19Z",
                "Link": "http://arxiv.org/pdf/2401.08035v2",
                "arxiv_id": "2401.08035v2"
            },
            {
                "Title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language\n  Model",
                "Authors": "Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail",
                "Abstract": "  In this paper, we describe a research method that generates Bangla word\nclusters on the basis of relating to meaning in language and contextual\nsimilarity. The importance of word clustering is in parts of speech (POS)\ntagging, word sense disambiguation, text classification, recommender system,\nspell checker, grammar checker, knowledge discover and for many others Natural\nLanguage Processing (NLP) applications. In the history of word clustering,\nEnglish and some other languages have already implemented some methods on word\nclustering efficiently. But due to lack of the resources, word clustering in\nBangla has not been still implemented efficiently. Presently, its\nimplementation is in the beginning stage. In some research of word clustering\nin English based on preceding and next five words of a key word they found an\nefficient result. Now, we are trying to implement the tri-gram, 4-gram and\n5-gram model of word clustering for Bangla to observe which one is the best\namong them. We have started our research with quite a large corpus of\napproximate 1 lakh Bangla words. We are using a machine learning technique in\nthis research. We will generate word clusters and analyze the clusters by\ntesting some different threshold values.\n",
                "publicationDate": "2017-01-27T18:43:31Z",
                "Link": "http://arxiv.org/pdf/1701.08702v1",
                "arxiv_id": "1701.08702v1"
            },
            {
                "Title": "Bangla Natural Language Processing: A Comprehensive Analysis of\n  Classical, Machine Learning, and Deep Learning Based Methods",
                "Authors": "Ovishake Sen, Mohtasim Fuad, MD. Nazrul Islam, Jakaria Rabbi, Mehedi Masud, MD. Kamrul Hasan, Md. Abdul Awal, Awal Ahmed Fime, Md. Tahmid Hasan Fuad, Delowar Sikder, MD. Akil Raihan Iftee",
                "Abstract": "  The Bangla language is the seventh most spoken language, with 265 million\nnative and non-native speakers worldwide. However, English is the predominant\nlanguage for online resources and technical knowledge, journals, and\ndocumentation. Consequently, many Bangla-speaking people, who have limited\ncommand of English, face hurdles to utilize English resources. To bridge the\ngap between limited support and increasing demand, researchers conducted many\nexperiments and developed valuable tools and techniques to create and process\nBangla language materials. Many efforts are also ongoing to make it easy to use\nthe Bangla language in the online and technical domains. There are some review\npapers to understand the past, previous, and future Bangla Natural Language\nProcessing (BNLP) trends. The studies are mainly concentrated on the specific\ndomains of BNLP, such as sentiment analysis, speech recognition, optical\ncharacter recognition, and text summarization. There is an apparent scarcity of\nresources that contain a comprehensive review of the recent BNLP tools and\nmethods. Therefore, in this paper, we present a thorough analysis of 75 BNLP\nresearch papers and categorize them into 11 categories, namely Information\nExtraction, Machine Translation, Named Entity Recognition, Parsing, Parts of\nSpeech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake\nDetection, Text Summarization, Word Sense Disambiguation, and Speech Processing\nand Recognition. We study articles published between 1999 to 2021, and 50% of\nthe papers were published after 2015. Furthermore, we discuss Classical,\nMachine Learning and Deep Learning approaches with different datasets while\naddressing the limitations and current and future trends of the BNLP.\n",
                "publicationDate": "2021-05-31T10:58:58Z",
                "Link": "http://arxiv.org/pdf/2105.14875v3",
                "arxiv_id": "2105.14875v3"
            },
            {
                "Title": "Improving Twitter Sentiment Classification via Multi-Level\n  Sentiment-Enriched Word Embeddings",
                "Authors": "Shufeng Xiong",
                "Abstract": "  Most of existing work learn sentiment-specific word representation for\nimproving Twitter sentiment classification, which encoded both n-gram and\ndistant supervised tweet sentiment information in learning process. They assume\nall words within a tweet have the same sentiment polarity as the whole tweet,\nwhich ignores the word its own sentiment polarity. To address this problem, we\npropose to learn sentiment-specific word embedding by exploiting both lexicon\nresource and distant supervised information. We develop a multi-level\nsentiment-enriched word embedding learning method, which uses parallel\nasymmetric neural network to model n-gram, word level sentiment and tweet level\nsentiment in learning process. Experiments on standard benchmarks show our\napproach outperforms state-of-the-art methods.\n",
                "publicationDate": "2016-11-01T04:48:09Z",
                "Link": "http://arxiv.org/pdf/1611.00126v1",
                "arxiv_id": "1611.00126v1"
            },
            {
                "Title": "A Multi-sentiment-resource Enhanced Attention Network for Sentiment\n  Classification",
                "Authors": "Zeyang Lei, Yujiu Yang, Min Yang, Yi Liu",
                "Abstract": "  Deep learning approaches for sentiment classification do not fully exploit\nsentiment linguistic knowledge. In this paper, we propose a\nMulti-sentiment-resource Enhanced Attention Network (MEAN) to alleviate the\nproblem by integrating three kinds of sentiment linguistic knowledge (e.g.,\nsentiment lexicon, negation words, intensity words) into the deep neural\nnetwork via attention mechanisms. By using various types of sentiment\nresources, MEAN utilizes sentiment-relevant information from different\nrepresentation subspaces, which makes it more effective to capture the overall\nsemantics of the sentiment, negation and intensity words for sentiment\nprediction. The experimental results demonstrate that MEAN has robust\nsuperiority over strong competitors.\n",
                "publicationDate": "2018-07-13T10:01:19Z",
                "Link": "http://arxiv.org/pdf/1807.04990v1",
                "arxiv_id": "1807.04990v1"
            },
            {
                "Title": "A Review of Bangla Natural Language Processing Tasks and the Utility of\n  Transformer Models",
                "Authors": "Firoj Alam, Arid Hasan, Tanvirul Alam, Akib Khan, Janntatul Tajrin, Naira Khan, Shammur Absar Chowdhury",
                "Abstract": "  Bangla -- ranked as the 6th most widely spoken language across the world\n(https://www.ethnologue.com/guides/ethnologue200), with 230 million native\nspeakers -- is still considered as a low-resource language in the natural\nlanguage processing (NLP) community. With three decades of research, Bangla NLP\n(BNLP) is still lagging behind mainly due to the scarcity of resources and the\nchallenges that come with it. There is sparse work in different areas of BNLP;\nhowever, a thorough survey reporting previous work and recent advances is yet\nto be done. In this study, we first provide a review of Bangla NLP tasks,\nresources, and tools available to the research community; we benchmark datasets\ncollected from various platforms for nine NLP tasks using current\nstate-of-the-art algorithms (i.e., transformer-based models). We provide\ncomparative results for the studied NLP tasks by comparing monolingual vs.\nmultilingual models of varying sizes. We report our results using both\nindividual and consolidated datasets and provide data splits for future\nresearch. We reviewed a total of 108 papers and conducted 175 sets of\nexperiments. Our results show promising performance using transformer-based\nmodels while highlighting the trade-off with computational costs. We hope that\nsuch a comprehensive survey will motivate the community to build on and further\nadvance the research on Bangla NLP.\n",
                "publicationDate": "2021-07-08T13:49:46Z",
                "Link": "http://arxiv.org/pdf/2107.03844v3",
                "arxiv_id": "2107.03844v3"
            },
            {
                "Title": "Mood Classification of Bangla Songs Based on Lyrics",
                "Authors": "Maliha Mahajebin, Mohammad Rifat Ahmmad Rashid, Nafees Mansoor",
                "Abstract": "  Music can evoke various emotions, and with the advancement of technology, it\nhas become more accessible to people. Bangla music, which portrays different\nhuman emotions, lacks sufficient research. The authors of this article aim to\nanalyze Bangla songs and classify their moods based on the lyrics. To achieve\nthis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,\nand used Natural Language Processing and the Bert Algorithm to analyze the\ndata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362\nfor the romantic mood, 886 for happiness, and the rest 239 are classified as\nrelaxation. By embedding the lyrics of the songs, the authors have classified\nthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is\ncrucial as it enables a multi-class classification of songs' moods, making the\nmusic more relatable to people's emotions. The article presents the automated\nresult of the four moods accurately derived from the song lyrics.\n",
                "publicationDate": "2023-07-19T03:31:41Z",
                "Link": "http://arxiv.org/pdf/2307.10314v1",
                "arxiv_id": "2307.10314v1"
            },
            {
                "Title": "A Complete Workflow for Development of Bangla OCR",
                "Authors": "Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas",
                "Abstract": "  Developing a Bangla OCR requires bunch of algorithm and methods. There were\nmany effort went on for developing a Bangla OCR. But all of them failed to\nprovide an error free Bangla OCR. Each of them has some lacking. We discussed\nabout the problem scope of currently existing Bangla OCR's. In this paper, we\npresent the basic steps required for developing a Bangla OCR and a complete\nworkflow for development of a Bangla OCR with mentioning all the possible\nalgorithms required.\n",
                "publicationDate": "2012-04-05T12:28:11Z",
                "Link": "http://arxiv.org/pdf/1204.1198v1",
                "arxiv_id": "1204.1198v1"
            },
            {
                "Title": "Bangla License Plate Recognition Using Convolutional Neural Networks\n  (CNN)",
                "Authors": "M M Shaifur Rahman, Mst Shamima Nasrin, Moin Mostakim, Md Zahangir Alom",
                "Abstract": "  In the last few years, the deep learning technique in particular\nConvolutional Neural Networks (CNNs) is using massively in the field of\ncomputer vision and machine learning. This deep learning technique provides\nstate-of-the-art accuracy in different classification, segmentation, and\ndetection tasks on different benchmarks such as MNIST, CIFAR-10, CIFAR-100,\nMicrosoft COCO, and ImageNet. However, there are a lot of research has been\nconducted for Bangla License plate recognition with traditional machine\nlearning approaches in last decade. None of them are used to deploy a physical\nsystem for Bangla License Plate Recognition System (BLPRS) due to their poor\nrecognition accuracy. In this paper, we have implemented CNNs based Bangla\nlicense plate recognition system with better accuracy that can be applied for\ndifferent purposes including roadside assistance, automatic parking lot\nmanagement system, vehicle license status detection and so on. Along with that,\nwe have also created and released a very first and standard database for BLPRS.\n",
                "publicationDate": "2018-09-04T11:55:34Z",
                "Link": "http://arxiv.org/pdf/1809.00905v1",
                "arxiv_id": "1809.00905v1"
            },
            {
                "Title": "A Subword Level Language Model for Bangla Language",
                "Authors": "Aisha Khatun, Anisur Rahman, Hemayet Ahmed Chowdhury, Md. Saiful Islam, Ayesha Tasnim",
                "Abstract": "  Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.\n",
                "publicationDate": "2019-11-15T08:22:33Z",
                "Link": "http://arxiv.org/pdf/1911.07613v1",
                "arxiv_id": "1911.07613v1"
            },
            {
                "Title": "SlangSD: Building and Using a Sentiment Dictionary of Slang Words for\n  Short-Text Sentiment Classification",
                "Authors": "Liang Wu, Fred Morstatter, Huan Liu",
                "Abstract": "  Sentiment in social media is increasingly considered as an important resource\nfor customer segmentation, market understanding, and tackling other\nsocio-economic issues. However, sentiment in social media is difficult to\nmeasure since user-generated content is usually short and informal. Although\nmany traditional sentiment analysis methods have been proposed, identifying\nslang sentiment words remains untackled. One of the reasons is that slang\nsentiment words are not available in existing dictionaries or sentiment\nlexicons. To this end, we propose to build the first sentiment dictionary of\nslang words to aid sentiment analysis of social media content. It is laborious\nand time-consuming to collect and label the sentiment polarity of a\ncomprehensive list of slang words. We present an approach to leverage web\nresources to construct an extensive Slang Sentiment word Dictionary (SlangSD)\nthat is easy to maintain and extend. SlangSD is publicly available for research\npurposes. We empirically show the advantages of using SlangSD, the newly-built\nslang sentiment word dictionary for sentiment classification, and provide\nexamples demonstrating its ease of use with an existing sentiment system.\n",
                "publicationDate": "2016-08-17T23:32:57Z",
                "Link": "http://arxiv.org/pdf/1608.05129v1",
                "arxiv_id": "1608.05129v1"
            },
            {
                "Title": "SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis",
                "Authors": "Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, Feng Wu",
                "Abstract": "  Recently, sentiment analysis has seen remarkable advance with the help of\npre-training approaches. However, sentiment knowledge, such as sentiment words\nand aspect-sentiment pairs, is ignored in the process of pre-training, despite\nthe fact that they are widely used in traditional sentiment analysis\napproaches. In this paper, we introduce Sentiment Knowledge Enhanced\nPre-training (SKEP) in order to learn a unified sentiment representation for\nmultiple sentiment analysis tasks. With the help of automatically-mined\nknowledge, SKEP conducts sentiment masking and constructs three sentiment\nknowledge prediction objectives, so as to embed sentiment information at the\nword, polarity and aspect level into pre-trained sentiment representation. In\nparticular, the prediction of aspect-sentiment pairs is converted into\nmulti-label classification, aiming to capture the dependency between words in a\npair. Experiments on three kinds of sentiment tasks show that SKEP\nsignificantly outperforms strong pre-training baseline, and achieves new\nstate-of-the-art results on most of the test datasets. We release our code at\nhttps://github.com/baidu/Senta.\n",
                "publicationDate": "2020-05-12T09:23:32Z",
                "Link": "http://arxiv.org/pdf/2005.05635v2",
                "arxiv_id": "2005.05635v2"
            },
            {
                "Title": "Comparison of Classical Machine Learning Approaches on Bangla Textual\n  Emotion Analysis",
                "Authors": "Md. Ataur Rahman, Md. Hanif Seddiqui",
                "Abstract": "  Detecting emotions from text is an extension of simple sentiment polarity\ndetection. Instead of considering only positive or negative sentiments,\nemotions are conveyed using more tangible manner; thus, they can be expressed\nas many shades of gray. This paper manifests the results of our experimentation\nfor fine-grained emotion analysis on Bangla text. We gathered and annotated a\ntext corpus consisting of user comments from several Facebook groups regarding\nsocio-economic and political issues, and we made efforts to extract the basic\nemotions (sadness, happiness, disgust, surprise, fear, anger) conveyed through\nthese comments. Finally, we compared the results of the five most popular\nclassical machine learning techniques namely Naive Bayes, Decision Tree,\nk-Nearest Neighbor (k-NN), Support Vector Machine (SVM) and K-Means Clustering\nwith several combinations of features. Our best model (SVM with a non-linear\nradial-basis function (RBF) kernel) achieved an overall average accuracy score\nof 52.98% and an F1 score (macro) of 0.3324\n",
                "publicationDate": "2019-07-18T01:00:42Z",
                "Link": "http://arxiv.org/pdf/1907.07826v1",
                "arxiv_id": "1907.07826v1"
            },
            {
                "Title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource\n  Sentiment Analysis of Bangla Language",
                "Authors": "Aunabil Chakma, Masum Hasan",
                "Abstract": "  This paper describes the system of the LowResource Team for Task 2 of\nBLP-2023, which involves conducting sentiment analysis on a dataset composed of\npublic posts and comments from diverse social media platforms. Our primary aim\nis to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus,\nusing various strategies including fine-tuning, dropping random tokens, and\nusing several external datasets. Our final model is an ensemble of the three\nbest BanglaBert variations. Our system has achieved overall 3rd in the Test Set\namong 30 participating teams with a score of 0.718. Additionally, we discuss\nthe promising systems that didn't perform well namely task-adaptive pertaining\nand paraphrasing using BanglaT5. Training codes and external datasets which are\nused for our system are publicly available at\nhttps://github.com/Aunabil4602/bnlp-workshop-task2-2023\n",
                "publicationDate": "2023-11-21T17:21:15Z",
                "Link": "http://arxiv.org/pdf/2311.12735v1",
                "arxiv_id": "2311.12735v1"
            },
            {
                "Title": "SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment\n  Analysis",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Antara Mahmud, Antonios Anastasopoulos, Marcos Zampieri",
                "Abstract": "  Code-mixing is a well-studied linguistic phenomenon when two or more\nlanguages are mixed in text or speech. Several datasets have been build with\nthe goal of training computational models for code-mixing. Although it is very\ncommon to observe code-mixing with multiple languages, most datasets available\ncontain code-mixed between only two languages. In this paper, we introduce\nSentMix-3L, a novel dataset for sentiment analysis containing code-mixed data\nbetween three languages Bangla, English, and Hindi. We carry out a\ncomprehensive evaluation using SentMix-3L. We show that zero-shot prompting\nwith GPT-3.5 outperforms all transformer-based models on SentMix-3L.\n",
                "publicationDate": "2023-10-27T09:59:24Z",
                "Link": "http://arxiv.org/pdf/2310.18023v2",
                "arxiv_id": "2310.18023v2"
            },
            {
                "Title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence\n  Inciting Text Detection in Bangla",
                "Authors": "Md Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Marcos Zampieri",
                "Abstract": "  In this paper, we discuss the nlpBDpatriots entry to the shared task on\nViolence Inciting Text Detection (VITD) organized as part of the first workshop\non Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task\nis to identify and classify the violent threats, that provoke further unlawful\nviolent acts. Our best-performing approach for the task is two-step\nclassification using back translation and multilinguality which ranked 6th out\nof 27 teams with a macro F1 score of 0.74.\n",
                "publicationDate": "2023-11-25T13:47:34Z",
                "Link": "http://arxiv.org/pdf/2311.15029v1",
                "arxiv_id": "2311.15029v1"
            },
            {
                "Title": "LSA: Modeling Aspect Sentiment Coherency via Local Sentiment Aggregation",
                "Authors": "Heng Yang, Ke Li",
                "Abstract": "  Aspect sentiment coherency is an intriguing yet underexplored topic in the\nfield of aspect-based sentiment classification. This concept reflects the\ncommon pattern where adjacent aspects often share similar sentiments. Despite\nits prevalence, current studies have not fully recognized the potential of\nmodeling aspect sentiment coherency, including its implications in adversarial\ndefense. To model aspect sentiment coherency, we propose a novel local\nsentiment aggregation (LSA) paradigm based on constructing a\ndifferential-weighted sentiment aggregation window. We have rigorously\nevaluated our model through experiments, and the results affirm the proficiency\nof LSA in terms of aspect coherency prediction and aspect sentiment\nclassification. For instance, it outperforms existing models and achieves\nstate-of-the-art sentiment classification performance across five public\ndatasets. Furthermore, we demonstrate the promising ability of LSA in ABSC\nadversarial defense, thanks to its sentiment coherency modeling. To encourage\nfurther exploration and application of this concept, we have made our code\npublicly accessible. This will provide researchers with a valuable tool to\ndelve into sentiment coherency modeling in future research.\n",
                "publicationDate": "2021-10-16T16:22:43Z",
                "Link": "http://arxiv.org/pdf/2110.08604v4",
                "arxiv_id": "2110.08604v4"
            },
            {
                "Title": "Deep neural network-based classification model for Sentiment Analysis",
                "Authors": "Donghang Pan, Jingling Yuan, Lin Li, Deming Sheng",
                "Abstract": "  The growing prosperity of social networks has brought great challenges to the\nsentimental tendency mining of users. As more and more researchers pay\nattention to the sentimental tendency of online users, rich research results\nhave been obtained based on the sentiment classification of explicit texts.\nHowever, research on the implicit sentiment of users is still in its infancy.\nAiming at the difficulty of implicit sentiment classification, a research on\nimplicit sentiment classification model based on deep neural network is carried\nout. Classification models based on DNN, LSTM, Bi-LSTM and CNN were established\nto judge the tendency of the user's implicit sentiment text. Based on the\nBi-LSTM model, the classification model of word-level attention mechanism is\nstudied. The experimental results on the public dataset show that the\nestablished LSTM series classification model and CNN classification model can\nachieve good sentiment classification effect, and the classification effect is\nsignificantly better than the DNN model. The Bi-LSTM based attention mechanism\nclassification model obtained the optimal R value in the positive category\nidentification.\n",
                "publicationDate": "2019-07-03T17:24:14Z",
                "Link": "http://arxiv.org/pdf/1907.02046v1",
                "arxiv_id": "1907.02046v1"
            },
            {
                "Title": "Feature Extraction Using Deep Generative Models for Bangla Text\n  Classification on a New Comprehensive Dataset",
                "Authors": "Md. Rafi-Ur-Rashid, Sami Azam, Mirjam Jonkman",
                "Abstract": "  The selection of features for text classification is a fundamental task in\ntext mining and information retrieval. Despite being the sixth most widely\nspoken language in the world, Bangla has received little attention due to the\nscarcity of text datasets. In this research, we collected, annotated, and\nprepared a comprehensive dataset of 212,184 Bangla documents in seven different\ncategories and made it publicly accessible. We implemented three deep learning\ngenerative models: LSTM variational autoencoder (LSTM VAE), auxiliary\nclassifier generative adversarial network (AC-GAN), and adversarial autoencoder\n(AAE) to extract text features, although their applications are initially found\nin the field of computer vision. We utilized our dataset to train these three\nmodels and used the feature space obtained in the document classification task.\nWe evaluated the performance of the classifiers and found that the adversarial\nautoencoder model produced the best feature space.\n",
                "publicationDate": "2023-08-21T22:18:09Z",
                "Link": "http://arxiv.org/pdf/2308.13545v1",
                "arxiv_id": "2308.13545v1"
            },
            {
                "Title": "On Evaluation of Bangla Word Analogies",
                "Authors": "Mousumi Akter, Souvika Sarkar, Shubhra Kanti Karmaker Santu",
                "Abstract": "  This paper presents a high-quality dataset for evaluating the quality of\nBangla word embeddings, which is a fundamental task in the field of Natural\nLanguage Processing (NLP). Despite being the 7th most-spoken language in the\nworld, Bangla is a low-resource language and popular NLP models fail to perform\nwell. Developing a reliable evaluation test set for Bangla word embeddings are\ncrucial for benchmarking and guiding future research. We provide a\nMikolov-style word analogy evaluation set specifically for Bangla, with a\nsample size of 16678, as well as a translated and curated version of the\nMikolov dataset, which contains 10594 samples for cross-lingual research. Our\nexperiments with different state-of-the-art embedding models reveal that Bangla\nhas its own unique characteristics, and current embeddings for Bangla still\nstruggle to achieve high accuracy on both datasets. We suggest that future\nresearch should focus on training models with larger datasets and considering\nthe unique morphological characteristics of Bangla. This study represents the\nfirst step towards building a reliable NLP system for the Bangla language1.\n",
                "publicationDate": "2023-04-10T14:27:35Z",
                "Link": "http://arxiv.org/pdf/2304.04613v1",
                "arxiv_id": "2304.04613v1"
            },
            {
                "Title": "USA: Universal Sentiment Analysis Model & Construction of Japanese\n  Sentiment Text Classification and Part of Speech Dataset",
                "Authors": "Chengguang Gan, Qinghao Zhang, Tatsunori Mori",
                "Abstract": "  Sentiment analysis is a pivotal task in the domain of natural language\nprocessing. It encompasses both text-level sentiment polarity classification\nand word-level Part of Speech(POS) sentiment polarity determination. Such\nanalysis challenges models to understand text holistically while also\nextracting nuanced information. With the rise of Large Language Models(LLMs),\nnew avenues for sentiment analysis have opened. This paper proposes enhancing\nperformance by leveraging the Mutual Reinforcement Effect(MRE) between\nindividual words and the overall text. It delves into how word polarity\ninfluences the overarching sentiment of a passage. To support our research, we\nannotated four novel Sentiment Text Classification and Part of Speech(SCPOS)\ndatasets, building upon existing sentiment classification datasets.\nFurthermore, we developed a Universal Sentiment Analysis(USA) model, with a\n7-billion parameter size. Experimental results revealed that our model\nsurpassed the performance of gpt-3.5-turbo across all four datasets,\nunderscoring the significance of MRE in sentiment analysis.\n",
                "publicationDate": "2023-09-07T15:35:00Z",
                "Link": "http://arxiv.org/pdf/2309.03787v2",
                "arxiv_id": "2309.03787v2"
            },
            {
                "Title": "BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls\n  of Large Language Models on Bengali NLP",
                "Authors": "Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque",
                "Abstract": "  Large Language Models (LLMs) have emerged as one of the most important\nbreakthroughs in NLP for their impressive skills in language generation and\nother language-specific tasks. Though LLMs have been evaluated in various\ntasks, mostly in English, they have not yet undergone thorough evaluation in\nunder-resourced languages such as Bengali (Bangla). To this end, this paper\nintroduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to\nbenchmark their performance in the Bengali language that has modest resources.\nIn this regard, we select various important and diverse Bengali NLP tasks, such\nas text summarization, question answering, paraphrasing, natural language\ninference, transliteration, text classification, and sentiment analysis for\nzero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and\nClaude-2. Our experimental results demonstrate that while in some Bengali NLP\ntasks, zero-shot LLMs could achieve performance on par, or even better than\ncurrent SOTA fine-tuned models; in most tasks, their performance is quite poor\n(with the performance of open-source LLMs like LLaMA-2-13b-chat being\nsignificantly bad) in comparison to the current SOTA results. Therefore, it\ncalls for further efforts to develop a better understanding of LLMs in\nmodest-resourced languages like Bengali.\n",
                "publicationDate": "2023-09-22T20:29:34Z",
                "Link": "http://arxiv.org/pdf/2309.13173v2",
                "arxiv_id": "2309.13173v2"
            },
            {
                "Title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for\n  Sentiment Classification and Quantification",
                "Authors": "Sebastian Ruder, Parsa Ghaffari, John G. Breslin",
                "Abstract": "  This paper describes our deep learning-based approach to sentiment analysis\nin Twitter as part of SemEval-2016 Task 4. We use a convolutional neural\nnetwork to determine sentiment and participate in all subtasks, i.e. two-point,\nthree-point, and five-point scale sentiment classification and two-point and\nfive-point scale sentiment quantification. We achieve competitive results for\ntwo-point scale sentiment classification and quantification, ranking fifth and\na close fourth (third and second by alternative metrics) respectively despite\nusing only pre-trained embeddings that contain no sentiment information. We\nachieve good performance on three-point scale sentiment classification, ranking\neighth out of 35, while performing poorly on five-point scale sentiment\nclassification and quantification. An error analysis reveals that this is due\nto low expressiveness of the model to capture negative sentiment as well as an\ninability to take into account ordinal information. We propose improvements in\norder to address these and other issues.\n",
                "publicationDate": "2016-09-09T11:16:56Z",
                "Link": "http://arxiv.org/pdf/1609.02746v1",
                "arxiv_id": "1609.02746v1"
            },
            {
                "Title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
                "Authors": "Ramisa Anan, Tasnim Sakib Apon, Zeba Tahsin Hossain, Elizabeth Antora Modhu, Sudipta Mondal, MD. Golam Rabiul Alam",
                "Abstract": "  A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.\n",
                "publicationDate": "2023-03-22T17:35:35Z",
                "Link": "http://arxiv.org/pdf/2303.12772v1",
                "arxiv_id": "2303.12772v1"
            },
            {
                "Title": "Detecting Domain Polarity-Changes of Words in a Sentiment Lexicon",
                "Authors": "Shuai Wang, Guangyi Lv, Sahisnu Mazumder, Bing Liu",
                "Abstract": "  Sentiment lexicons are instrumental for sentiment analysis. One can use a set\nof sentiment words provided in a sentiment lexicon and a lexicon-based\nclassifier to perform sentiment classification. One major issue with this\napproach is that many sentiment words are domain dependent. That is, they may\nbe positive in some domains but negative in some others. We refer to this\nproblem as domain polarity-changes of words. Detecting such words and\ncorrecting their sentiment for an application domain is very important. In this\npaper, we propose a graph-based technique to tackle this problem. Experimental\nresults show its effectiveness on multiple real-world datasets.\n",
                "publicationDate": "2020-04-29T17:35:05Z",
                "Link": "http://arxiv.org/pdf/2004.14357v1",
                "arxiv_id": "2004.14357v1"
            },
            {
                "Title": "Sentiment Identification in Code-Mixed Social Media Text",
                "Authors": "Souvick Ghosh, Satanu Ghosh, Dipankar Das",
                "Abstract": "  Sentiment analysis is the Natural Language Processing (NLP) task dealing with\nthe detection and classification of sentiments in texts. While some tasks deal\nwith identifying the presence of sentiment in the text (Subjectivity analysis),\nother tasks aim at determining the polarity of the text categorizing them as\npositive, negative and neutral. Whenever there is a presence of sentiment in\nthe text, it has a source (people, group of people or any entity) and the\nsentiment is directed towards some entity, object, event or person. Sentiment\nanalysis tasks aim to determine the subject, the target and the polarity or\nvalence of the sentiment. In our work, we try to automatically extract\nsentiment (positive or negative) from Facebook posts using a machine learning\napproach.While some works have been done in code-mixed social media data and in\nsentiment analysis separately, our work is the first attempt (as of now) which\naims at performing sentiment analysis of code-mixed social media text. We have\nused extensive pre-processing to remove noise from raw text. Multilayer\nPerceptron model has been used to determine the polarity of the sentiment. We\nhave also developed the corpus for this task by manually labeling Facebook\nposts with their associated sentiments.\n",
                "publicationDate": "2017-07-04T23:29:44Z",
                "Link": "http://arxiv.org/pdf/1707.01184v1",
                "arxiv_id": "1707.01184v1"
            },
            {
                "Title": "Latent Variable Sentiment Grammar",
                "Authors": "Liwen Zhang, Kewei Tu, Yue Zhang",
                "Abstract": "  Neural models have been investigated for sentiment classification over\nconstituent trees. They learn phrase composition automatically by encoding tree\nstructures but do not explicitly model sentiment composition, which requires to\nencode sentiment class labels. To this end, we investigate two formalisms with\ndeep sentiment representations that capture sentiment subtype expressions by\nlatent variables and Gaussian mixture vectors, respectively. Experiments on\nStanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar\nover vanilla neural encoders. Using ELMo embeddings, our method gives the best\nresults on this benchmark.\n",
                "publicationDate": "2019-06-29T15:15:16Z",
                "Link": "http://arxiv.org/pdf/1907.00218v2",
                "arxiv_id": "1907.00218v2"
            },
            {
                "Title": "Sentiment analysis and opinion mining on E-commerce site",
                "Authors": "Fatema Tuz Zohra Anny, Oahidul Islam",
                "Abstract": "  Sentiment analysis or opinion mining help to illustrate the phrase NLP\n(Natural Language Processing). Sentiment analysis has been the most significant\ntopic in recent years. The goal of this study is to solve the sentiment\npolarity classification challenges in sentiment analysis. A broad technique for\ncategorizing sentiment opposition is presented, along with comprehensive\nprocess explanations. With the results of the analysis, both sentence-level\nclassification and review-level categorization are conducted. Finally, we\ndiscuss our plans for future sentiment analysis research.\n",
                "publicationDate": "2022-11-28T16:43:33Z",
                "Link": "http://arxiv.org/pdf/2211.15536v2",
                "arxiv_id": "2211.15536v2"
            },
            {
                "Title": "A Statistical Parsing Framework for Sentiment Classification",
                "Authors": "Li Dong, Furu Wei, Shujie Liu, Ming Zhou, Ke Xu",
                "Abstract": "  We present a statistical parsing framework for sentence-level sentiment\nclassification in this article. Unlike previous works that employ syntactic\nparsing results for sentiment analysis, we develop a statistical parser to\ndirectly analyze the sentiment structure of a sentence. We show that\ncomplicated phenomena in sentiment analysis (e.g., negation, intensification,\nand contrast) can be handled the same as simple and straightforward sentiment\nexpressions in a unified and probabilistic way. We formulate the sentiment\ngrammar upon Context-Free Grammars (CFGs), and provide a formal description of\nthe sentiment parsing framework. We develop the parsing model to obtain\npossible sentiment parse trees for a sentence, from which the polarity model is\nproposed to derive the sentiment strength and polarity, and the ranking model\nis dedicated to selecting the best sentiment tree. We train the parser directly\nfrom examples of sentences annotated only with sentiment polarity labels but\nwithout any syntactic annotations or polarity annotations of constituents\nwithin sentences. Therefore we can obtain training data easily. In particular,\nwe train a sentiment parser, s.parser, from a large amount of review sentences\nwith users' ratings as rough sentiment polarity labels. Extensive experiments\non existing benchmark datasets show significant improvements over baseline\nsentiment classification approaches.\n",
                "publicationDate": "2014-01-24T12:56:36Z",
                "Link": "http://arxiv.org/pdf/1401.6330v2",
                "arxiv_id": "1401.6330v2"
            },
            {
                "Title": "A Convolutional Neural Network for Aspect Sentiment Classification",
                "Authors": "Yongping Xing, Chuangbai Xiao, Yifei Wu, Ziming Ding",
                "Abstract": "  With the development of the Internet, natural language processing (NLP), in\nwhich sentiment analysis is an important task, became vital in information\nprocessing.Sentiment analysis includes aspect sentiment classification. Aspect\nsentiment can provide complete and in-depth results with increased attention on\naspect-level. Different context words in a sentence influence the sentiment\npolarity of a sentence variably, and polarity varies based on the different\naspects in a sentence. Take the sentence, 'I bought a new camera. The picture\nquality is amazing but the battery life is too short.'as an example. If the\naspect is picture quality, then the expected sentiment polarity is 'positive',\nif the battery life aspect is considered, then the sentiment polarity should be\n'negative'; therefore, aspect is important to consider when we explore aspect\nsentiment in the sentence. Recurrent neural network (RNN) is regarded as a good\nmodel to deal with natural language processing, and RNNs has get good\nperformance on aspect sentiment classification including Target-Dependent LSTM\n(TD-LSTM) ,Target-Connection LSTM (TC-LSTM) (Tang, 2015a, b), AE-LSTM, AT-LSTM,\nAEAT-LSTM (Wang et al., 2016).There are also extensive literatures on sentiment\nclassification utilizing convolutional neural network, but there is little\nliterature on aspect sentiment classification using convolutional neural\nnetwork. In our paper, we develop attention-based input layers in which aspect\ninformation is considered by input layer. We then incorporate attention-based\ninput layers into convolutional neural network (CNN) to introduce context words\ninformation. In our experiment, incorporating aspect information into CNN\nimproves the latter's aspect sentiment classification performance without using\nsyntactic parser or external sentiment lexicons in a benchmark dataset from\nTwitter but get better performance compared with other models.\n",
                "publicationDate": "2018-07-04T09:07:34Z",
                "Link": "http://arxiv.org/pdf/1807.01704v1",
                "arxiv_id": "1807.01704v1"
            },
            {
                "Title": "Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks\n  for Accurate Bangla Sign Language Recognition",
                "Authors": "Haz Sameen Shahgir, Khondker Salman Sayeed, Md Toki Tahmid, Tanjeem Azwad Zaman, Md. Zarif Ul Alam",
                "Abstract": "  Recent advances in Deep Learning and Computer Vision have been successfully\nleveraged to serve marginalized communities in various contexts. One such area\nis Sign Language - a primary means of communication for the deaf community.\nHowever, so far, the bulk of research efforts and investments have gone into\nAmerican Sign Language, and research activity into low-resource sign languages\n- especially Bangla Sign Language - has lagged significantly. In this research\npaper, we present a new word-level Bangla Sign Language dataset - BdSL40 -\nconsisting of 611 videos over 40 words, along with two different approaches:\none with a 3D Convolutional Neural Network model and another with a novel Graph\nNeural Network approach for the classification of BdSL40 dataset. This is the\nfirst study on word-level BdSL recognition, and the dataset was transcribed\nfrom Indian Sign Language (ISL) using the Bangla Sign Language Dictionary\n(1997). The proposed GNN model achieved an F1 score of 89%. The study\nhighlights the significant lexical and semantic similarity between BdSL, West\nBengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in\nthe literature. We release the dataset and source code to stimulate further\nresearch.\n",
                "publicationDate": "2024-01-22T18:52:51Z",
                "Link": "http://arxiv.org/pdf/2401.12210v1",
                "arxiv_id": "2401.12210v1"
            }
        ]
    }
]